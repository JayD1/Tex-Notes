\documentclass[11pt,a4paper]{article}
\usepackage[margin=1in]{geometry}
\usepackage[utf8]{inputenc}
\usepackage{amsmath,amsthm,amsfonts,amssymb}
\usepackage[english]{babel}
\usepackage{algorithm,algpseudocode}
\usepackage{enumerate}
\usepackage[hidelinks]{hyperref}
\usepackage{cancel}
\usepackage{titlesec}

\hypersetup{
    filecolor=magenta,      
    urlcolor=blue,
    pdftitle={Notes on Riemannian Preconditioning},
    pdfpagemode=FullScreen,
}


\author{Jayadev Naram}
\title{Notes on Riemannian Preconditioning} 

\begin{document}

\date{}
\maketitle
\tableofcontents
\newpage

\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{remark}[theorem]{Remark}
\newtheorem{example}[theorem]{Example}
\newtheorem{assume}{Assumption}

\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}

% \newtheorem{theorem}{Theorem}
% \newtheorem{lemma}[theorem]{Lemma}[subsection]
% \newtheorem{definition}{Definition}[subsection]
% \newtheorem{remark}{Remark}[subsection]
% \newtheorem{example}{Example}[subsection]
% \newtheorem{remark}{remark}[subsection]
% \newtheorem{proposition}{Proposition}[subsection]
% \newtheorem{corollary}{Corollary}[prop]
% \newtheorem{assume}{Assumption}

\newcommand{\R}{\mathbb{R}}
\newcommand{\A}{\mathcal{A}}
\newcommand{\M}{\mathcal{M}}
\newcommand{\N}{\mathcal{N}}
\newcommand{\T}{\mathcal{T}}
\newcommand{\Proj}{\mathcal{P}}
\newcommand{\perpProj}{\mathcal{P}^\perp}
\newcommand{\bb}{\mathbb{B}}
\newcommand{\Sprod}{\mathbb{S}_{xy}}
\newcommand{\highlight}[1]{\textsl{\textbf{#1}}}
\newcommand{\mapping}[3]{#1:#2\rightarrow #3}
\newcommand{\doubt}{\highlight{[??]}}
% \newcommand{\bigvert}[2]{#1{\raisebox{-.5ex}{$|$}_{#2}}}
\newcommand{\bigvert}[2]{\left.#1\right|_{#2}}
\newcommand{\sdnn}[1]{${#1}$}
\newcommand{\bsdnn}[1]{$\boldsymbol{#1}$}
\newcommand{\ifthen}[2]{\textbf{(#1)}\boldsymbol{\implies}\textbf{(#2)}}
\newcommand{\bsdn}[1]{\boldsymbol{#1}}
\newcommand{\forward}{$(\implies)$}
\newcommand{\converse}{$(\impliedby)$}
\newcommand{\Lt}[1]{\underset{#1\rightarrow 0}{Lt}}
\newcommand{\norm}[1]{\|#1\|}
\newcommand{\dparder}[2]{\dfrac{\partial #1}{\partial x_{#2}}}
\newcommand{\fparder}[2]{\frac{\partial #1}{\partial x_{#2}}}
\newcommand{\parder}[2]{\partial #1/\partial x_{#2}}
\newcommand{\parop}[1]{\dfrac{\partial}{\partial x_{#1}}}
\newcommand{\innerproduct}[2]{\langle #1, #2 \rangle}
\newcommand{\genst}{St_B(n,p)}
\newcommand{\igenst}[1]{St_{B_{#1}}(n_{#1},p)}
\newcommand{\realmat}[2]{\R^{#1\times #2}}
\newcommand{\Skew}{\mathcal{S}_{skew}(p)}
\newcommand{\Sym}{\mathcal{S}_{sym}(p)}
\newcommand{\XperpB}{X_{B^\perp}}
\newcommand{\polarRetr}{R^{polar}_X}
\newcommand{\qrRetr}{R^{QR}_X}
\newcommand{\vectransport}{\mathcal{T}}
\newcommand{\grad}{\text{grad}\,}
\newcommand{\hess}{\text{Hess}\,}
\newcommand{\lift}{\text{lift}}

\section{Problem Definition}

Consider the optimization problem
\begin{alignat}{3}
\underset{x\in\R^n}{\text{min}}& &&f(x) \nonumber\\
\text{subject to}&\hspace{0.5cm} &&h(x) = 0, \label{eqn:optimization_problem}
\end{alignat}
where $\mapping{f}{\R^n}{\R}$ and $\mapping{h}{\R^n}{\R^m}$ are smooth functions. Note that $h(x) = 0$ is a set of $m$ (possibly nonlinear) constraints $h_i(x) = 0$, where $h(x) = (h_1(x),\ldots,h_m(x))$. We consider two approaches to solve this problem namely - Sequential Quadratic Programming, and Riemannian Newton Method. Later we show that under certain assumptions these methods are equivalent to each other.

\subsubsection*{Notation} The Euclidean gradient of $f$ at $x\in\R^n$ will be denoted by $\nabla f(x) = \Big[\fparder{f(x)}{1} \ldots \fparder{f(x)}{n}\Big]^T\in \R^n$ and its Hessian matrix by $\nabla^2 f(x)\in \R^{n\times n}$. For a differentiable matrix-valued function $F$, we let $DF(x)[z]$ denote the directional derivative of $F$ at $x$ along $z$. If $F$ is vector valued, then $DF(x)$ also denotes the Jacobian matrix of $F$ at $x$ and $DF(x)z$ stands for $DF(x)[z]$. The gradient of the vector-valued function $h$ is denoted by $\nabla h(x) = D h(x)^T =\big[ \nabla h_1(x) \ldots \nabla h_m(x)\big]\in \R^{n\times m}$. Let $\Skew$ denote the set of all real skew-symmetric matrices of $p\times p$ and $\Sym$ denote the set of all real symmetric matrices of $p\times p$. 

\section{Sequential Quadratic Programming}

\subsection{Newton's Method}

The Lagrangian function for the problem in \eqref{eqn:optimization_problem} is $\mathcal{L}(x,\lambda) = f(x) - \lambda^T h(x)$. The KKT conditions of the equality constrained problem can be written as a system of $n+p$ equations in the $n+p$ unknowns $x$ and $\lambda$:
\begin{equation}
F(x,\lambda) = \left[
\begin{array}{c} 
\nabla_x \mathcal{L}(x,\lambda) \\ 
h(x)
\end{array} \right]
= \left[
\begin{array}{c} 
\nabla f(x) - \nabla h(x) \lambda \\ 
h(x)
\end{array} \right] = 0.\label{eqn:original_problem_KKT}
\end{equation}
This is nonlinear system of equation which can be solved by Newton's method. A linear model $M$ is constructed at $(x,\lambda)$ using the Taylor's expansion of $F$ as follows:
\begin{equation}
M(p) = F(x,\lambda) + \nabla F(x,\lambda) p,\label{eqn:Newton_model}
\end{equation}
where $p = (p_x,p_\lambda)$ and the Jacobian of $F$ is given by
\begin{equation}
\nabla F(x,\lambda) = \left[
\begin{array}{cc} 
\nabla^2_{xx} \mathcal{L}(x,\lambda) & -\nabla h(x) \\ 
\nabla h(x)^T & 0
\end{array} \right].\label{eqn:Jacobian_of_F}
\end{equation}
Then the Newton step is taken to be the vector $p$ such that $M(p) = 0$, i.e.,
\begin{equation}
\left[
\begin{array}{cc} 
\nabla^2_{xx} \mathcal{L}(x,\lambda) & -\nabla h(x) \\ 
\nabla h(x)^T & 0
\end{array} \right]
\left[
\begin{array}{c} 
p_x \\ 
p_\lambda
\end{array} \right] = 
\left[
\begin{array}{c} 
-\nabla f(x) + \nabla h(x)\lambda \\ 
-h(x)
\end{array} \right], \label{eqn:Newton_KKT}
\end{equation}
which is called as Newton-KKT system and the update is done as
\begin{equation}
\left[
\begin{array}{c} 
x_+ \\ 
\lambda_+
\end{array} \right] = 
\left[
\begin{array}{c} 
x \\ 
\lambda
\end{array} \right] +
\left[
\begin{array}{c} 
p_x \\ 
p_\lambda
\end{array} \right].\label{eqn:Newton_update}
\end{equation}

\begin{assume}\label{assume:nonsingular_jacobian}
\leavevmode
\begin{enumerate}[(a)]
    \item LICQ holds.
    \item The matrix $\nabla^2_{xx}\mathcal{L}(x,\lambda)$ is positive definite for all $d\neq 0$ such that $\nabla h(x)^T d = 0$.
\end{enumerate}
\end{assume}
Under these assumptions the Jacobian matrix $\nabla F$ is nonsingular at $(x,\lambda)$ (see Prop. \ref{prop:nonsingular_jacobian}). Note that LICQ condition ensures that the first equation in \eqref{eqn:original_problem_KKT} has a unique least-squares solution
\begin{equation}
\lambda(x) = (\nabla h(x)^T\nabla h(x))^{-1}\nabla h(x)^T\nabla f(x)\in \R^m.\label{eqn:least_squares_solution_for_lambda}
\end{equation}

\subsection{SQP Framework}

There is an alternative way to view the iteration \eqref{eqn:Newton_KKT}, \eqref{eqn:Newton_update}. Suppose that at $(x,\lambda)$ we model problem \eqref{eqn:optimization_problem} using the quadratic program
\begin{alignat}{3}
\underset{p\in \R^n}{\text{min}}& &&q(p)\equiv f(x) + \nabla f(x)^Tp + \frac{1}{2}p^T\nabla^2_{xx}\mathcal{L}(x,\lambda)p \nonumber\\
\text{subject to}&\hspace{0.5cm} && \nabla h(x)^Tp+h(x) = 0. \label{eqn:SQP_model}
\end{alignat}
The KKT conditions for this problem are
\begin{align}
\nabla^2_{xx}\mathcal{L}(x,\lambda)p + \nabla f(x) - \nabla h(x)l =&\; 0, \nonumber \\
\nabla h(x)^Tp + h(x) =&\; 0, \label{eqn:SQP_KKT}
\end{align}
where $l$ is the Lagrange multiplier. In block matrix form we have
\begin{equation}
\left[
\begin{array}{cc} 
\nabla^2_{xx} \mathcal{L}(x,\lambda) & -\nabla h(x) \\ 
\nabla h(x)^T & 0
\end{array} \right]
\left[
\begin{array}{c} 
p \\ 
l
\end{array} \right] = 
\left[
\begin{array}{c} 
-\nabla f(x) \\ 
-h(x)
\end{array} \right]. \label{eqn:SQP_KKT_matrix_form}
\end{equation}

The vectors $p$ and $l$ can be identified with the solution of the Newton equations \eqref{eqn:Newton_KKT}. If we subtract $\nabla h(x)\lambda$ from both sides of the first equation in \eqref{eqn:Newton_KKT}, we obtain
\begin{equation}
\left[
\begin{array}{cc} 
\nabla^2_{xx} \mathcal{L}(x,\lambda) & -\nabla h(x) \\ 
\nabla h(x)^T & 0
\end{array} \right]
\left[
\begin{array}{c} 
p_x \\ 
p_\lambda+\lambda
\end{array} \right] = 
\left[
\begin{array}{c} 
-\nabla f(x) \\ 
-h(x)
\end{array} \right]. \label{eqn:Newton_SQP_KKT_matrix_form}
\end{equation}
By comparing \eqref{eqn:Newton_SQP_KKT_matrix_form} and \eqref{eqn:SQP_KKT_matrix_form}, we see that $p_x = p$ and that $p_\lambda = l-\lambda$ solves the Newton-KKT system in \eqref{eqn:Newton_KKT} and \eqref{eqn:Newton_update}. 
If Assumptions \ref{assume:nonsingular_jacobian} hold at $(x,\lambda)$, there exists a unique solution $p^*$ to \eqref{eqn:SQP_model} that satisfies KKT conditions in \eqref{eqn:SQP_KKT_matrix_form}, and therefore in \eqref{eqn:Newton_KKT} (see Prop. \ref{prop:uniqueness_QP}).

\subsection{FP-SQP}

The update of SQP algorithm is $x_+ = x+p^*$. But it might so happen that $h(x_+)\neq 0$, in other words, $x_+$ is infeasible. To ensure the feasibility, one can simply project the update onto the feasible set. Thus we obtain feasibly-projected sequential quadratic programming (see Algo. \ref{algo:FP-SQP}).

\begin{algorithm}
    \centering
    \caption{simple FP-SQP for \eqref{eqn:optimization_problem}}\label{algo:FP-SQP}
    \begin{algorithmic}[1]
        \State Given a feasible starting point $x_0$, i.e., $h(x_0) = 0$.
        \For{$k = 0,1,2,\ldots$} 
            \State Choose $\lambda_k\in\R^m$.
            \State Compute the search direction $p_k$ that is the solution of \eqref{eqn:SQP_model}.
            \State The next iterate $x_{k+1}$ is obtained by projecting $x_k+p_k$ onto the feasible set.
        \EndFor
    \end{algorithmic}
\end{algorithm}

\section{Riemannian Newton Method}

\subsection{Riemannian Newton method on the feasible set}

\begin{assume}\label{assume:manifold}
For the constraint function $h$ in \eqref{eqn:optimization_problem}, $0$ is the regular value.
\end{assume}

Under Assumption \ref{assume:manifold}, the set $\M = h^{-1}(0)$ forms a Riemannian submanifold of $\R^n$ (see Sec. \ref{section:diffgeo}). The Riemannian Newton method which is direct generalization of classical Newton Method is presented in Algo. \ref{algo:Riemannian_Newton}.

\begin{algorithm}
    \centering
    \caption{Riemannian Newton Method on $\M$ for \eqref{eqn:optimization_problem}}\label{algo:Riemannian_Newton}
    \begin{algorithmic}[1]
        \State Given a retraction $R$ on $\M$ and a feasible starting point $x_0\in M$.
        \For{$k = 0,1,2,\ldots$} 
            \State Solve, for the unknown $p_k \in T_{x_k}\M$, the Newton equation: 
            \begin{equation}\label{eqn:Riemannian_Newton_system}
            \hess\bigvert{f}{\M}(x_k)[p_k] = -\grad\bigvert{f}{\M}(x_k).
            \end{equation}
            \State $x_{k+1} = R_{x_k}(p_k)$.
        \EndFor
    \end{algorithmic}
\end{algorithm}

\subsection{Connecting SQP and Riemannian Newton Method}

We rephrase the KKT conditions for SQP subproblem \eqref{eqn:SQP_KKT} on $\M$. For $x\in M$, we have $h(x) = 0$ and also note that 
\begin{align*}
\Proj_x(\nabla h(x)) =&\; (I - \nabla h(x)(\nabla h(x)^T\nabla h(x))^{-1}\nabla h(x)^T) \nabla h(x) \\
=&\; \nabla h(x) - \nabla h(x)(\nabla h(x)^T\nabla h(x))^{-1}(\nabla h(x)^T \nabla h(x)) = 0.
\end{align*}
Expanding the expression for $\nabla^2_{xx}\mathcal{L}(x,\lambda)$, the transformed KKT conditions for $p\in T_x\M$ are as follows:
\begin{align}
\Proj_x\bigg(\bigg[\nabla^2 f(x)+\sum_{i=1}^m\nabla^2 h_i(x)\lambda_i\bigg]p\bigg) =&\; -\Proj_x(\nabla f(x)) \nonumber \\
\nabla h(x)^Tp =&\; 0. \label{eqn:Riemannian_SQP_KKT}
\end{align}

Expanding the expression for Riemannian gradient of $\bigvert{f}{\M}$ gives
\begin{align*}
\grad\bigvert{f}{\M}(x) =&\; \Proj_x(\nabla f(x))\\
=&\; (I - \nabla h(x)(\nabla h(x)^T\nabla h(x))^{-1}\nabla h(x)^T) \nabla f(x) \\
\overset{*}{=}&\; \nabla f(x) - \nabla h(x)\{(\nabla h(x)^T\nabla h(x))^{-1}\nabla h(x)^T \nabla f(x)\} \\
=&\; \nabla f(x) - \nabla h(x)\lambda(x) = \nabla f(x) - \Sigma_{i=1}^m\nabla h_i(x)\lambda_i(x).
\end{align*}
since the expression in the flower bracket in (*) is equal to $\lambda(x)$ defined in \eqref{eqn:least_squares_solution_for_lambda}. Also note that for a real-valued function $\mapping{g}{\R^n}{\R}$, $D(\nabla g)(x)[p] = \nabla^2 g(x)p$. We compute the following expression
\begin{align*}
D(\text{grad}\bigvert{f}{\M})(x)[p] 
=&\; D(\nabla f - \nabla h\lambda)(x)[p] \\
=&\; D(\nabla f)(x)[p] - D(\nabla h\lambda)(x)[p] \\
=&\; \nabla^2 f(x)p - D(\Sigma_{i=1}^m\nabla h_i\lambda_i)(x)[p] \\
=&\; \nabla^2 f(x)p - \lambda_i(x)D(\Sigma_{i=1}^m\nabla h_i)(x)[p] - \nabla h(x)D\lambda(x)[p] \\
=&\; \nabla^2 f(x)p - \Sigma_{i=1}^m\lambda_i(x)\nabla^2 h_i(x)p - \nabla h(x)D\lambda(x)[p] \\
=&\; \bigg[\nabla^2 f(x)+\sum_{i=1}^m\nabla^2 h_i(x)\lambda_i\bigg]p - \nabla h(x)D\lambda(x)[p].
\end{align*}
Substituting this is the expression of Riemannian Hessian, we have
\begin{align*}
\hess\bigvert{f}{\M}(x)[p] =&\; \Proj_x(D(\grad\bigvert{f}{\M})(x)[p]) \\
=&\; \Proj_x\bigg(\bigg[\nabla^2 f(x)+\sum_{i=1}^m\nabla^2 h_i(x)\lambda_i\bigg]p\bigg),
\end{align*}
where we used the identity $\Proj_x(\nabla h(x)) = 0$. Then we see that the Riemannian Newton equation in \eqref{eqn:Riemannian_Newton_system} and $p\in T_x\M$ gives us the KKT condition of \eqref{eqn:Riemannian_SQP_KKT}.

\begin{proposition}[\highlight{Riemannain Newton and FP-SQP}]
Assume as always that $0$ is a regular value of $h$, so that the set $\M$ is an embedded submanifold of $\R^n$. Then simple FP-SQP (Algo. \ref{algo:FP-SQP}), with $\lambda$ in Step 3 chosen as in \eqref{eqn:least_squares_solution_for_lambda}, is equivalent to Riemannian Newton method (Algo. \ref{algo:Riemannian_Newton}). \end{proposition}

\begin{remark}
Even though they coincide under the above assumptions, FP-SQP and Newton
on manifolds are not identical methods. FP-SQP makes no assumption on $\lambda_k$ beyond the fact that it must converge to the correct value, whereas Newton on manifolds requires $\lambda_k$ to be defined by \eqref{eqn:least_squares_solution_for_lambda}. On the other hand, the Newton algorithm on manifolds is not restricted to submanifolds.
\end{remark}

\begin{proposition}
Assume that $\M$ is an embedded submanifold of $\R^n$ and $\mapping{f}{\M}{\R}$ is a smooth function with isolated minima on $\M$. If $x\in \M$ is a local minimum of $f$ on $\M$, then the second-order derivative of the Lagrangian along $v \in T_{x}\M$ captures all second-order information of the cost function $f$ on $\M$.
\end{proposition}

\begin{proof}
Note that 
\begin{equation*}
\Proj_x(\nabla^2_{xx}\mathcal{L}(x,\lambda)v) =  \Proj_x\bigg(\bigg[\nabla^2 f(x)+\sum_{i=1}^m\nabla^2 h_i(x)\lambda_i\bigg]v\bigg) = \hess\bigvert{f}{\M}(x)[v].
\end{equation*}
Then by Remark \ref{remark:second_order_optimality_conditions}, $\nabla^2_{xx}\mathcal{L}(x,\lambda)$ captures the second-order information of $f$ on $\M$.
\end{proof}

\section{Applications}

\subsection{Linear Discriminant Analysis}

Let $x^{(1)}_1,\ldots,x^{(1)}_{n_1},\,x^{(2)}_1,\ldots,x^{(2)}_{n_2},\,\ldots,x^{(l)}_1,\ldots,x^{(l)}_{n_l}\in \R^d$ be samples from $l\le d$ different classes, and denote by $x_1,\ldots,x_n$ the union of the different classes (the entire dataset in a sequential index). For $i = 1,\ldots,n$, let $y_i$ denote the label corresponding to $x_i$, i.e., $y_i = k$ if $x_i = x^{(k)}_j$ for some $j$. Let $m_k$, for $k = 1,\ldots,l,$ denote the sample mean of class $j$ (i.e., $m_k := (1/n_k)\sum_{i=1}^{n_k}x_i^{(k)}$), and $m:=(1/n)\sum_{i=1}^nx_i = (1/n)\sum_{k = 1}^ln_km_k$ denote the dataset sample mean of the entire dataset. Let $S_B$ and $S_W$ be the between-class and within-class scatter matrices (respectively):
\begin{equation*}
S_B:= \sum_{k=1}^l n_k(m_k-m)(m_k-m)^T,\qquad S_W:= \sum_{i=1}^n (x_i-m_{y_i})(x_i-m_{y_i})^T.
\end{equation*}

Let $X\in\realmat{n}{d}$ be the matrix such that $i$-th row is $x_i^T$ and $Y\in \realmat{n}{d}$ be the matrix with $m_{y_i}$ in $i$-th row. Then note that $S_W = \hat{X}^T\hat{X}$, where $\hat{X} := X-Y$ and $S_B = \hat{Y}^T\hat{Y}$, where $\hat{Y}\in\realmat{l}{d}$ is such that $k$-th row is $\sqrt{n_k}(m_k-m)^T$.

Let $\lambda\ge 0$ be a regularization parameter. Then for $p\le l-1$, the top $p$ discriminant variables $w_1,\ldots,w_p$ are the columns of $W\in\realmat{d}{p}$ which solve the following optimization problem:
\begin{alignat}{3}
{\text{max}}& &&Tr(W^TS_BW) \nonumber\\
\text{subject to}&\hspace{0.5cm} &&W^T (S_W+\lambda I_d) W = I_p. \label{eqn:LDA_problem}
\end{alignat}

The solution of \eqref{eqn:LDA_problem} is not unique because the objective function and constraint are invariant to multiplication by orthonormal matrices. To resolve this issue, we modify the objective to $Tr(W^TS_BWN)$, where $N = diag(\mu_1,\ldots,\mu_p)$ are arbitrary such that $\mu_1>\ldots>\mu_p>0$. The modified objective function $Tr(W^TS_BWN)$ is the \highlight{Brockett cost function}.

The Riemannian formulation of the LDA problem is 
\begin{alignat}{3}
\underset{W\in St_{(S_W+\lambda I_d)}(d,p)}{\text{min}} f(W) := -\dfrac{1}{2} Tr(W^TS_BWN). \label{eqn:Riemannian_LDA_problem}
\end{alignat}
Let $M\in \realmat{d}{d}$ be fixed SPD matrix using which Riemannian metric is defined on $St_{(S_W+\lambda I_d)}(d,p)$. Let $\bar{f}$ be smooth extension of $f$ defined on $\realmat{d}{p}$ where $\bar{f}$ is defined by \eqref{eqn:Riemannian_LDA_problem} as well. For $W\in St_{(S_W+\lambda I_d)}(d,p)$, let $\Proj_W(\cdot)$ denote the projection on $T_W St_{(S_W+\lambda I_d)}(d,p)$. The expression for Riemannian gradient is 
\begin{equation}
\grad f(W) = \Proj_W (M^{-1}\nabla \bar{f}(W)) = -\Proj_W (M^{-1}S_B WN).
\end{equation}

We denote the eigenvalues of the matrix pencil $(S_B,S_W+\lambda I_d)$ by $\rho_1\ge \rho_2\ge\ldots\ge \rho_d\ge 0$.

\begin{proposition}[\highlight{Critical Points}]
A point $W\in St_{(S_W+\lambda I_d)}(d,p)$ is a critical point of \eqref{eqn:Riemannian_LDA_problem} iff the columns of $W$ are some $p$ generalized eigenvectors of the matrix pencil $(S_B,S_W+\lambda I_d)$.
\end{proposition}

\begin{remark}
Note that $W$ is a critical point w.r.t. every metric on $St_{(S_W+\lambda I_d)}(d,p)$ because 
\begin{equation*}
0 = Tr(\grad\bar{f}(W)^TM\xi) = D\bar{f}(W)\xi = Tr(\grad\bar{f}(W)^TM'\xi) = 0,    
\end{equation*}
for any other fixed SPD matrix $M'\in \realmat{d}{d}$. So we may as well set $M = S_W+\lambda I_d$. Using Remark \ref{remark:unpreconditioned_Riemannian_gradient}, the expression for Riemannian gradient is
\begin{align*}
\grad f(W) &= -[(S_W+\lambda I_d)^{-1}S_BWN - Wsym(W^TS_BWN)]\\ 
&= -[(I_d-WW^T(S_W+\lambda I_d))(S_W+\lambda I_d)^{-1}S_BWN + Wskew(W^TS_BWN)].
\end{align*}
\end{remark}

\begin{proof}
\converse Consider the generalized eigenvalue problem
\begin{equation*}
S_BW = (S_W+\lambda I_d)WA,
\end{equation*}
where $A = diag(\alpha_1,\ldots,\alpha_p)$ are the generalized eigenvalues and the columns of $W$ are the generalized eigenvector of the matrix pencil $(S_B,S_W+\lambda I_d)$. Let $W\in St_{(S_W+\lambda I_d)}(d,p)$, then we have
\begin{align*}
\grad f(W) &= -[(S_W+\lambda I_d)^{-1}S_BWN - Wsym(W^TS_BWN)]\\
&= -[WAN - Wsym(W^T(S_W+\lambda I_d)WAN)] \\
&= -[WAN - Wsym(AN)] = 0.
\end{align*}
\forward Suppose for some $W\in St_{(S_W+\lambda I_d)}(d,p)$, we have \begin{equation*}
\grad f(W) = [(I_d-WW^T(S_W+\lambda I_d))(S_W+\lambda I_d)^{-1}S_BWN + Wskew(W^TS_BWN)] = 0.
\end{equation*}
Note the the columns of the first term in the above expression lie in the orthogonal complement of $span(W)$ and those of second term lie in $span(W)$, thus we have
\begin{align}
(I_d-WW^T(S_W+\lambda I_d))(S_W+\lambda I_d)^{-1}S_BWN &= 0,\label{eqn:grad_perp_W}\\  Wskew(W^TS_BWN) &= 0.\label{eqn:grad_in_W}
\end{align}
From \eqref{eqn:grad_perp_W}, we get
\begin{equation*}
(S_W+\lambda I_d)^{-1}S_BW = W(W^TS_BW),
\end{equation*}
since $N$ is invertible. Also since $W\in St_{(S_W+\lambda I_d)}(d,p)$ it is a full column rank matrix, then by \eqref{eqn:grad_in_W} we have
\begin{equation*}
skew(W^TS_BWN) = 0,
\end{equation*}
which means that $W^TS_BWN$ is symmetric, i.e., $(W^TS_BW)N = N(W^TS_BW)$. This happens only when $W^TS_BW$ is itself diagonal, since it commutes with a diagonal matrix with distinct diagonal entries. Finally we get
\begin{equation*}
(S_W+\lambda I_d)^{-1}S_BW = WD,
\end{equation*}
where $D = W^TS_BW$ which is a diagonal matrix. This implies that the columns of $W$ corresponds to some $p$ generalized eigenvectors of the matrix pencil $(S_B,S_W+\lambda I_d)$.
\end{proof}

\begin{remark}
In the above proposition, we have proved that the critical points of $f$ are matrices $W\in St_{(S_W+\lambda I_d)}(d,p)$ such that the columns are $p$ generalized eigenvectors of the matrix pencil $(S_B,S_W+\lambda I_d)$. Consequently, of all the possible critical points, the optimal solutions are only those $W\in St_{(S_W+\lambda I_d)}(d,p)$ such that the columns are the $p$-dominant generalized eigenvectors of the matrix pencil $(S_B,S_W+\lambda I_d)$. Moreover, if we assume that $\rho_1>\rho_2>\ldots>\rho_{p+1}\ge 0$, then for the aforementioned $W\in St_{(S_W+\lambda I_d)}(d,p)$ that solve \eqref{eqn:Riemannian_LDA_problem} are unique up the the signs of the columns of $W$.
\end{remark}

\begin{proposition}
Let $\mapping{F}{St_{(S_W+\lambda I_d)}(d,p)}{St_{(S_W+\lambda I_d)}(d,p)}$ be a descent mapping for $f$ and assume that, for every $W\in St_{(S_W+\lambda I_d)}(d,p)$, all the accumulation points of $\{F^{(k)}(W)\}_{k=1,2,\ldots}$ are critical points of $f$. Assume that for every critical point $W^*\in St_{(S_W+\lambda I_d)}(d,p)$ of $f$, $\underset{W\rightarrow W^*}{Lt}dist(F(W),W) = 0$ and that $\rho_1>\rho_2>\ldots>\rho_{p+1}\ge 0$, i.e., the $p$-dominant generalized eigenvalues of the matrix pencil $(S_B,S_W+\lambda I_d)$ are nondegenerate, then $W\in St_{(S_W+\lambda I_d)}(d,p)$ such that the columns are the $p$-dominant generalized eigenvectors of the matrix pencil $(S_B,S_W+\lambda I_d)$ are asymptotically stable. Furthermore, critical points which are not a local minimum of Problem \eqref{eqn:Riemannian_LDA_problem} are unstable.
\end{proposition}

\begin{proof}
To prove the asymptotic stability of $W\in St_{(S_W+\lambda I_d)}(d,p)$ such that the columns are $p$-dominant generalized eigenvectors of the matrix pencil $(S_B, S_W + \lambda I_d)$ we use Prop. \ref{prop:capture_theorem}. The assumption that the $p$-dominant generalized eigenvalues of the matrix pencil $(S_B, S_W + \lambda I_d)$ are simple implies that for the aforementioned $W\in St_{(S_W+\lambda I_d)}(d,p)$, the columns belong each to a one dimensional generalized eigenspace. Thus, in such case points $W$ that solve Problem \eqref{eqn:Riemannian_LDA_problem} are unique up the the signs of the columns of $W$, making these point isolated global (and consequently local) minimizers of $f$ on $St_{(S_W+\lambda I_d)}(d,p)$. According to Prop. \ref{prop:capture_theorem}, such points $W$ are asymptotically stable.

Suppose $W\in St_{(S_W+\lambda I_d)}(d,p)$ is a critical point of $f$ which is not a local minimum. Then, there exists compact neighborhoods with either no other critical points, if there are no
multiplicities of the generalized eigenspaces, or where all other critical point achieve the same value for the cost function, if there are multiplicities of the generalized eigenspaces. Thus, according to Prop. \ref{prop:unstable_fixed_points}, such $W$ are unstable.
\end{proof}

\begin{proposition}
Consider using Riemannian optimization to solve \eqref{eqn:Riemannian_LDA_problem}, where we use the Riemannian metric defined by an SPD matrix $M\in\realmat{d}{d}$, which is a given preconditioner matrix. Assume that that $\rho_1>\rho_2>\ldots>\rho_{p+1}\ge 0$ and that $S_W+\lambda I_d$ is a SPD matrix. Let $W^*$ denote the global minimizer of \eqref{eqn:Riemannian_LDA_problem}. Then,
\begin{equation*}
\kappa(\hess f(W^*)) \le \kappa^*\cdot \kappa(S_W+\lambda I_d, M)
\end{equation*}
where 
\begin{equation*}
\kappa^* := \dfrac{\mu_1(\rho_1-\rho_d)}{\text{min}\{\mu_p(\rho_p-\rho_{p+1}),{\text{min}}_{1\le j\le p}\{\frac{1}{2}(\mu_j-\mu_{j+1})(\rho_j-\rho_{j+1})\}\}}
\end{equation*}
and $\mu_1>\ldots>\mu_p>0$ are diagonal elements of $N$.

If we further assume that for all $i = 1,\ldots, d$ the values $\rho_i$ are distinct, then the critical point $W^*$ is the only (strict) local minimizer of \eqref{eqn:Riemannian_LDA_problem} (up to the signs of the columns of $W^*$), and all other critical points are either saddle points or strict local maximizers. Thus, $W^*\in St_{(S_W+\lambda I_d)}(d,p)$ is the only asymptotically stable critical point, and all other critical points are unstable.
\end{proposition}

\subsection{Canonical Correlation Analysis}

Let $X\in \realmat{n}{d_x}$ and $Y\in \realmat{n}{d_y}$ be two data matrices, and $\lambda_x,\lambda_y\ge 0$ be two regularization parameter. Let $\Sigma_{xx}:=X^TX+\lambda_x I_{d_x}$, $\Sigma_{yy}:=Y^TY+\lambda_y I_{d_y}$, $\Sigma_{xy}:= X^TY$, and $q := max\{rank(\Sigma_{xx}),rank(\Sigma_{yy})\}$. Then for $p\le q$, the \highlight{\bsdnn{(\lambda_x,\lambda_y)} top p-canonical correlations} $\sigma_1\ge \ldots\ge \sigma_p$ and the \highlight{\bsdnn{(\lambda_x,\lambda_y)} top p-canonical weights} $u_1,\ldots,u_p$,  $v_1,\ldots,v_p$, are the solution of the problem:
\begin{alignat*}{3}
{\text{max}}& &&Tr(U^T\Sigma_{xy}V) \\
\text{subject to}&\hspace{0.5cm} &&U^T\Sigma_{xx}U = I_p \\
& &&V^T\Sigma_{yy}V = I_p\\
& &&U^T\Sigma_{xy}V = diag(\sigma_1,\ldots,\sigma_p),
\end{alignat*}
where $U = [u_1 \ldots u_p]\in\realmat{d_x}{p}$ and $V = [v_1 \ldots v_p]\in\realmat{d_y}{p}$.

Without the constraint that $U^T\Sigma_{xy}V$ be diagonal, the constraint set is product of two generalized Stiefel manifolds. But then the solution is not unique because the objective function and other constraints are invariant to multiplication by orthonormal matrices, which leads to additional steps to extract canonical correlations. To resolve this issue, we make the following modification:
\begin{alignat}{3}
{\text{max}}& &&Tr(U^T\Sigma_{xy}VN) \nonumber\\
\text{subject to}&\hspace{0.5cm} &&U^T\Sigma_{xx}U = I_p \nonumber\\
& &&V^T\Sigma_{yy}V = I_p,\label{eqn:CCA_problem}
\end{alignat}
where $N = diag(\mu_1,\ldots,\mu_p)$ is arbitrary, such that $\mu_1>\ldots,\mu_p>0$.

Let $\Sprod:= St_{\Sigma_{xx}}(d_x,p)\times St_{\Sigma_{yy}}(d_y,p)$. Then we can rewrite \eqref{eqn:CCA_problem} as 
\begin{equation}
\underset{(U,V)\in\Sprod}{min} f(U,V) := -Tr(U^T\Sigma_{xy}VN).\label{eqn:Riemannian_CCA_problem}
\end{equation}
Denote $d = d_x+d_y$ and 
\begin{equation*}
    Z = \left[\begin{array}{c}U\\V\end{array}\right]\in\realmat{d}{p},
\end{equation*}
then the objective function can be compactly written as 
\begin{equation}
    f(Z) = -\dfrac{1}{2} Tr\Bigg(Z^T
    \left[\begin{array}{cc}
        0 & \Sigma_{xy} \\
        \Sigma_{xy}^T & 0
    \end{array}\right]ZN
    \Bigg).\label{eqn:CCA_objective}
\end{equation}
Let $\bar{f}$ be smooth extension of $f$ defined on $\realmat{d}{p}$ where $\bar{f}$ is defined by \eqref{eqn:CCA_objective} as well. 

Let $M_{xx}$ and $M_{yy}$ be fixed SPD matrices using which Riemannian metric is defined on $St_{\Sigma_{xx}}(d_x,p)$ and $St_{\Sigma_{yy}}(d_y,p)$ respectively. Let $M = diag(M_{xx},M_{yy})$, and $\Proj_U(\cdot)$ denote the orthogonal projection onto $T_USt_{\Sigma_{xx}}(d_x,p)$, and similarly $\Proj_V(\cdot)$ denote the orthogonal projection onto $T_VSt_{\Sigma_{yy}}(d_y,p)$. Given $Z\in\Sprod$, the orthogonal projection onto the tangent space $T_Z\Sprod$ is
\begin{equation*}
\Proj_Z(\xi_Z) = \left[\begin{array}{c}\Proj_U(\xi_U)\\\Proj_V(\xi_V)\end{array}\right], \text{ where } \xi_Z = \left[\begin{array}{c}\xi_U\\\xi_V\end{array}\right]\in\realmat{d}{p}.
\end{equation*}

The Riemannian gradient of $f$ is given by
\begin{equation}
\grad f(Z) = \Proj_Z(M^{-1}\nabla \bar{f}(Z)) = -
\left[\begin{array}{c}\Proj_U(M_{xx}^{-1}\Sigma_{xy}VN)\\\Proj_V(M_{yy}^{-1}\Sigma_{xy}^TUN)\end{array}\right].
\end{equation}

\begin{proposition}[\highlight{Critical Points}]
A point $Z = (U,V)\in\Sprod$ is a critical point of \eqref{eqn:Riemannian_CCA_problem} iff the columns of $(\Tilde{U},\Tilde{V}) = (\Sigma_{xx}^{1/2}U,\Sigma_{yy}^{1/2}V)$ are $p$ left and right singular vectors of corresponding singular matrix $R:= \Sigma_{xx}^{1/2}\Sigma_{xy}\Sigma_{yy}^{1/2}$.
\end{proposition}

\begin{proposition}
Let $\mapping{F}{\Sprod}{\Sprod}$ be a descent mapping for $f$ and assume that, for every $Z\in \Sprod$, all the accumulation points of $\{F^{(k)}(Z)\}_{k=1,2,\ldots}$ are critical points of $f$. Assume that for every critical point $Z^*\in \Sprod$ of $f$, $\underset{Z\rightarrow Z^*}{Lt}dist(F(Z),Z) = 0$ and that $\sigma_1>\sigma_2>\ldots>\sigma_{p+1}\ge 0$, i.e., the $p$-dominant singular values of the matrix $R$ are nondegenerate, then $Z = (U,V)\in \Sprod$ such that the columns of $(\Tilde{U},\Tilde{V}) = (\Sigma_{xx}^{1/2}U, \Sigma_{yy}^{1/2}V)$ are the $p$-dominant left and right singular vectors of the matrix $R$ on the same phase are asymptotically stable. Furthermore, critical points which are not a local minimum of Problem \eqref{eqn:Riemannian_CCA_problem} are unstable.
\end{proposition}

\begin{proposition}
Consider using Riemannian optimization to solve \eqref{eqn:Riemannian_CCA_problem}, where we use the Riemannian metric defined by an SPD matrix $M = diag(M_{xx},M_{yy})$, where $M_{xx}\in\realmat{d_x}{d_x}$ and $M_{yy}\in\realmat{d_y}{d_y}$, which are given preconditioner matrices. Assume that that $\sigma_1>\sigma_2>\ldots>\sigma_{p+1}\ge 0$ and that $\Sigma$ is a SPD matrix. Let $Z^*$ denote the global minimizer of \eqref{eqn:Riemannian_CCA_problem}. Then,
\begin{equation*}
\kappa(\hess f(Z^*)) \le \kappa^*\cdot \kappa(\Sigma, M)
\end{equation*}
where 
\begin{equation*}
\kappa^* := \dfrac{\text{max}\{\mu_1(\sigma_1+\sigma_{p+1}),\frac{1}{2}(\mu_1+\mu_2)(\sigma_1+\sigma_{2})\}}{\text{min}\{\mu_p(\sigma_p-\sigma_{p+1}),{\text{min}}_{1\le j\le p}\{\frac{1}{2}(\mu_j-\mu_{j+1})(\sigma_j-\sigma_{j+1})\}\}}
\end{equation*}
and $\mu_1>\ldots>\mu_p>0$ are diagonal elements of $N$.

If we further assume that for all $i = 1,\ldots, q$ the values $\sigma_i$ are distinct, then the critical point $Z^*$ is the only (strict) local minimizer of \eqref{eqn:Riemannian_CCA_problem} (up to the signs of the columns of $U^*$ and $V^*$), and all other critical points are either saddle points or strict local maximizers. Thus, $Z^*\in \Sprod$ is the only asymptotically stable critical point, and all other critical points are unstable.
\end{proposition}


\section{Appendix}

\subsection{Non-Linear Programming}

\begin{definition}[\highlight{LICQ}]
Given the point $x$, we say that the linear independence constraint qualification (LICQ) holds if the set of constraint gradients $\{\nabla h_i(x):\,i = 1,\ldots,p\}$ is linearly independent.
\end{definition}

\begin{proposition}[\highlight{KKT conditions}]
Suppose that $x^*$ is a local solution of \eqref{eqn:optimization_problem}, that the functions $f$ and $h$ are continuously differentiable, and that LICQ holds at $x^*$. Then there is a Lagrange multiplier vector $\lambda^*$ such that the following conditions are satisfied at $(x^*,\lambda^*)$
\begin{equation}
\nabla_x \mathcal{L}(x^*,\lambda^*) = 0,\qquad h(x^*) = 0. 
\end{equation}
\end{proposition}

\begin{proposition}\label{prop:nonsingular_jacobian}
Suppose Assumptions \ref{assume:nonsingular_jacobian} hold at $(x,\lambda)$. Then the KKT matrix 
\begin{equation*}
\nabla F(x,\lambda) = \left[
\begin{array}{cc} 
\nabla^2_{xx} \mathcal{L}(x,\lambda) & -\nabla h(x) \\ 
\nabla h(x)^T & 0
\end{array} \right]
\end{equation*}
is nonsingular, and hence there is a unique pair $(p_x^*,p_\lambda^*)$ satisfying the Newton-KKT system \eqref{eqn:Newton_KKT}.
\end{proposition}

\begin{proof}
We show that $\nabla F(x,\lambda) p = 0$ implies $p = 0$. Suppose that
\begin{equation}
\left[
\begin{array}{cc} 
\nabla^2_{xx} \mathcal{L}(x,\lambda) & -\nabla h(x) \\ 
\nabla h(x)^T & 0
\end{array} \right]
\left[\begin{array}{c} w \\ v \end{array}\right] = 0.\label{eqn:proof_nonsingular}
\end{equation}
Then from the second equation of \eqref{eqn:proof_nonsingular} we have $h(x)^Tw = 0$. Also note that
\begin{equation*}
0 = \left[\begin{array}{c} w \\ v \end{array}\right]
\left[
\begin{array}{cc} 
\nabla^2_{xx} \mathcal{L}(x,\lambda) & -\nabla h(x) \\ 
\nabla h(x)^T & 0
\end{array} \right]
\left[\begin{array}{c} w \\ v \end{array}\right] = w^T\nabla_{xx}^2\mathcal{L}(x,\lambda)w.
\end{equation*}
Therefore for a vector $w$ such that $h(x)^Tw = 0$, we have $w^T\nabla^2_{xx} \mathcal{L}(x,\lambda)w = 0$. Then by Assumption \ref{assume:nonsingular_jacobian}(b), we conclude that $w = 0$. Now consider the first equation in \eqref{eqn:proof_nonsingular}, $\nabla^2_{xx} \mathcal{L}(x,\lambda)w-\nabla h(x)v = 0$. But $w = 0$ which implies that $\nabla h(x) v = 0$. The LICQ assumption implies that $v = 0$.
\end{proof}

\begin{proposition}\label{prop:uniqueness_QP}
Suppose Assumptions \ref{assume:nonsingular_jacobian} hold at $(x,\lambda)$. Then the vector $p^*$ satisfying \eqref{eqn:SQP_KKT_matrix_form} is the unique global solution of \eqref{eqn:SQP_model}.
\end{proposition}

\begin{proof}
First note that $p^*$ is also a feasible point, i.e., $\nabla h(x)^Tp^*_x + h(x) = 0$. Let $p_x$ be any other feasible point (satisfying $\nabla h(x)^Tp + h(x) = 0$), and define $w = p^*-p$. Then $\nabla h(x)^Tw = \nabla h(x)^T(p^*-p) = 0$. By substituting the objective in \eqref{eqn:SQP_model}, we obtain
\begin{align}
q(p) =&\; \frac{1}{2}(p^*-w)^T\nabla_{xx}^2\mathcal{L}(x,\lambda)(p^*-w)+\nabla f(x)^T(p^*-w)+f(x)\nonumber \\
=&\; \frac{1}{2}w^T\nabla_{xx}^2\mathcal{L}(x,\lambda)w - w^T\nabla_{xx}^2\mathcal{L}(x,\lambda)p^* - \nabla f(x)^Tw + q(p^*).\label{eqn:optimal_value_plus_residue}
\end{align}
Since $(p^*,l^*)$ satisfy the KKT conditions in \eqref{eqn:SQP_KKT_matrix_form}, we have $\nabla_{xx}^2\mathcal{L}(x,\lambda)p^* = -\nabla f(x)+\nabla h(x)l^*$, so from $\nabla h(x)^Tw = 0$ we have that
\begin{equation*}
w^T\nabla_{xx}^2\mathcal{L}(x,\lambda)p^* = w^T(-\nabla f(x)+\nabla h(x)l^*) = -w^T\nabla f(x).
\end{equation*}
By substituting this relation into \eqref{eqn:optimal_value_plus_residue}, we obtain
\begin{equation*}
q(p) = \frac{1}{2}w^T\nabla_{xx}^2\mathcal{L}(x,\lambda)w + q(p^*).
\end{equation*}
By Assumption \ref{assume:nonsingular_jacobian}(b), we have $\frac{1}{2}w^T\nabla_{xx}^2\mathcal{L}(x,\lambda)w>0$ except when $w = 0$, i.e., $p = p^*$. So we have $q(p)>q(p^*)$ when $p\neq p^*$, thus $p^*$ is the unique global minimizer of \eqref{eqn:SQP_model}.
\end{proof}

\begin{proposition}[\highlight{Lojasiewicz inequality}] \label{prop:Lojasiewicz_inequality}
Consider an open set $U\subseteq \R^n$ and an analytic function $\mapping{f}{U}{\R}$. For every critical point $x^*\in U$ of $f$ there is a neighborhood $V$ of $x^*$, an exponent $\mu\in [0,1)$ and a constant $c>0$ such that for all $x\in V$
\begin{equation*}
\|\nabla f(x)\| \ge c|f(x) - f(x^*)|^\mu.
\end{equation*}
\end{proposition}

\begin{definition}[\highlight{Polyak-Lojasiewicz inequality}] \label{def:pl_inequality}
A function $\mapping{f}{U}{\R}$ that is differentiable satisfies Polyak-Lojasiewicz (PL) inequality if there exists a positive scalar $c > 0$ such that for all $x\in U$
\begin{equation*}
\dfrac{1}{2}\|\nabla f(x)\|^2 \ge c(f(x) - f(x^*)),
\end{equation*}
where $x^*$ is the global minimizer of $f$.
\end{definition}

\subsection{Differential Geometry} \label{section:diffgeo}

\subsubsection{General Theory}

\begin{definition}
A bijection $\varphi$ of a subset $U$ of $\M$ onto an open subset of $\R^d$ is called a \highlight{\bsdnn{d}-dimensional chart} of the set $\M$, denoted by $(U,\varphi)$.
\end{definition}

Given a chart $(U,\varphi)$ of $\M$ and $x\in U$, the $d$-tuple of elements $\varphi(x)\in \R^d$ are called the \highlight{coordinates of \bsdnn{x}} in the chart $(U,\varphi)$.

\begin{remark}
In the following, whenever we use a specific chart in the definition, it can be show that the definition is independent of the chart chosen.
\end{remark}

\begin{definition}
A \highlight{(\bsdnn{\mathcal{C}^\infty} or smooth) atlas} of $\M$ into $\R^d$ is a collection of charts $(U_\alpha,\varphi_\alpha)$ of the set $\M$ such that $\cup_\alpha U_\alpha = \M$ and for any pair $\alpha,\beta$ with $U_\alpha\cap U_\beta \neq \emptyset$, the sets $\varphi_\alpha(U_\alpha\cap U_\beta)$ and $\varphi_\beta(U_\alpha\cap U_\beta)$ are open sets in $\R^d$ and the change of coordinates $\mapping{\varphi_\beta\circ \varphi_\alpha^{-1}}{\R^d}{\R^d}$ is smooth on its domain $U_\alpha\cap U_\beta$.
\end{definition}

\begin{definition}
Given a maximal atlas $\mathcal{A}^+$ on a set $\M$, the topology generated by taking chart domains of $\mathcal{A}^+$ as the basis elements is called the \highlight{atlas topology} on $\M$.
\end{definition}

\begin{definition}
A \highlight{(\bsdnn{\mathcal{C}^\infty} or smooth) manifold} is a pair $(\M, \mathcal{A}^+)$ consisting of a set $\M$ and a maximal atlas $\mathcal{A}^+$ on $\M$ such that the atlas topology is Hausdorff and second-countable.
\end{definition}

\begin{definition}\label{def:smooth_maps}
Let $\M_1$ and $\M_2$ be manifolds of dimensions $d_1$ and $d_2$ respectively, and let $\mapping{F}{\M_1}{\M_2}$ be any map. We say that $F$ is a \highlight{smooth map} if for every $p\in \M_1$, there exists smooth chart $(U,\varphi)$ containing $p$ and $(V,\psi)$ containing $F(p)$ such that $F(U)\subseteq V$ and the composite map  $\hat{F} = \mapping{\psi\circ F\circ \varphi^{-1}}{\varphi(U)\subseteq \R^{d_1}}{\psi(V)\subseteq \R^{d_2}}$, called the \highlight{coordinate representation of \bsdnn{F}}, is smooth. We denote the set of all smooth real-valued functions from an open subset $U$ of $\M$ by $\mathfrak{F}(U)$.
\end{definition}

Let $x$ be a point on a $d$-dimensional manifold $\M$. Consider the set $C_x$ of smooth curves on $\M$ passing through $x$ at $t=0$:
\begin{equation*}
    C_x = \{ c:\, \mapping{c}{I\subseteq \R}{\M}\text{ is smooth and }c(0) = x \},
\end{equation*}
where smoothness of $c$ on an open interval $I\subseteq \R$ around $0$ is to be understood through Def. \ref{def:smooth_maps}. We define an equivalence relation on $C_x$, denoted by $\sim$. Let $(U,\varphi)$ be a chart of $\M$ around $x$ and consider $c_1,c_2\in C_x$. Then we have
\begin{equation*}
    c_1\sim c_2 \Longleftrightarrow \dfrac{d}{dt}\bigvert{(\varphi\circ c_1)(t)}{t=0} = \dfrac{d}{dt}\bigvert{(\varphi\circ c_2)(t)}{t=0}.
\end{equation*}
The equivalence class $[c]$ of a curve $c\in C_x$ is the set $[c] = \{\bar{c}\in C_x:\, c\sim \bar{c}\}.$

\begin{definition}
Let $x\in \M$ be a point on the manifold. A \highlight{tangent vector} to $\M$ at $x$ is defined to be an equivalence class $[c]$, for some $c\in C_x$. The \highlight{tangent space} to $\M$ at $x$, denoted by $T_x\M$, the quotient set, i.e., the set of all equivalence classes:
\begin{equation*}
T_x\M = C_x/\sim\;= \{[c]:\, c\in C_x\}.
\end{equation*}
\end{definition}

Given a chart $(U,\varphi)$ around $x\in \M$, define the map $\theta^\varphi_x$ as
\begin{equation*}
\mapping{\theta^\varphi_x}{T_x\M}{\R^d}:\,[c]\mapsto \theta^\varphi_x([c]) = \dfrac{d}{dt}\bigvert{(\varphi\circ c)(t)}{t=0}.
\end{equation*}
Then we define a linear structure on $T_x\M$, by exploiting the linear structure of $\R^d$:
\begin{equation}\label{eqn:linear_structure_on_tangent_space}
a\cdot [c_1]+b\cdot [c_2] \equiv (\theta^\varphi_x)^{-1}(a\, \theta^\varphi_x([c_1])+b\, \theta^\varphi_x([c_2])).
\end{equation}

\begin{proposition}
Tangent spaces are linear space of dimension dim$\,\M$, with the linear structure given through \eqref{eqn:linear_structure_on_tangent_space}.
\end{proposition}

\begin{definition}
The \highlight{tangent bundle} of a manifold $\M$ is the set:
\begin{equation*}
T\M = \{ (x,v):\, x\in \M\text{ and }v\in T_x\M \}.
\end{equation*}
The projection $\mapping{\pi}{T\M}{\M}$ extracts the foot of a tangent vector, i.e., $\pi(x,v) = x$.
\end{definition}

\begin{proposition}
For any manifold $\M$ of dimension $d$, the tangent bundle $T\M$ is itself a manifold of dimension $2d$, in such a way that the projection $\mapping{\pi}{T\M}{\M}$ is smooth.
\end{proposition}

\begin{definition}
Given manifolds $\M$ and $\N$, the \highlight{differential} of a smooth map $\mapping{F}{\M}{\N}$ at $x$ is a linear operator $\mapping{DF(x)}{T_x\M}{T_{F(x)\N}}$ defined by:
\begin{equation*}
DF(x)[v] = [t\mapsto F(c(t))],
\end{equation*}
where $c$ is a smooth curve on $\M$ passing through $x$ at $t = 0,$ such that $v = [c]$.
\end{definition}

\begin{proposition}
Consider a smooth map $\mapping{F}{\M}{\N}$ and its differential $\mapping{DF}{T\M}{T\N}$ defined by $DF(x,v) = DF(x)[v]$. With the natural smooth structures on $T\M$ and $T\N$, the map $DF$ is smooth.
\end{proposition}

\begin{definition}
A \highlight{vector field} $V$ is a map from $\M$ to $T\M$ such that $\pi\circ V$ is the identity map. The set of all smooth vector fields on $\M$ are denoted by $\mathfrak{X}(\M)$.
\end{definition}

Let $V$ be a vector field on $\M$. We define the action of $V$ on a smooth function $f\in \mathcal{F}(U)$ with $U$ open in $\M$ as the function $\mapping{Vf}{U}{\R}$ defined by $(Vf)(x) = Df(x)[V(x)].$

\begin{proposition}
Let $V$ be a vector field on $\M$. Then the following statements are equivalent:
\begin{enumerate}[(a)]
    \item $V$ is smooth.
    \item For every chart $(U,\varphi)$ of $\M$, the map $x\mapsto \theta_x^\varphi(V(x))$ is smooth on $U$.
    \item $Vf$ is smooth for all $f\in \mathcal{F}(U)$.
\end{enumerate}
\end{proposition}

\begin{definition}\label{def:retraction}
A \highlight{retraction} on $\M$ is a smooth map $\mapping{R}{T\M}{\M}$ with the following properties. For each $x\in \M$, let $\mapping{R_x}{T_x\M}{\M}$ be the restriction of $R$ at $x$, so that $R_x(v) = R(x,v)$. Then,
\begin{enumerate}[(a)]
    \item $R_x(0) = 0$, and
    \item $\dfrac{d}{dt}\bigvert{R_x(tv)}{t = 0} = v,$ for all $v\in T_x\M$ or equivalently, $DR_x(0) = Id_{T_x\M}$.
\end{enumerate}
\end{definition}

\begin{definition}
An inner product on $T_x\M$ is a bilinear, symmetric, positive definite function $\mapping{\innerproduct{\cdot}{\cdot}_x}{T_x\M\times T_x\M}{\R}$. It induces a norm for the tangent vectors: $\|u\|_x = \sqrt{\innerproduct{u}{u}_x}$. A \highlight{metric} on $\M$ is a choice of inner product $\innerproduct{\cdot}{\cdot}_x$ for each $x\in \M$. 
\end{definition}

\begin{definition}
A metric $\innerproduct{\cdot}{\cdot}_x$ on $\M$ is a \highlight{Riemannian metric} if it varies smoothly with $x$, in the sense that if $V,W$ are two smooth vector fields on $\M$ then the function $x\mapsto \innerproduct{V(x)}{W(x)}_x$ is smooth from $\M$ to $\R$. A manifold with a Riemannian metric is a \highlight{Riemannian manifold}.
\end{definition}

\begin{definition}
Let $\mapping{f}{\M}{\R}$ be a smooth function on a Riemannian manifold $\M$. The \highlight{Riemannian gradient} of $f$ is the vector field $\grad f$ on $\M$ uniquely defined by:
\begin{equation*}
    Df(x)[v] = \innerproduct{v}{\grad f(x)}_x, \qquad \forall\,(x,v)\in T\M.
\end{equation*}
\end{definition}

\begin{proposition}
For $f\in \mathcal{F}(\M)$, where $\M$ is a Riemannian manifold, $\grad f$ is smooth.
\end{proposition}

\begin{definition}
The \highlight{Lie bracket} of smooth vector fields $U,V\in \mathfrak{X}(\M)$ on a manifold $\M$ is a map $\mapping{[U,V]}{\mathfrak{F}(\M)}{\mathfrak{F}(\M)}$ such that $[U,V]f = U(Vf) - V(Uf)$.
\end{definition}

\begin{proposition}
Let $U,V\in \mathfrak{X}(M)$ on a manifold $\M$. There exists a unique smooth vector field $W$ on $\M$ such that $[U,V]f = Wf$ for all $f\in \mathfrak{F}(\M)$. We identify $[U,V]$ with that smooth vector field.
\end{proposition}

\begin{definition}
An \highlight{(affine) connection} on $\M$ is an operator:
\begin{equation*}
\mapping{\nabla}{\mathfrak{X}(\M)\times \mathfrak{X}(\M)}{\mathfrak{X}(\M)}: (U,V)\mapsto \nabla_V U
\end{equation*}
which satisfies the following three properties for arbitrary $U,V,W\in \mathfrak{X}(\M)$, $f,g\in\mathfrak{F}(\M)$ and $a,b\in \R$:
\begin{enumerate}[(a)]
    \item \highlight{$\mathfrak{F}(\M)$-linearity in \bsdnn{U}}: $\nabla_{fV+gW}U = f\nabla_V U + g\nabla_W U$;
    \item \highlight{${\R}$-linearity in \bsdnn{V}}: $\nabla_U(aV+bW) = a\nabla_U V+b\nabla_U W$; and
    \item \highlight{Leibniz rule}: $\nabla_U (fV) = (Uf)V + f\nabla_U V$.
\end{enumerate}
The field $\nabla_U V$ is the \highlight{covariant derivative} of $V$ along $U$ with respect to $\nabla$. 
\end{definition}

\begin{proposition}
On a Riemannian manifold $\M$, there exists a unique connection $\nabla$ which satisfied two additional properties for all $U,V,W\in\mathfrak{X}(\M)$:
\begin{enumerate}[(d)]
    \item \highlight{Symmetry}: $[U,V] = \nabla_U V - \nabla_V U$; and
    \item \highlight{Compatibility with the metric}: $U\innerproduct{V}{W} = \innerproduct{\nabla_U V}{W} + \innerproduct{V}{\nabla_U W}$.
\end{enumerate}
This connection is called the \highlight{Levi-Civita} or \highlight{Riemannian connection}.
\end{proposition}

\begin{proposition}
For any connection $\nabla$ and smooth vector fields $U, V$ on manifold $\M$, the vector field $\nabla_U V$ at $x$ depends on $U$ only through $U(x)$. Thus, we can write $\nabla_u V$ to mean $(\nabla_U V)(x)$ for any $U\in\mathfrak{X}(\M)$ such that $U(x) = u$, without ambiguity.
\end{proposition}

\begin{definition}
Let $\M$ be a Riemannian manifold with its Riemannian connection $\nabla$. The \highlight{Riemannian Hessian} of $f\in \mathfrak{F}(\M)$ at $x\in\M$ is a linear operator $\mapping{\hess f(x)}{T_x\M}{T_x\M}$ defined as follows:
\begin{equation*}
    \hess f(x)[u] = \nabla_u \grad f.
\end{equation*}
Equivalently, $\hess f$ maps $\mathfrak{X}(\M)$ to $\mathfrak{X}(\M)$ as $\hess f[U] = \nabla_U\grad f$.
\end{definition}

\begin{proposition}
The Rimennian Hessian is self-adjoint w.r.t. the Riemannian metric. That is, for all $x\in \M$ and $u,v\in T_x\M$,
\begin{equation*}
\innerproduct{\hess f(x)[u]}{v}_x = \innerproduct{u}{\hess f(x)[v]}_x.
\end{equation*}
\end{proposition}

\begin{definition}
Given a smooth curve $\mapping{c}{I}{\M}$ on a manifold $\M$, the map $\mapping{Z}{I}{T\M}$ is a \highlight{smooth vector field on \bsdnn{c}} if $Z(t)$ is in $T_{c(t)}\M$ for all $t\in I$, and if it is smooth as a map from $I$ (open in $\R$) to $T\M$. The set of smooth vector fields on $c$ is denoted by $\mathfrak{X}(c)$.
\end{definition}

\begin{proposition}
Let $\mapping{c}{I}{\M}$ be a smooth curve on a manifold equipped with a connection $\nabla$. There exists a unique operator $\mapping{\dfrac{D}{dt}}{\mathfrak{X}(c)}{\mathfrak{X}(c)}$ satisfying these three properties for all $Y,Z\in\mathfrak{X}(c)$, $U\in\mathfrak{X}(\M)$, $g\in \mathfrak{F}(I)$, and $a,b\in\R$:
\begin{enumerate}[(a)]
    \item \highlight{$\R$-linearity}: $\dfrac{D}{dt}(aY+bZ) = a\dfrac{D}{dt}Y+b\dfrac{D}{dt}Z$;
    \item \highlight{Leibniz rule}: $\dfrac{D}{dt}(gZ) = g'Z + g\dfrac{D}{dt}Z$;
    \item \highlight{Chain rule}: $\bigg( \dfrac{D}{dt}(U\circ c) \bigg)(t) = \nabla_{c'(t)} U$ for all $t\in I$.
\end{enumerate}
This operator is called the \highlight{induced covariant derivative}. Furthermore, if $\M$ is a Riemannian manifold with $\innerproduct{\cdot}{\cdot}$ and $\nabla$ is compatible with the metric (e.g., if it is the Riemannian connection), then the induced covariant derivative also satisfies:
\begin{enumerate}[(d)]
    \item \highlight{Product rule}: $\dfrac{d}{dt}\innerproduct{Y}{Z} = \bigg\langle{\dfrac{D}{dt}Y},{Z}\bigg\rangle+\bigg\langle{Y},{\dfrac{D}{dt}}Z\bigg\rangle$,
\end{enumerate}
where $\innerproduct{Y}{Z}\in\mathfrak{F}(I)$ is defined by $\innerproduct{Y}{Z}(t) = \innerproduct{Y(t)}{Z(t)}_{c(t)}$.
\end{proposition}

\begin{definition}
Let $\mapping{c}{I}{\M}$ be a smooth curve on a manifold $\M$. The \highlight{velocity} of $c$ is a smooth vector field on $c$ defined for all $t\in I$ by $c'(t) = [\tau \mapsto c(t+\tau)]$,
where the brackets on the right-hand side take the equivalence class of the shifted. The \highlight{acceleration} of $c$ is a smooth vector field on $c$ defined as the covariant derivative of the velocity $c'' = \dfrac{D}{dt}c'$. A \highlight{geodesic} is a smooth curve $\mapping{c}{I}{\M}$ such that $c''(t) = 0$ for all $t\in I$.
\end{definition}

\begin{definition}\label{def:vector_transport}
A \highlight{vector transport} on a manifold $\M$ is a smooth mapping
\begin{equation*}
T\M\oplus T\M\rightarrow T\M :\,(\eta_x,\xi_x)\mapsto \vectransport_{\xi_x}(\eta_x)\in T\M
\end{equation*}
satisfying the following properties for all $x\in \M$:
\begin{enumerate}[(a)]
    \item (Associated retraction). There exists a retraction $R$ called the retraction associated with $\vectransport$, such that for all $(\eta_x,\xi_x)\in T\M\oplus T\M$, it holds that $\vectransport_{\xi_x}(\eta_x)\in T_{R_x(\eta_x)}\M$.
    \item (Consistency). $\vectransport_0(\xi_x) = \xi_x$ for all $\xi_x\in T_x\M$.
    \item (Linearity). $\vectransport_{\eta_x}(a\xi_x+b\zeta_x) = a\vectransport_{\eta_x}(\xi_x)+b\vectransport_{\eta_x}(\zeta_x)$.
\end{enumerate}
\end{definition}

\begin{proposition}\label{prop:vector_transport_diff_retraction}
Let $\M$ be a manifold endowed with a retraction $R$. Then a vector transport on $\M$ is defined by 
\begin{equation}
\vectransport_{\eta_x}(\xi_x) := DR_x(\eta_x)[\xi_x] = \dfrac{d}{dt} \bigvert{R_x(\eta_x+t\xi_x)}{t=0}.
\end{equation}
\end{proposition}

\begin{definition}
Suppose $\M$ and $\N$ are manifolds. Given a smooth map $\mapping{F}{\M}{\N}$ and a point $x\in \M$, we define the \highlight{rank of \bsdnn{F} at \bsdnn{x}} to be the rank of the linear map $\mapping{DF(x)}{T_x\M}{T_{F(x)}\N}$. If $F$ has the same rank $r$ at every point, we say that it has \highlight{constant rank}, and write rank$\,F = r$. We say $F$ is a \highlight{submersion} if its differential is surjective at each point in the domain (or equivalently, if rank$\,F =$ dim$\,\N$). It is called an \highlight{immersion} if its differential is injective at each point (equivalently, rank$\,F = $ dim$\,\M$).
\end{definition}

\begin{proposition}
Suppose $\M$ and $\N$ are manifolds of dimension $m$ and $n$, respectively and $\mapping{F}{\M}{\N}$ is a smooth map with constant rank $r$. For each $x\in \M$ there exists charts $(U,\varphi)$ of $x$ for $\M$ and $(V,\psi)$ of $F(x)$ for $\N$ such that $F(U)\subseteq V$, in which $F$ has a coordinate representation of the form
\begin{equation*}
\hat{F}(x^1,\ldots,x^r,x^{r+1},\ldots,x^m) = (x^1,\ldots,x^r,0,\ldots,0).
\end{equation*}
In particular, if $F$ is a submersion, this becomes
\begin{equation*}
\hat{F}(x^1,\ldots,x^n,x^{n+1},\ldots,x^m) = (x^1,\ldots,x^n),
\end{equation*}
and if $F$ is an immersion, it is
\begin{equation*}
\hat{F}(x^1,\ldots,x^m) = (x^1,\ldots,x^m,0,\ldots, 0).
\end{equation*}
\end{proposition}

\subsubsection{Embedded submanifolds}

\begin{definition}
Consider two manifolds $\M,\bar{\M}$ such that $\M\subseteq \bar{\M}$. If the inclusion map $\mapping{\iota}{\M}{\bar{\M}}$ is smooth with constant rank $dim\,\M$ for all $x\in \M$, we say that $\M$ is an \highlight{submanifold} of $\bar{\M}$. If in addition, the atlas topology on $\M$ coincides with the subspace topology of $\M$, then $\M$ is called an \highlight{embedded submanifold} of $\bar{\M}$, while $\bar{\M}$ is called the \highlight{ambient} or \highlight{embedding space}.
\end{definition}

\begin{proposition}
A subset $\M$ of a manifold $\bar{\M}$ is an embedded submanifold of $\bar{\M}$ iff either of the following holds:
\begin{enumerate}[(a)]
    \item $\M$ is an open subset of $\bar{\M}$. Then, we also call $\M$ an \highlight{open submanifold} and $dim\,\M = dim\,\bar{\M}$.
    \item For a fixed integer $k\ge 1$ and for each $x\in \M$ there exists a neighborhood $\bar{U}$ of $x$ in $\bar{\M}$ and a smooth function $\mapping{h}{\bar{U}}{\R^k}$ such that $h^{-1}(0) = \M\cap \bar{U}$ and rank of $h$ at $x$ or $rank\,Dh(x) = k$. Then, $dim\,\M = dim\,\bar{\M} - k$ and $h$ is called a \highlight{local defining function}. The tangent spaces of $\M$ are $T_x\M = ker\,Dh(x)\subseteq T_x\bar{\M}$.
\end{enumerate}
\end{proposition}

\begin{definition}
If $\M$ and $\N$ are manifolds and $\mapping{\Phi}{\M}{\N}$ is a smooth map. Then a point $x\in \M$ is said to be a \highlight{regular point} of $h$ if $h$ is a rank$\,\Phi$ at $x$ is $dim\,\N$, i.e., $D\Phi(x)$ is surjective. A point $c\in \N$ is called a \highlight{regular value} of $\Phi$ if every point of the level set $\Phi^{-1}(c)$ is a regular point, equivalently, for each $x\in \Phi^{-1}(c)$, LICQ holds at $x$.
\end{definition}

\begin{corollary}[\highlight{Submersion Theorem}] \label{prop:submersion_theorem}
Let $\mapping{h}{\bar{\M}}{\N}$ be a smooth map and $c\in \N$ is a regular value of $h$. Then $\M = h^{-1}(c)$ is a closed embedded submanifold of $\bar{\M}$ with dimension $dim\,\M  = dim\bar{\M}-dim\,\N$, and $T_x\M = ker\,Dh(x)$.
\end{corollary}

\begin{corollary}[\highlight{Subimmersion Theorem}] 
Let $\mapping{h}{\bar{\M}}{\N}$ be a smooth map and $\M = h^{-1}(c)$ be a nonempty. If $h$ has constant rank $r$ for all $x$ in a neighborhood of $\M$, then $\M$ is a closed embedded submanifold of $\bar{\M}$ with dimension $dim\,\M  = dim\bar{\M}-r$, and $T_x\M = ker\,Dh(x)$.
\end{corollary}

% \begin{proposition} 
% Let $p$ be a regular value of $h$. Then the level set 
% \begin{equation*}
% \M = h^{-1}(p) = \{ x\in\R^n:\,h(x) = p \}
% \end{equation*}
% is a closed $d$-dimensional submanifold of $\R^n$, where $d = n-p$, with tangent space given by $T_x\M = ker(\nabla h(x)^T)$.
% \end{proposition}

\begin{proposition}
Let $\bar{\M}, \N$ be manifolds and $\M$ be an embedded submanifold of $\bar{\M}$. If $\mapping{\bar{F}}{\bar{\M}}{\N}$ is smooth (at $x\in \M$), then the restriction $F = \bigvert{\bar{F}}{\M}$ is smooth (at $x$). Conversely, if $\mapping{F}{\M}{\N}$ is smooth at $x$, then there exists a neighborhood $\bar{U}$ of $x$ in $\bar{\M}$ and a smooth map $\mapping{\bar{F}}{\bar{U}}{\N}$ such that $\bar{F}(y) = F(y)$ for all $y\in \M\cap \bar{U}$. If $F$ is smooth, then there exists a neighborhood $\bar{U}$ of $\M$ in $\bar{\M}$ and a smooth map $\mapping{\bar{F}}{\bar{U}}{\N}$ such that $F = \bigvert{\bar{F}}{\M}$. Such maps $\bar{F}$ are called \highlight{smooth extensions} of $F$.
\end{proposition}

\begin{proposition}
Let $\bar{\M}, \N$ be manifolds and $\M$ be an embedded submanifold of $\bar{\M}$. A map $\mapping{F}{\N}{\M}$ is smooth (at x) iff $\mapping{\bar{F}}{\N}{\bar{\M}}$, defined by $\bar{F(y)} = F(y)$ for all $y\in \N$, is smooth (at $x$).
\end{proposition}

\begin{proposition}
Let $\innerproduct{\cdot}{\cdot}$ be the Riemannian metric on Riemannian manifold $\bar{\M}$ and $\M$ be an embedded submanifold of $\bar{\M}$. Then the metric on $\M$ defined at each $x$ by restriction, $\innerproduct{u}{v}_x = \innerproduct{u}{v}$, for $u, v \in T_x\M$, is a \highlight{Riemannian metric}. Thus $\M$ is a \highlight{Riemannian submanifold} of $\bar{\M}$.
\end{proposition}

\begin{proposition}
Let $\M$ be a level set of $\mapping{h}{\R^n}{\R^m}$ such that it is an embedded submanifold of $\R^n$. The map $\mapping{\Proj_x}{\R^n}{T_x\M}$ defined as 
\begin{equation*}
\Proj_x = I - \nabla h(x)(\nabla h(x)^T\nabla h(x))^{-1}\nabla h(x)^T    
\end{equation*}
is the orthogonal projector onto the tangent space $T_x\M$ at $x\in \M$.
\end{proposition}

\begin{proposition}
Let $\M$ be a Riemannian submanifold of a Euclidean space $\mathcal{E}$ and $\M$ is endowed with a retraction $R$, then a vector transport on $\M$ is defined by 
\begin{equation}
\vectransport_{\eta_x}(\xi_x) := \Proj_{R_x(\eta_x)}(\xi_x),
\end{equation}
where $\Proj_x$ denotes the orthogonal projector onto $T_x\M$.
\end{proposition}

\begin{proposition}
Let $\M$ be a Riemannian submanifold of $\R^n$ and let $\mapping{f}{\M}{\R}$ be a smooth function. The \highlight{Riemannian
gradient of \bsdnn{f}} is given by $\grad f(x) = \Proj_x(\nabla \bar{f}(x))$, where $\bar{f}$ is any smooth extension of $f$ to a neighborhood of $\M$ in $\R^n$.
\end{proposition}

\begin{proposition}
Let $\M$ be a Riemannian submanifold of $\R^n$. Consider a smooth function $\mapping{f}{\M}{\R}$, and let $\bar{G}$ be a smooth extension of $\grad f$. The \highlight{Riemannian Hessian of \bsdnn{f}} at $x \in\M$ is a linear operator $\mapping{\hess f(x)}{T_x\M}{T_x\M}$ defined as follows:
\begin{equation*}
\hess f(x)[u] = \Proj_x(D\bar{G}(x)[u]), \;\;u\in T_x\M.
\end{equation*}
\end{proposition}

\subsubsection{Quotient Manifolds}

Let $\sim$ be an equivalence relation on a manifold $\bar{\M}$ and let 
\begin{equation*}
\M = \bar{\M}/\sim \,= \{[x]:\,x\in \bar{\M}\}
\end{equation*}
be the resulting quotient set. The \highlight{canonical projection} or \highlight{natural projection} links the total space $\bar{\M}$ to its quotient $\M$:
\begin{equation*}
    \mapping{\pi}{\bar{\M}}{\M}:\,x\mapsto \pi(x) = [x].
\end{equation*}

\begin{definition}
The quotient set $\M$ inherits a topology from $\bar{\M}$ called the \highlight{quotient topology}, turning $\M$ into a \highlight{quotient space}. The topology is defined as follows:
\begin{equation*}
U\subseteq \M \text{ is open } \Longleftrightarrow \pi^{-1}(U)\text{ is open in }\bar{\M}.
\end{equation*}
\end{definition}

\begin{definition}
The quotient set $\M = \bar{\M}/\sim$ equipped with a smooth structure is a \highlight{quotient manifold} of $\bar{\M}$ if the projection $\pi$ is a smooth submersion.
\end{definition}

\begin{proposition}
Let $\M = \bar{\M}/\sim$ be a quotient manifold. For any $x\in \bar{\M}$, the equivalence class $\mathcal{F} = \pi^{-1}(\pi(x))$, also called a \highlight{fiber}, is a closed embedded submanifold of $\bar{\M}$. Its tangent spaces are given by
\begin{equation*}
T_y\mathcal{F} = ker\,D\pi(y)\subseteq T_y\bar{\M}.
\end{equation*}
In particular, $dim\,\mathcal{F} = dim\,\bar{\M}-dim\,\M$.
\end{proposition}

\begin{proposition}
Given a quotient manifold $\M = \bar{\M}/\sim$ with the canonical projection $\pi$ and any manifold $\N$, a map $\mapping{F}{\M}{\N}$ is smooth iff $\mapping{\bar{F}}{\bar{\M}}{\N}$ is smooth.
\end{proposition}

% \begin{proposition}
% For any $x\in \bar{\M}$ there exists a neighborhood $U$ of $[x]$ on the quotient manifold $\M = \bar{\M}/\sim$ and a smooth map $\mapping{\sigma}{U}{\bar{\M}}$ (called a local section) such that $\pi\circ \sigma = Id_U$ and $\sigma([x]) = x$, i.e., $\sigma$ is the smooth right inverse for $\pi$ on $U$.
% \end{proposition}

\begin{proposition}
Let $\mapping{\bar{F}}{\N}{\bar{\M}}$ be a map from one manifold into another, and let $\M = \bar{\M}/\sim$ be a quotient manifold with the projection $\pi$. If $\bar{F}$ is smooth, then $F = \mapping{\pi\circ \bar{F}}{\N}{\M}$ is smooth. Conversely, if $\mapping{F}{\N}{\M}$ is smooth, then for all $[x]\in \M$ there exists a neighborhood $U$ of $[x]$ such that $\bigvert{F}{F^{-1}(U)} = \pi\circ \bar{F}$ for some smooth map $\mapping{\bar{F}}{F^{-1}(U)}{\bar{\M}}$. 
\end{proposition}

\begin{definition}
For a quotient manifold $\M = \bar{\M}/\sim$, the \highlight{vertical space} at $x\in\bar{\M}$ is the subspace
\begin{equation*}
    V_x = T_x\mathcal{F} = ker\,D\pi(x),
\end{equation*}
where $\mathcal{F} = \pi^{-1}(\pi(x))\subseteq \bar{\M}$ is the fiber of $x$. If $\bar{\M}$ is Riemannian, we call the orthogonal complement of $V_x$ the \highlight{horizontal space} at $x$:
\begin{equation*}
H_x = (V_x)^\perp  = \{u\in T_x\bar{\M}:\, \innerproduct{u}{v}_x = 0\text{ for all }v\in V_x\}.
\end{equation*}
Then, $T_x\bar{\M} = V_x\oplus H_x$ is a direct sum of linear spaces.
\end{definition}

For $x\in \bar{\M}$, the vertical space $V_x$ can also be seen as the set of tangent vectors, movements along which keep $x$ in its equivalence class or fiber. And the horizontal space $H_x$ can also be seen as the set of tangent vectors, movements along which move $x$ across equivalence classes or fibers.

\begin{definition}
Consider a point $x\in \bar{\M}$ and a tangent vector $\boldsymbol\xi\in T_{[x]}\M$. The \highlight{horizontal lift} (or simply lift) of $\boldsymbol\xi$ at $x$ is the (unique) horizontal vector $u\in H_x$ such that $D\pi(x)[u] = \boldsymbol\xi$. We write
\begin{equation*}
    u = (\bigvert{D\pi(x)}{H_x})^{-1}[\boldsymbol\xi] = \lift_x(\boldsymbol\xi).
\end{equation*}
We have the following compositions:
\begin{equation*}
D\pi(x)\circ \lift_x = Id\qquad\text{and}\qquad \lift_x\circ D\pi(x) = \Proj_x^H,
\end{equation*}
where $\Proj_x^H$ is the orthogonal projector from $T_x\bar{\M}$ to $H_x$.
\end{definition}

\begin{remark}
It should be noted that the horizontal lift depends on the point at which the vector is lifted, but there is no ambiguity as to which abstract tangent vector it represents. Specifically, for a tangent vector $\boldsymbol\xi\in T_{[x]}\M$, if $x\sim y$, we may consider horizontal lifts $u_x\in H_x$ and $u_y\in H_y$. While $u_x$ and $u_y$ are generally different objects, they represent the same tangent vector of $\M$:
\begin{equation*}
    D\pi(x)[u_x] = \boldsymbol\xi = D\pi(y)[u_y].
\end{equation*}
\end{remark}

We can relate the vector fields on $\M$ to \highlight{horizontal vector fields}, i.e., vector fields on $\bar{\M}$ whose tangent vectors are horizontal.

\begin{definition}
Let $\mapping{V}{\M}{T\M}$ be a vector field on a quotient manifold $\M = \bar{\M}/\sim$. The \highlight{horizontal lift} of $V$ is a (unqiue) horizontal vector field $\bar{V}$ on $\bar{\M}$ defined by 
\begin{equation*}
\bar{V}(x) = \lift_x(V([x]))\text{ or compactly as } \bar{V} = \lift(V).
\end{equation*}
\end{definition}

\begin{proposition}
A vector field $V$ on a quotient manifold $\M = \bar{\M}/\sim$ with canonical projection $\pi$ is related to its horizontal lift $\bar{V}$ by:
\begin{equation*}
V\circ \pi = D\pi\circ \bar{V}.
\end{equation*}
Furthermore, $V$ is smooth on $\M$ iff $\bar{V}$ is smooth on $\bar{\M}$.
\end{proposition}

\begin{proposition}
For a quotient manifold $\M = \bar{\M}/\sim$ with canonical projection $\pi$, consider $V\in \mathfrak{X}(\M)$ and $f\in\mathfrak{F}(\M)$ together with their lifts $\bar{V}\in\mathfrak{X}(\bar{\M})$ and $\bar{f}\in\mathfrak{F}(\bar{\M})$. Then, the lift of $Vf$ is $\bar{V}\bar{f}$, i.e., 
\begin{equation*}
(Vf)\circ \pi = \bar{V}\bar{f}.
\end{equation*}
In words: we may lift then act, or act then lift.
\end{proposition}

\begin{proposition}
If the retraction $\bar{R}$ on the total space $\bar{\M}$ satisfies
\begin{equation*}
x\sim y\implies \bar{R}_x(\lift_x(\boldsymbol\xi)) = \bar{R}_y(\lift_y(\boldsymbol\xi))
\end{equation*}
for all $([x],\boldsymbol\xi)\in T\M$, then the following map defines a retraction on $\M = \bar{\M}/\sim$:
\begin{equation*}
R_{[x]}(\boldsymbol\xi) = [\bar{R}_x(\lift_x(\boldsymbol\xi))].
\end{equation*}
\end{proposition}

\begin{proposition}
If the Riemannian metric on $\bar{\M}$ satisfies
\begin{equation*}
x\sim y\implies \innerproduct{\lift_x(\boldsymbol\xi)}{\lift_x(\boldsymbol\zeta)}_x = \innerproduct{\lift_y(\boldsymbol\xi)}{\lift_y(\boldsymbol\zeta)}_y
\end{equation*}
for all $\boldsymbol\xi,\boldsymbol\zeta\in T_{[x]}\M$. Then the Riemannian metric on the quotient manifold $\M = \bar{\M}/\sim$ is given by 
\begin{equation*}
\innerproduct{\boldsymbol\xi}{\boldsymbol\zeta}_{[x]} = \innerproduct{\lift_x(\boldsymbol\xi)}{\lift_x(\boldsymbol\zeta)}_x.
\end{equation*}
With this metric, $\M$ is called a \highlight{Riemannian quotient manifold} of $\bar{\M}$.
\end{proposition}

\begin{proposition}
The Riemannian gradient of $f\in\mathfrak{F}(\M)$ on a Riemannian quotient manifold $\M$ is related to the Riemannian gradient of the lifted function $\bar{f} = f\circ \pi$ on the total space for all $x\in \bar{\M}$ through
\begin{equation*}
\lift_x(\grad f([x])) = \grad \bar{f}(x).
\end{equation*}
\end{proposition}

\subsection{Riemannian Optimization}

\begin{proposition}[\highlight{Second-order Necessary Conditions}]
Consider a smooth function $\mapping{f}{\M}{\R}$ defined on a Riemannian manifold $\M$. If $x$ is a local (or global) minimizer of $f$, then $\grad f(x) = 0$ and $\hess f(x) \succeq 0$.
\end{proposition}

\begin{proposition}[\highlight{Second-order Sufficient Conditions}]
Consider a smooth function $\mapping{f}{\M}{\R}$ defined on a Riemannian manifold $\M$. If $\grad f(x) = 0$ and $\hess f(x) \succ 0$, then $x$ is a strict local (or global) minimizer of $f$.
\end{proposition}

\begin{remark}[\highlight{Second-order Necessary and Sufficient Conditions}]\label{remark:second_order_optimality_conditions}
A point $x^*\in \M$ is a strict local minimizer of a smooth function $\mapping{f}{\M}{\R}$ defined on a Riemannian manifold $\M$ iff $\grad f(x) = 0$ and $\hess f(x) \succ 0$.
\end{remark}

\begin{definition}[\highlight{Gradient-related sequence}]
Given a cost function $f$ on a Riemannian manifold $\M$, a sequence $(\eta_k), \eta_k\in T_{x_k}\M$, is \highlight{gradient-related} if, for any subsequence $(x_k)_{k\in\mathcal{K}}$ of $(x_k)$ that converges to a non-critical point of $f$, the corresponding subsequence $(\eta_k)_{k\in\mathcal{K}}$ is bounded and satisfies
\begin{equation*}
\underset{k\rightarrow \infty,k\in\mathcal{K}}{\;\;Lt\;sup} \innerproduct{\grad f(x_k)}{\eta_k} < 0.
\end{equation*}
\end{definition}

\begin{definition}[\highlight{Armijo point}]
Given a cost function $f$ on a Riemannian manifold $\M$ with retraction $R$, a point $x\in \M$, a tangent vector $\eta\in T_x\M$, and scalars $\bar{\alpha}>0,\beta,\sigma\in (0,1)$, the \highlight{Armijo point} is $\eta_A = t^A\eta = \beta^m\bar{\alpha}\eta$, where $m$ is the smallest nonnegative integer such that 
\begin{equation*}
f(x) - f(R_x(\beta^m\bar{\alpha}\eta)) \ge -\sigma\innerproduct{\grad f(x)}{\beta^m\bar{\alpha}\eta}.
\end{equation*}
The real number $t^A$ is the \highlight{Armijo step size}.
\end{definition}

\begin{algorithm}
    \centering
    \caption{Accelerated Line Search (ALS)}\label{algo:ALS}
    \begin{algorithmic}[1]
        \Require Riemannian manifold $\M$; continuously differentiable scalar field $f$ on $\M$; retraction $R$ from $T\M$ to $\M$; scalars $\bar{\alpha}>0,c,\beta,\sigma\in(0,1)$.
        \State Given an initial iterate $x_0\in\M$.
        \For{$k = 0,1,2,\ldots$} 
        \State Pick $\eta_k$ in $T_{x_k}\M$ such that the sequence $(\eta_i)_{i=0,1,\ldots}$ is gradient-related.
        \State Select $x_{k+1}$ such that 
        \begin{equation}
            f(x_k) - f(x_{k+1}) \ge c(f(x_k) - f(R_{x_k}(t^A_k\eta_k))),
        \end{equation}
        where $t^A_k$ is the Armijo step size for a given $\bar{\alpha},\beta,\sigma,\eta_k$.
        \EndFor
    \end{algorithmic}
\end{algorithm}

\begin{proposition}[\highlight{Convergence of ALS}]
Let $(x_k)$ be an infinite sequence of iterates generated by Algo. \ref{algo:ALS}. Then every accumulation point of $\{x_k\}$ is a critical point of the cost function $f$.
\end{proposition}

\begin{corollary}
Let $(x_k)$ be an infinite sequence of iterates generated by Algo. \ref{algo:ALS}. Assume that the level set $\mathcal{L} = \{x\in\M:\,f(x)\le f(x_0)\}$ is compact (which holds in particular when $\M$ itself is compact). Then 
\begin{equation*}
\underset{k\rightarrow\infty}{Lt} \|\grad f(x_k)\| = 0.
\end{equation*}
\end{corollary}

Let $F^{(n)}$ denote the result of $n$ applications of the map $F$ to $x$, i.e., 
\begin{equation*}
    F^{(1)}(x) = F(x),\qquad F^{(i+1)}(x) = F(F^{(i)}(x)), i = 1,2,\ldots.
\end{equation*}

\begin{definition}
Let $\mapping{F}{\M}{\M}$ be a mapping. A point $x^*\in\M$ is a \highlight{fixed point} of $F$ if $F(x^*) = x^*$.
\end{definition}

\begin{definition}
A fixed point $x^*$ is a \highlight{stable point} of $\mapping{F}{\M}{\M}$ if, for every neighborhood $\mathcal{U}$ of $x^*$, there exists a neighborhood $\mathcal{V}$ of $x^*$ such that, for all $x\in\mathcal{V}$ and all positive integer $n$, it holds that $F^{(n)}\in \mathcal{U}$. 
\end{definition}

\begin{definition}
The fixed point $x^*$ is \highlight{asymptotically stable} of $\mapping{F}{\M}{\M}$ if it is a stable, and, moreover, $\underset{k\rightarrow\infty}{Lt}F^{(k)} = x^*$ for all sufficiently close to $x^*$. 
\end{definition}

\begin{definition}
The fixed point $x^*$ is \highlight{unstable} of $\mapping{F}{\M}{\M}$ if it is not stable; in other words, there exists a neighborhood $\mathcal{U}$ of $x^*$ such that, for all neighborhoods $\mathcal{V}$ of $x^*$, there is a point $x\in \mathcal{V}$ such that $F^{(n)}\notin \mathcal{U}$ for some $n$.
\end{definition}

\begin{definition}
We say that $F$ is a \highlight{descent mapping} for a cost function $f$ if 
\begin{equation*}
f(F(x))\le f(x),\;\forall\,x\in\M.
\end{equation*}
\end{definition}

\begin{proposition}[\highlight{Unstable fixed points}] \label{prop:unstable_fixed_points}
Let $\mapping{F}{\M}{\M}$ be a descent mapping for a smooth cost function $f$ and assume that for every $x\in \M$, all the accumulation points of $\{F^{(k)}(x)\}_{k = 1,2,\ldots}$ are critical points of $f$. Let $x^*$ be a fixed point of $F$ (thus $x^*$ is a critical point of $f$ because $x^*$ is an accumulation point of $\{F^{(k)}(x^*)\} = \{x^*\}$). Assume that $x^*$ is not a local minimum of $f$. Further assume that there is a compact neighborhood $\mathcal{U}$ of $x^*$ where, for every critical point $y$ of $f$ in $\mathcal{U}$, $f(y) = f(x^*)$. Then $x^*$ is an unstable point for $F$.
\end{proposition}

\begin{remark}
The conditions on $F$ in Prop. \ref{prop:unstable_fixed_points} are satisfied by any method in the class of Algo. \ref{algo:ALS}. As for the conditions on the critical points of $f$, it holds for example when $f$ is real analytic by Lojasiewicz inequality (Prop. \ref{prop:Lojasiewicz_inequality}). For a critical point $x^*$ of $f$, there exists a neighborhood $V$ of $x^*$, and constants $\mu\in[0,1)$ and $c>0$ such that for all $x\in V$
\begin{equation*}
\|\nabla f(x)\| \ge c|f(x)-f(x^*)|^\mu.
\end{equation*}
Since $V\subseteq \R^n$, there exists an open ball $\bb_{x^*}(r)\subseteq V$ for some $r>0$ such that $U = \bar{\bb}_{x^*}(r)\subseteq V$. Then for any critical point $y\in U$, we have
\begin{equation*}
0 = \|\nabla f(y)\| \ge c|f(y)-f(x^*)|^\mu \implies f(y) = f(x^*).
\end{equation*}
\end{remark}

\begin{proposition}[\highlight{Capture Theorem}] \label{prop:capture_theorem}
Let $\mapping{F}{\M}{\M}$ be a descent mapping for a smooth cost function $f$ and assume that for every $x\in \M$, all the accumulation points of $\{F^{(k)}(x)\}_{k = 1,2,\ldots}$ are critical points of $f$. Let $x^*$ be a local minimizer and an isolated critical point of $f$. Assume further that $\underset{x\rightarrow x^*}{Lt} dist(F(x),x) = 0$. Then $x^*$ is an asymptotically stable point of $F$.
\end{proposition}

\begin{remark}
The additional condition on $dist(F(x),x)$ is Prop. \ref{prop:capture_theorem} is not satisfied by every instance of Algo. \ref{algo:ALS} because the line-search framework in Line 4 does not put any restriction on the step length. The distance condition is satisfied, for example, when $\eta_k$ is selected such that $\|\eta_k\|\le c\|\grad f(x_k)\|$ for some constant $c$ and $x_{k+1}$ is selected as the Armijo point.
\end{remark}

\subsection{Generalized Stiefel Manifold}

For a SPD matrix $B\in \R^{n\times n}$, we denote by $\genst$ the \highlight{generalized Stiefel manifold} defined by
\begin{equation}\label{eqn:gen_stiefel_manifold}
    \genst = \{X\in\realmat{n}{p}:\,X^TBX = I_p\}.
\end{equation}

\subsubsection{Metric Independent Notions}

\begin{proposition}
The generalized Stiefel manifold $\genst$ is an embedded submanifold of $\realmat{n}{p}$ of dimension $np-p(p+1)/2$. The tangent space at $X\in \genst$ is given by 
\begin{align}\label{eqn:tangent_space_genst_1}
T_X\genst &= \{Z\in\realmat{n}{p}:\,Z^TBX + X^TBZ = 0\}.
\end{align}
An equivalent formulation of the tangent space is
\begin{align}\label{eqn:tangent_space_genst_2}
T_X\genst &= \{X\Omega + \XperpB K:\, \Omega \in \Skew,K\in\realmat{(n-p)}{p}\},
\end{align}
where $\XperpB$ satisfies the relations: $\XperpB^TB\XperpB = I_{n-p}$, and $\XperpB^T BX = 0$.
\end{proposition}

\begin{proof}
Consider the mapping $\mapping{F}{\realmat{n}{p}}{\Sym}$ defined by $F(X) = X^TBX - I_p$ such that $\genst = F^{-1}(0)$. We show that $0$ is a regular value of $F$. We compute the directional derivative of $F$ at $X$ in the direction of $Z\in\realmat{n}{p}$
\begin{align*}
DF(X)[Z] &= \Lt{t} \dfrac{(X+tZ)^TB(X+tZ)-X^TBX}{t} \\
&= \Lt{t} \dfrac{tX^TBZ+tZ^TBX + t^2Z^TBZ}{t} = X^TBZ+Z^TBX.
\end{align*}
Let $Z = \frac{1}{2}X\Tilde{Z}$, where $\Tilde{Z}\in \Sym$. Then 
\begin{equation*}
DF(X)[Z] = \frac{1}{2}[(X^TBX)\Tilde{Z} + \Tilde{Z}(X^TBX)] = \Tilde{Z}.
\end{equation*}
Thus, for every $\Tilde{Z}\in\Sym$, we have $Z = \frac{1}{2}X\Tilde{Z}$ such that $DF(X)[Z] = \Tilde{Z}$. So $DF(X) = \nabla F(X)^T$ has full-rank $p(p+1)/2$ at each $X\in\genst$. Then $0$ is a regular value of $F$ and consequently by Submersion Theorem (Prop. \ref{prop:submersion_theorem}), $\genst = F^{-1}(0)$ is an embedded submanifold of $\realmat{n}{p}$ of dimension $np-p(p+1)/2$ and the tangent space at $X\in\genst$ is given by
\begin{equation*}
T_X\genst = ker(\nabla F(X)^T) = \{Z\in\realmat{n}{p}:\,Z^TBX + X^TBZ = 0\}.
\end{equation*}

Every $Z\in\realmat{n}{p}$ can be represented as $Z = X\Omega+\XperpB K$ for arbitrary $\Omega\in\realmat{p}{p}$ and $K\in\realmat{(n-p)}{p}$ ($np$ degrees of freedom). Denote $S = \{X\Omega + \XperpB K:\, \Omega \in \Skew,K\in\realmat{(n-p)}{p}\}$. Suppose $Z\in T_X\genst$, then 
\begin{align*}
&\;\hspace{1cm} Z^TBX + X^TBZ = 0 \\
&\implies (X\Omega+\XperpB K)^TBX + X^TB(X\Omega+\XperpB K) = 0 \\
&\implies \Omega^T(X^TBX) + K^T(\XperpB^TBX) + (X^TBX)\Omega + (X^TB\XperpB) K = 0 \\
&\implies \Omega^T + \Omega = 0, \text{ i.e., }\Omega\in\Skew.
\end{align*}
This shows that $T_X\genst\subseteq S$. Note that $S$ is a subspace of $\realmat{n}{p}$ of dimension $np-p(p+1)/2$ containing a subspace of the same dimension, then $T_X\genst = S$.
\end{proof}

\begin{proposition}
The map $R^{\text{polar}}$ defined by 
\begin{equation}
\polarRetr(\xi_X) = (X+\xi_Z)(I_p + \xi_X^TB\xi_X)^{-1/2},
\end{equation}
is a retraction on $\genst$.
\end{proposition}

\begin{proof}
We show that Def. \ref{def:retraction} holds for $\polarRetr$ at $X\in \genst$. We check the first condition of the definition
\begin{equation*}
\polarRetr(0) = (X+0)(I_p+0^T B 0)^{-1/2} = X.
\end{equation*}
Suppose $\lambda_1,\ldots,\lambda_p\ge 0$ denote the eigenvalues of $\xi_X^TB\xi_X$, and $Q$ is its eigenvector matrix. Then
\begin{align*}
(I_p+t^2\xi_X^TB\xi_X)^{-1/2} = 
Q\left(
\begin{array}{ccc}
\frac{1}{\sqrt{1+t^2\lambda_1}} &  & \\
 & \ddots & \\
 & & \frac{1}{\sqrt{1+t^2\lambda_p}}
\end{array}
\right)Q^T
\end{align*}
Then by condition (b) of Def. \ref{def:retraction}, we have
\begin{align*}
&\dfrac{d}{dt}\polarRetr(t\xi_X) = 
\dfrac{d}{dt}\big[(X+t\xi_X)(I_p+t^2\xi_X^TB\xi_X)^{-1/2} \big]\\
&= \dfrac{d}{dt}\left[
(X+t\xi_X)Q\left(
\begin{array}{ccc}
\frac{1}{\sqrt{1+t^2\lambda_1}} &  & \\
 & \ddots & \\
 & & \frac{1}{\sqrt{1+t^2\lambda_p}}
\end{array}
\right)Q^T\right] \\
&= \xi_XQ\left(
\begin{array}{ccc}
\frac{1}{\sqrt{1+t^2\lambda_1}} &  & \\
 & \ddots & \\ 
 & & \frac{1}{\sqrt{1+t^2\lambda_p}}
\end{array}
\right)Q^T - 
(X+t\xi_X)Q\left(
\begin{array}{ccc}
\frac{t\lambda_1}{(1+t^2\lambda_1)^{3/2}} &  & \\
 & \ddots & \\
 & & \frac{t\lambda_p}{(1+t^2\lambda_p)^{3/2}}
\end{array}
\right)Q^T.
\end{align*}
Setting $t = 0$ in the above expression, the second expression vanishes and we have
\begin{equation*}
\dfrac{d}{dt}\bigvert{\polarRetr(t\xi_X)}{t=0} = \xi_X.
\end{equation*}
\end{proof}

\begin{proposition}
The map $R^{\text{QR}}$ defined by 
\begin{equation}
\qrRetr(\xi_X) = B^{-1/2}qf(B^{1/2}(X+\xi_X))
\end{equation}
is a retraction on $\genst$, where $qf(\cdot)$ denotes the Q-factor of the QR factorization of the matrix in parentheses.
\end{proposition}

\begin{proof}
For $X\in\genst$, we have
\begin{equation*}
(B^{1/2}X)^T(B^{1/2}X) = X^TBX = I_p,
\end{equation*}
which means that $B^{1/2}X\in St(n,p)$. Then $B^{1/2}X = (B^{1/2}X) I_p$ is a QR-factorization of $B^{1/2}X$ and $qf(B^{1/2}X) = B^{1/2}X$. Hence,
\begin{equation*}
\qrRetr(0) = B^{-1/2}qf(B^{1/2}X) = B^{-1/2}B^{1/2}X.
\end{equation*}
This proves the first condition of Def. \ref{def:retraction}. 

To prove the second condition of retraction, we use the fact that Q-factor retraction $R$ on $St(n,p)$ is a retraction on $St(n,p)$. Then $DR_Y(0)[Z] = Dqf(Y)[Z] = Z$, for $Y\in St(n,p)$ and $Z\in T_YSt(n,p)$. Here we have $B^{1/2}X\in St(n,p)$ and 
\begin{equation*}
(B^{1/2}\xi_X)^T(B^{1/2}X) + (B^{1/2}X)^T(B^{1/2}\xi_X) = \xi_x^TBX + X^TB\xi_X = 0,
\end{equation*}
for any $\xi_X\in T_X\genst$, which implies that $B^{1/2}\xi_X\in T_{B^{1/2}X}St(n,p)$. Then we have $Dqf(B^{1/2}X)[B^{1/2}\xi_X] = B^{1/2}\xi_X$ and consequently,
\begin{equation*}
D\qrRetr(0)[\xi_X] = B^{-1/2}Dqf(B^{1/2}X)[B^{1/2}\xi_X] = B^{-1/2}B^{1/2}\xi_X = \xi_X.
\end{equation*}
\end{proof}

\subsubsection{Metric Dependent Notions}

\begin{proposition}[\highlight{Preconditioned Riemannian Metric}] \label{prop:preconditioned_metric}
The metric $\bar{g}$ on $\realmat{n}{p}$ defined for any $X\in \realmat{n}{p}$ as
\begin{equation*}
\bar{g}_X(\bar{\xi}_X,\bar{\eta}_X) := \innerproduct{\bar{\xi}_X}{\bar{\eta}_X}_M = Tr(\bar{\xi}_X^TM\bar{\eta}_X),
\end{equation*}
is a Riemannian metric on $\realmat{n}{p}$, where $M\in\realmat{n}{n}$ is some constant SPD matrix. Then for any $X\in \genst$, $\eta_X,\xi_X\in T_X\genst$, the Riemannian metric on $\genst$ is given by
\begin{equation}
g_X(\xi_X,{\eta}_X) := \innerproduct{{\xi}_X}{{\eta}_X}_M = Tr({\xi}_X^TM{\eta}_X).
\end{equation}
\end{proposition}

\begin{remark}
Classically, the metric employed for the generalized Stiefel manifold corresponds to $M = B$. However, as we shall see, various operations required for Riemannian optimization require products with $M^{-1}$ which might be very costly when we use $M = B$. In such situations preconditioning $B$, such that convergence rate is preserved and $M^{-1}$ products are made efficient, can help.
\end{remark}

\begin{proposition}
The normal space, i.e., the orthogonal complement of the tangent space, at $X\in\genst$ is given by
\begin{equation}
(T_X\genst)^\perp = \{M^{-1}BXS:\, S\in\Sym \}.
\end{equation}
Consequently, the dimension of the normal space is $p(p+1)/2$.
\end{proposition}

\begin{proof}
Suppose $S = \{M^{-1}BXS:\, S\in\Sym \}$ for $X\in\genst$. Then for any $\zeta\in S$ and $\eta\in T_X\genst$, i.e., $\zeta = M^{-1}BXS$ for some $S\in \Sym$ and $\eta = X\Omega+\XperpB K$ for some $\Omega\in \Skew$ and $K\in \realmat{(n-p)}{p}$. Then
\begin{align*}
\innerproduct{\zeta}{\eta}_M &= Tr(\zeta^TM\eta) = Tr((M^{-1}BXS)^TM(X\Omega+\XperpB K)) \\
&= Tr((S^TX^TB^T)(M^{-1}M)(X\Omega+\XperpB K)) \\
&= Tr(S(X^TBX)\Omega+S(X^TB\XperpB) K) = Tr(S\Omega) = 0.
\end{align*}
This shows that $S\subseteq (T_X\genst)^\perp$. But note that a vector space $S$ of dimension $p(p+1)/2$ is contained in $(T_X\genst)^\perp$ of same dimension, thus $(T_X\genst)^\perp = S$.
\end{proof}

\begin{lemma}[\highlight{Sylvester Equation}]\label{lemma:sylvester_equation}
Let $A,B\in\realmat{p}{p}$ be given. The equation $AX+XB = C$ has a unique solution $X\in\realmat{p}{p}$ for each given $C\in\realmat{p}{p}$ iff $A$ and $-B$ have no eigenvalue in common. In particular, if $A$ and $-B$ have no eigenvalue in common then the only $X$ such that $AX + X B = 0$ is $X = 0$.
\end{lemma}

\begin{lemma}[\highlight{Gram Matrix}]\label{lemma:gram_matrix_spd}
Suppose $G =X^TX$ is the Gram matrix of $X$. Then $G\succ 0$ iff $X$ has full rank.
\end{lemma}

\begin{proposition}
The orthogonal projections w.r.t. $g_X$ (Prop. \ref{prop:preconditioned_metric}) on $(T_X\genst)^\perp$ and on $T_X\genst$ are:
\begin{equation}\label{eqn:normal_projector}
\perpProj_X(\xi) = M^{-1}BXS_{\xi}
\end{equation}
and 
\begin{equation}\label{eqn:tangent_projector}
\Proj_X(\xi) = (Id_{T_X\genst} - \perpProj_X)(\xi) = \xi - M^{-1}BXS_\xi
\end{equation}
where $\xi\in \realmat{n}{p}$, and $S_\xi\in \Sym$ is the solution of the following Sylvester equation
\begin{equation*}
(X^TBM^{-1}BX)S_\xi + S_\xi (X^TBM^{-1}BX) = X^TB\xi + (X^TB\xi)^T.
\end{equation*}
\end{proposition}

\begin{proof}
From the representations of $T_X\genst$ and $(T_X\genst)^\perp$, we can write any vector $\xi\in \realmat{n}{p}$ in terms of unique $\Omega_\xi\in \Skew$, $K\in \realmat{(n-p)}{p}$ and $S_\xi\in \Sym$ such that
\begin{equation}\label{eqn:decomposition_of_vector}
\xi = (X\Omega_\xi + \XperpB K_\xi) + M^{-1}BXS_\xi.
\end{equation}
By left-multiplying \eqref{eqn:decomposition_of_vector} by $X^TB$, we have
\begin{equation*}
X^TB\xi = \Omega_\xi + X^TBM^{-1}BXS_\xi.
\end{equation*}
Summing $X^TB\xi+(X^TB\xi)^T$, and using the fact that $\Omega_\xi\in\Skew$, we obtain
\begin{equation}\label{eqn:sylvester_equation}
X^TB\xi+(X^TB\xi)^T = (X^TBM^{-1}BX)S_\xi + S_\xi(X^TBM^{-1}BX).
\end{equation}
Note that $X^TBM^{-1}BX = (M^{-1/2}BX)^T(M^{-1/2}BX)$, i.e., $X^TBM^{-1}BX$ is the Gram matrix of $M^{-1/2}BX$. Also note that $M^{-1}BX$ is of full column rank as $M^{-1/2}, B$ are positive definite and $X\in\genst$ has full column rank. Then by Lemma \ref{lemma:gram_matrix_spd}, $X^TBM^{-1}BX$ is positive definite and consequently $-(X^TBM^{-1}BX)$ is negative definite, so they have no eigenvalues in common. Then by Lemma \ref{lemma:sylvester_equation}, \eqref{eqn:sylvester_equation} has a unique solution $S_\xi\in\Sym$. This proves the expressions \eqref{eqn:normal_projector} and \eqref{eqn:tangent_projector}.

We now show that both the projections satisfy the projection property: $\Proj_X^2(\cdot) = \Proj_X(\cdot)$ and $(\perpProj_X)^2(\cdot) = \perpProj_X(\cdot)$. Since $\perpProj_X(\xi) = M^{-1}BXS_\xi$ satisfies \eqref{eqn:sylvester_equation}, we have
\begin{align*}
&\;X^TB\perpProj_X(\xi)+(X^TB\perpProj_X(\xi))^T = (X^TBM^{-1}BX)S_{\perpProj_X(\xi)} + S_{\perpProj_X(\xi)}(X^TBM^{-1}BX) \\
&\;(X^TBM^{-1}BX)S_\xi+S_\xi(X^TBM^{-1}BX) = (X^TBM^{-1}BX)S_{\perpProj_X(\xi)} + S_{\perpProj_X(\xi)}(X^TBM^{-1}BX) \\
&\;(X^TBM^{-1}BX)(S_\xi-S_{\perpProj_X(\xi)})+(S_\xi-S_{\perpProj_X(\xi)})(X^TBM^{-1}BX) = 0
\end{align*}
Then by Lemma \ref{lemma:sylvester_equation}, $S_{\perpProj_X(\xi)} = S_\xi$, i.e., $(\perpProj_X)^2(\xi) = \perpProj_X(\xi) = M^{-1}BXS_\xi$. In addition, we have $\Proj_X = Id - \perpProj_X$. Then 
\begin{equation*}
\Proj_X^2 = (Id - \perpProj_X)^2 = Id - 2\perpProj_X+(\perpProj_X)^2 = Id - 2\perpProj_X + \perpProj_X = Id - \perpProj_X = \Proj_X.
\end{equation*}

Lastly, we show that both the projections are orthogonal w.r.t. the Riemannian metric $g$ (Prop. \ref{prop:preconditioned_metric}) on $\genst$. Using the cyclic property of trace, for any $\eta,\xi\in\realmat{n}{p}$, we have
\begin{align*}
g_X(\perpProj_X(\eta),\xi) &= Tr(S_\eta X^T B \xi) = Tr((S_\eta X^T B \xi)^T) = Tr(\xi^T B X S_\eta) \\
&= \frac{1}{2} Tr((\xi^T B X + (\xi^T B X)^T) S_\eta) \\
&= \frac{1}{2} Tr(((X^TBM^{-1}BX)S_\xi + S_\xi(X^TBM^{-1}BX)) S_\eta) \\
&= \frac{1}{2} Tr((X^TBM^{-1}BX)S_\xi S_\eta + S_\xi(X^TBM^{-1}BX) S_\eta) \\
&= \frac{1}{2} Tr(S_\eta(X^TBM^{-1}BX)S_\xi + (X^TBM^{-1}BX)S_\xi S_\eta) \\
&= \frac{1}{2} Tr((S_\eta(X^TBM^{-1}BX) + (X^TBM^{-1}BX)S_\xi )S_\eta) \\
&= \frac{1}{2} Tr((\eta^T B X + (\eta^T B X)^T) S_\xi) \\
&= Tr(\eta^T B X S_\xi) = g_X(\eta,\perpProj_X(\xi)),
\end{align*}
and similarly for $\Proj_X(\cdot)$.
\end{proof}

\begin{remark}\label{remark:unpreconditioned_Riemannian_gradient}
For the special case $M = B$, the orthogonal projections are given by
\begin{align*}
\perpProj_X(\xi) &= Xsym(X^TB\xi),\\
\Proj_X(\xi) &= \xi - Xsym(X^TB\xi) \nonumber\\
&= \xi - (XX^TB\xi - Xskew(X^TB\xi))\nonumber\\
\Proj_X(\xi) &= (I_n - XX^TB)\xi + Xskew(X^TB\xi).
\end{align*}
\end{remark}

\begin{proposition}
Let $\mapping{f}{\genst}{\R}$ be smooth function, and let $\bar{f}$ be a smooth extension of $f$ to $\realmat{n}{p}$. Then
\begin{equation}
\grad f(X) = \Proj_X(M^{-1}\nabla \bar{f}(X)).
\end{equation}
\end{proposition}

\begin{proof}
We know that $\grad f(X) = \Proj_X(\grad\bar{f}(X))$. Note that $\grad\bar{f}(X)\neq\nabla\bar{f}(X)$ because $\bar{f}$ is defined on $\realmat{n}{p}$ endowed with a non-standard inner product. Then for any $\xi_X\in T_X\genst$
\begin{equation*}
Tr(\grad\bar{f}(X)^TM\xi_X) = g_X(\grad\bar{f}(X), \xi_X) = D\bar{f}(X)\xi_X \overset{(*)}{=} Tr(\nabla\bar{f}(X)^T\xi_X),
\end{equation*}
where $(*)$ holds because directional derivative is independent of metric. Thus $\grad\bar{f}(X) = M^{-1}\nabla\bar{f}(X)$.
\end{proof}

\begin{proposition}[\highlight{Product Manifold}]
Let $B_1,\ldots,B_k$ be SPD matrices, where $B_i\in\realmat{n_i}{n_i}$, and denote $d = d_1+\ldots+d_k$. Suppose that on each $\igenst{i}$ the metric is defined by a constant SPD matrix $M_i$. The product manifold $\igenst{1}\times\ldots\times\igenst{k}$ is a Riemannian submanifold of $\realmat{n_1}{p}\times\ldots\times\realmat{n_k}{p}$ endowed with the product metric (sum of metric values on each product component). The product manifold $\igenst{1}\times\ldots\times\igenst{k}$ can be viewed as a Riemannian submanifold of $\realmat{n}{p}$ endowed by the metric defined by the $n\times n$ matrix
\begin{equation*}
M = 
\left[\begin{array}{ccc}
M_1 & & \\
 & \ddots & \\
 & & M_k
\end{array}\right].
\end{equation*}
The tangent space of the product manifold is the product of the tangent spaces of each manifold. The retraction and vector transport, and orthogonal projection on the tangent space is stacking the operations performed separably on each manifold on top of each other. The Riemannian gradient of a smooth function $f$ on the product manifold is the orthogonal projection to the tangent spaces after pre-mulitplying by $M^{-1}$.
\end{proposition}

% \newpage

\begin{thebibliography}{9}
\bibitem{BMishra}
Mishra B., Sepulchre R. (2016) Riemannian preconditioning, SIAM Journal on Optimization, 26, pp. 635–660

\bibitem{shustin2020randomized}
Shustin B., and Avron H. (2020) Randomized Riemannian Preconditioning for Orthogonality Constrained Problems, arXiv, 1902.01635v2 [math.NA]

\bibitem{NBoumal}
Boumal N. (2020) An introduction to optimization on smooth manifolds, Available online.

\bibitem{Num_Opt}
Nocedal J., Wright S.J. (2006) Numerical Optimization, 2nd edn. Springer Series in Operations Research and Financial Engineering, Springer, New York, USA

\bibitem{FPSQP}
Wright S.J., Tenny M.J. (2004) A feasible trust-region sequential quadratic programming algorithm. SIAM Journal on Optimization 14(4):1074–1105

\bibitem{AMS_08}
Absil P.A., Mahony R., Sepulchre R. (2008) Optimization Algorithms on Matrix Manifolds.
Princeton University Press, Princeton, NJ

\bibitem{Newton_FPSQP}
Absil P.A., Trumpf J., Mahony R., Andrews B. (2009) All roads lead to Newton: Feasible second-order methods for equality-constrained optimization. Tech. rep., UCL-INMA-2009.024

\bibitem{Gabay_82}
Gabay D. (1982) Minimizing a differentiable function over a differential manifold. Journal of Optimization Theory and Applications 37(2):177–219

\bibitem{horn_and_johnson}
Horn  R.A., Johnson C.R. (2012) Matrix analysis, 2nd ed., Cambridge University Press

\bibitem{RaviTejaQuotientGeo}
Vemulapalli R. (2013)  \href{http://ravitejav.weebly.com/uploads/2/4/7/2/24725306/quotient_geometry.pdf}{Basic Introduction to Quotient Geometry}

% \bibitem{Seq Lemma (b)}
% [Sequence Lemma (b)] \href{https://math.stackexchange.com/questions/1876224/}{math.SE}

\end{thebibliography}


\end{document}