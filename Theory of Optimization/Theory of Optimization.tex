\documentclass[11pt,a4paper]{article}
\usepackage[margin=1in]{geometry}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage[english]{babel}
\usepackage{amsthm}
\usepackage{amsfonts}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{amssymb}
\usepackage{enumerate}
\usepackage[hidelinks]{hyperref}
\usepackage{cancel}

\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,      
    urlcolor=blue,
    pdftitle={Theory of Optimization},
    pdfpagemode=FullScreen,
}


\author{Jayadev Naram}
\title{Theory of Optimization} 

\begin{document}

\date{}
\maketitle
\tableofcontents
\newpage

\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{corollary}{Corollary}[theorem]

\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{assume}{Assumption}

\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{example}[theorem]{Example}
\newtheorem{remark}[theorem]{Remark}

\newcommand{\R}{\mathbb{R}}
\newcommand{\B}{\mathbb{B}}
\newcommand{\A}{\mathcal{A}}
\newcommand{\M}{\mathcal{M}}
\newcommand{\N}{\mathcal{N}}
\newcommand{\h}{\mathcal{H}}
\newcommand{\T}{\mathcal{T}}
\newcommand{\Prob}{\mathbb{P}}
\newcommand{\Dist}{\mathcal{D}}
\newcommand{\perpProj}{\mathcal{P}^\perp}
\newcommand{\bb}{\mathbb{B}}
\newcommand{\Sprod}{\mathbb{S}_{xy}}
\newcommand{\highlight}[1]{\underline{\textit{\textbf{#1}}}}
\newcommand{\mapping}[3]{#1:#2\rightarrow #3}
\newcommand{\doubt}{\highlight{[??]}}
% \newcommand{\bigvert}[2]{#1{\raisebox{-.5ex}{$|$}_{#2}}}
\newcommand{\bigvert}[2]{\left.#1\right|_{#2}}
\newcommand{\sdnn}[1]{${#1}$}
\newcommand{\bsdnn}[1]{$\boldsymbol{#1}$}
\newcommand{\ifthen}[2]{\textbf{(#1)}\boldsymbol{\implies}\textbf{(#2)}}
\newcommand{\bsdn}[1]{\boldsymbol{#1}}
\newcommand{\forward}{$(\implies)$\ }
\newcommand{\converse}{$(\impliedby)$\ }
\newcommand{\Lt}[1]{\underset{#1\rightarrow 0}{Lt}}
\newcommand{\norm}[1]{\|#1\|}
\newcommand{\dparder}[2]{\dfrac{\partial #1}{\partial x_{#2}}}
\newcommand{\fparder}[2]{\frac{\partial #1}{\partial x_{#2}}}
\newcommand{\parder}[2]{\partial #1/\partial x_{#2}}
\newcommand{\parop}[1]{\dfrac{\partial}{\partial x_{#1}}}
\newcommand{\innerproduct}[2]{\langle #1, #2 \rangle}
\newcommand{\metric}[2]{[#1, #2]}
\newcommand{\genst}{St_B(n,p)}
\newcommand{\igenst}[1]{St_{B_{#1}}(n_{#1},p)}
\newcommand{\realmat}[2]{\R^{#1\times #2}}
\newcommand{\Skew}{\mathcal{S}_{skew}(p)}
\newcommand{\Sym}{\mathcal{S}_{sym}(p)}
\newcommand{\XperpB}{X_{B^\perp}}
\newcommand{\polarRetr}{R^{polar}_X}
\newcommand{\qrRetr}{R^{QR}_X}
\newcommand{\vectransport}{\mathcal{T}}
\newcommand{\grad}{\text{grad}\,}
\newcommand{\hess}{\text{Hess}\,}

\section{Unconstrained Minimization}

Let $\R^n$ denote the $n$-dimensional Euclidean real vector space with the inner product defined for any $x,y\in \R^n$ as $\innerproduct{x}{y} = x^Ty = \sum_{i=1}^n x_i y_i$, where $x = [x_1,\ldots, x_n]^T$ and $y = [y_1,\ldots, y_n]^T$ are the coordinates of $x$ and $y$ respectively. Let the norm and the metric on $\R^n$ be defined as $\|x\| = \sqrt{\innerproduct{x}{x}}$ and $d(x,y) = \|x-y\|$, respectively.

Suppose $\mapping{f}{\R^n}{\R}$ is a real-valued function on $\R^n$. We consider the following optimization problem 
\begin{equation}\label{eqn:unconst_opt}
    \min_{x\in\R^n} f(x).
\end{equation}
The problem \eqref{eqn:unconst_opt} is called as the unconstrained minimization problem as there are no constraints on $x$. 

\begin{definition}\label{def:optimal_points}
    A \highlight{local minimum} of $f$ in the problem \eqref{eqn:unconst_opt} is a vector $x^*\in\R^n$ for which there exists $\varepsilon>0$ such that for all $x\in\R^n$ we have
    \begin{equation}\label{eqn:local_min}
        f(x^*)\le f(x),
    \end{equation}
    when $\|x-x^*\|\le \varepsilon$.
    % 
    A \highlight{global minimum} of $f$ in the problem \eqref{eqn:unconst_opt} is a vector $x^*\in\R^n$ such that for all $x\in\R^n$ we have
    \begin{equation}\label{eqn:global_min}
        f(x^*)\le f(x).
    \end{equation}
    % 
    The global or local minimum $x^*$ is said to be \highlight{strict} if the corresponding inequality given above is strict for $x\neq x^*$. The vector $x^*$ with $\nabla f(x^*) = 0$ is referred to as a \highlight{stationary point}. 
\end{definition}

\begin{proposition}[Necessary Optimality Conditions]
    Let $x^*$ be an unconstrained local minimum of $\mapping{f}{\R^n}{\R}$, and assume that $f$ is continuously differentiable in an open set $U$ containing $x^*$. Then 
    \begin{equation*}
        \nabla f(x^*) = 0.\qquad\qquad (\text{First Order Necessary Condition})
    \end{equation*}
    If in addition $f$ is twice continuously differentiable within $U$, then
    \begin{equation*}
        \nabla^2 f(x^*) \succeq 0.\qquad\qquad (\text{Second Order Necessary Condition})
    \end{equation*}
\end{proposition}
    
\begin{proof}
    Fix some $d\in \R^n$. Then, using chain rule to differentiate the function $g(\alpha) = f(x^*+\alpha d)$, we have
    \begin{equation*}
        0\le \lim_{\alpha\downarrow 0} \dfrac{f(x^*+\alpha d) - f(x^*)}{\alpha} = \dfrac{dg(0)}{d\alpha} = \nabla f(x^*)^Td,
    \end{equation*}
    where the inequality follows from the assumption that $x^*$ is a local minimum and $\alpha\downarrow 0$ indicates the right-hand limit, i.e., $\alpha>0$ and $\alpha\rightarrow 0$. Since $d$ is arbitrary, the same inequality holds with $d$ replaced by $-d$. Therefore, $\nabla f(x^*)^Td = 0$ for all $d\in\R^n$, which shows that $\nabla f(x^*) = 0$.

    Assume that $f$ is twice continuously differentiable, and let $d$ be any vector in $\R^n$. For all $\alpha\in \R$, the second order Taylor expansion yields
    \begin{equation*}
        f(x^*+\alpha d) - f(x^*) = \alpha \nabla f(x^*)^T d + \frac{\alpha^2}{2} d^T\nabla^2 f(x^*) d + o(\alpha^2).
    \end{equation*}
    Using the condition $\nabla f(x^*) = 0$ and the local optimality of $x^*$, we see that there is a sufficiently small $\varepsilon>0$ such that for all $\alpha\in (0,\varepsilon)$,
    \begin{equation*}
        0\le\dfrac{f(x^*+\alpha d) - f(x^*)}{\alpha^2} = \frac{1}{2}d^T\nabla^2 f(x^*)d + \frac{o(\alpha^2)}{\alpha^2}.
    \end{equation*}
    Taking the limit $\alpha\rightarrow 0$ and using the fact that $\lim_{\alpha\rightarrow 0} o(\alpha^2)/\alpha^2 = 0$, we obtain $d^T\nabla^2 f(x^*)d\ge 0$, showing that $\nabla^2 f(x^*)$ is positive semidefinite.
\end{proof}

\begin{remark}
    Suppose for a function $\mapping{f}{\R^n}{\R}$, the point $x^*$ is a local minimum of $f$ along every line that passes through $x^*$, i.e., the function 
    \begin{equation*}
        g(\alpha) = f(x^*+\alpha d)
    \end{equation*}
    is minimized at $\alpha = 0$ for all $d\in \R^n$. Then 
    \begin{equation*}
        0 = \bigvert{\dfrac{dg}{d\alpha}}{\alpha = 0} = \nabla f(x^*)^T d = 0,\quad \forall\ d\in \R^n.
    \end{equation*}
    This shows that $\nabla f(x^*) = 0$, i.e., first order necessary condition is satisfied at $x^*$. This only shows that $x^*$ is a stationary point and it need not be a local minimum of $f$. For example, consider $f(y,z) = (z-py^2)(z-qy^2)$, where $0<p<q$. Here $(0,0)$ is one such stationary point that minimizes $f$ along every line passing through it but $(0,0)$ is not a local minimum of $f$.
\end{remark}

\begin{proposition}[Second Order Sufficient Optimality Conditions]
    Let $\mapping{f}{\R^n}{\R}$ be twice continuously differentiable over an open set $U$. Suppose that a vector $x^*\in U$ satisfies the conditions
    \begin{equation*}
        \nabla f(x^*) = 0, \qquad \nabla^2 f(x^*) \succeq 0.
    \end{equation*}
    Then, $x^*$ is a strict unconstrained local minimum of $f$. In particular, there exists a scalar $\gamma>0$ and $\varepsilon>0$ such that for all $x\in \R^n$ with $\|x-x^*\|<\varepsilon$, we have 
    \begin{equation*}
        f(x)\ge f(x^*) + \dfrac{\gamma}{2}\|x^*-x\|^2.
    \end{equation*}
\end{proposition}

\begin{proof}
    Denote by $\lambda>0$ the smallest eigenvalue of $\nabla^2 f(x^*)$. Then we have 
    \begin{equation*}
        d^T \nabla^2 f(x^*) d\ge \lambda \|d\|^2,\qquad \forall\ d\in \R^n.
    \end{equation*}
    Using this relation, the hypothesis $\nabla f(x^*) = 0$, and the second order Taylor expansion, we have for all $d$
    \begin{align*}
        f(x^*+d) -f(x^*) &= \nabla f(x^*)^T d + \frac{1}{2}d^T \nabla^2 f(x^*)d + o(\|d\|^2) \\
        &\ge \dfrac{\lambda}{2}\|d\|^2 + o(\|d\|^2) \\
        &= \bigg(\dfrac{\lambda}{2} + \dfrac{o(\|d\|^2)}{\|d\|^2}\bigg) \|d\|^2.
    \end{align*}
    Choose any $\varepsilon>0$ and $\gamma>0$ such that for all $d\in \R^n$ with $\|d\|<\varepsilon$,
    \begin{equation*}
        \dfrac{\lambda}{2} + \dfrac{o(\|d\|^2)}{\|d\|^2}\ge \dfrac{\gamma}{2}.
    \end{equation*}
\end{proof}

% \newpage

% \begin{thebibliography}{9}
% \bibitem{Rockafellar} 
%     Rockafellar, R.T. Theory of Optimization. Princeton University Press, 1972.
    
% \bibitem{Fenchel}
%     Fenchel, W. Convex Cones, Sets, and Functions. Princeton University, 1953.
    
    
% \end{thebibliography}
    

\end{document}