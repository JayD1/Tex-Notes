\documentclass[11pt,a4paper]{article}
\usepackage[margin=1in]{geometry}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage[english]{babel}
\usepackage{amsthm}
\usepackage{amsfonts}
\usepackage{mathtools}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{amssymb}
\usepackage{enumerate}
\usepackage[hidelinks]{hyperref}
\usepackage{cancel}

\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,      
    urlcolor=blue,
    pdftitle={Statistical Learning Theory},
    pdfpagemode=FullScreen,
}


\author{Jayadev Naram}
\title{Statistical Learning Theory} 

\begin{document}

\date{}
\maketitle
\tableofcontents
\newpage

\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{corollary}{Corollary}[theorem]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{assume}{Assumption}

\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{example}[theorem]{Example}
\newtheorem{remark}[theorem]{Remark}

\newcommand{\R}{\mathbb{R}}
\newcommand{\A}{\mathcal{A}}
\newcommand{\M}{\mathcal{M}}
\newcommand{\N}{\mathcal{N}}
\newcommand{\h}{\mathcal{H}}
\newcommand{\T}{\mathcal{T}}
\newcommand{\Prob}{\mathbb{P}}
\newcommand{\Dist}{\mathcal{D}}
\newcommand{\perpProj}{\mathcal{P}^\perp}
\newcommand{\bb}{\mathbb{B}}
\newcommand{\Sprod}{\mathbb{S}_{xy}}
\newcommand{\highlight}[1]{\textsl{\textbf{#1}}}
\newcommand{\mapping}[3]{#1:#2\rightarrow #3}
\newcommand{\doubt}{\highlight{[??]}}
% \newcommand{\bigvert}[2]{#1{\raisebox{-.5ex}{$|$}_{#2}}}
\newcommand{\bigvert}[2]{\left.#1\right|_{#2}}
\newcommand{\sdnn}[1]{${#1}$}
\newcommand{\bsdnn}[1]{$\boldsymbol{#1}$}
\newcommand{\ifthen}[2]{\textbf{(#1)}\boldsymbol{\implies}\textbf{(#2)}}
\newcommand{\bsdn}[1]{\boldsymbol{#1}}
\newcommand{\forward}{$(\implies)$}
\newcommand{\converse}{$(\impliedby)$}
\newcommand{\Lt}[1]{\underset{#1\rightarrow 0}{Lt}}
\newcommand{\norm}[1]{\|#1\|}
\newcommand{\dparder}[2]{\dfrac{\partial #1}{\partial x_{#2}}}
\newcommand{\fparder}[2]{\frac{\partial #1}{\partial x_{#2}}}
\newcommand{\parder}[2]{\partial #1/\partial x_{#2}}
\newcommand{\parop}[1]{\dfrac{\partial}{\partial x_{#1}}}
\newcommand{\innerproduct}[2]{\langle #1, #2 \rangle}
\newcommand{\genst}{St_B(n,p)}
\newcommand{\igenst}[1]{St_{B_{#1}}(n_{#1},p)}
\newcommand{\realmat}[2]{\R^{#1\times #2}}
\newcommand{\Skew}{\mathcal{S}_{skew}(p)}
\newcommand{\Sym}{\mathcal{S}_{sym}(p)}
\newcommand{\XperpB}{X_{B^\perp}}
\newcommand{\polarRetr}{R^{polar}_X}
\newcommand{\qrRetr}{R^{QR}_X}
\newcommand{\vectransport}{\mathcal{T}}
\newcommand{\grad}{\text{grad}\,}
\newcommand{\hess}{\text{Hess}\,}

\section{Learning Framework}

\begin{itemize}
    \item {(Input).} The learner is provided with a finite sequence $S = ((x_1,y_1),\ldots,(x_m,y_m))$ which is called as \textit{training data}. Each instance is called as \textit{example} (or \textit{sample}). It is a pair $(x_i,y_i)$ which belongs to $X\times Y$ for some domain set $X$ containing data points and label set $Y$.
    \item {(Output).}  The learner outputs a map $\mapping{h}{X}{Y}$ which is called as a \textit{predictor}, a \textit{hypothesis}, or a \textit{classifier}. We use the notation $A(S)$ to denote the hypothesis that a learning algorithm $A$ returns upon receiving the training sequence $S$.
    \item {(Data-generation model).} The instances are generated by some probability distribution $\Dist$ over $X$ and the labels are assigned by some labeling function $\mapping{f}{X}{Y}$, i.e., the label y of a randomly sampled data point $x\sim \Dist$ is $y = f(x).$
    \item {(Measure of success).} The error of a hypothesis is the probability that it does not predict the correct label on a random data point generated by $\Dist$, i.e., for a hypothesis $\mapping{h}{X}{Y}$, the \textit{prediction error}, the \textit{generalization error}, or the \textit{true risk} is defined to be
    \begin{equation}
    L_{\Dist, f}(h) \coloneqq \underset{x\sim \Dist}{\Prob} [h(x)\neq f(x)] \coloneqq \Dist (\{x:\,h(x)\neq f(x)\}).
    \end{equation}
\end{itemize}

\begin{remark}
The learner is blind to the underlying distribution $\Dist$ over $X$ and to the labeling function $f$. The only way the learner can interact is through observing the training set.
\end{remark}

\section{ERM with Finite Hypothesis Classes}

Since the learner does not know $\Dist$ and $f$, the true risk is not directly available. Instead the learner can compute the \textit{training error} or \textit{empirical risk} which is defined to be
\begin{equation}
L_S(h) \coloneqq \dfrac{|\{(x_i,y_i)\in S:\,h(x_i)\neq y_i\}|}{m}.
\end{equation}

\noindent Then, the \textit{Empirical Risk Minimization} (ERM) rule selects a hypothesis $h_S$ that minimizes the empirical risk over the training samples $S$, i.e.,
\begin{equation*}
\text{ERM rule:}\quad h_S \in \underset{\mapping{h}{X}{Y}}{\text{argmin}}\; L_S(h).
\end{equation*}
We show that ERM rule might lead to undesirable hypotheses. 

\begin{remark}[{Overfitting}]
Assume that $X = [0,1]^2$ and $Y = \{0,1\}$. Let the probability distribution $\Dist$ be uniform distribution over $X$ and the labeling function $f$ determine the label to be $1$ for all data points in $[0,1/2]\times [0,1]$. Then note that area of region with labels 1 is $1/2$ and the total area of domain set is $1$. Consider the following hypothesis:
\begin{equation*}
h_S(x) = 
\begin{cases}
y_i\quad\text{if }\exists (x_i,y_i)\in S\text{ with } x_i = x\\
0  \quad \;\text{otherwise.}
\end{cases}
\end{equation*}
Then we see that the empirical risk $L_S(h_S) = 0$, but the generalization error $L_{\Dist, f}(h_S) = 1/2$ because $h_S$ correctly labels all the negative (i.e., label $0$) examples, but incorrectly labels all but finite positive (i.e., label $1$) examples. The ERM rule can choose this hypothesis which performs no better than random guess.
\end{remark}

To avoid overfitting, we provide the learner with some prior knowledge. We restrict the search space to a finite hypothesis class denoted by $\h$ and we apply ERM rule on it as before. Formally, 
\begin{equation*}
    \text{ERM rule on a finite hypothesis class:}\quad h_S \in \underset{h\in \h}{\text{argmin}}\; L_S(h).\qquad \qquad
\end{equation*}
Additionally, we make the following assumption:

\begin{assume}[{Realizability Assumption}]
There exists $h^*\in \h$ s.t. $L_{\Dist, f}(h^*) = 0$.
\end{assume}

By this assumption, we have with probability $1$ over random samples $S$ that $L_S(h^*) = 0$. This also implies that for every ERM hypothesis $h_S$ we have that $L_S(h_S) = 0$.

\begin{proposition}
Let $\h$ denote a finite hypothesis class. Let $\delta \in (0,1)$ and $\varepsilon >0$ and let $m$ be an integer that satisfies $m\ge \frac{\log(|\h|/\delta)}{\varepsilon}.$ Suppose the realizability assumption holds for a labeling function $f$, a distribution $\Dist$, and the hypothesis class $\mathcal{H}$. Then with probability of at least $1-\delta$ over the choice of an i.i.d. sample $S$ of size $m$, we have that for every ERM hypothesis $h_S$ that
\begin{equation*}
    L_{\Dist, f}(h_S)\le \varepsilon.
\end{equation*}
\end{proposition}

\begin{proof}
We will show that, over the choice of i.i.d. sample $S$ of size $m$, with probability not more than $\delta$, there exists a hypothesis $h_S$ that the ERM might select for which $L_{\Dist, f}(h_S)> \varepsilon$. Formally, we would like to upper bound
\begin{equation}\label{eqn:upper_bound_qty}
\Dist^m(\{\bigvert{S}{X}:\, L_{\Dist,f}(h_S)>\varepsilon\}),
\end{equation}
where $\bigvert{S}{X}$ is the restriction of $S$ to the domain $X$ and $\Dist^m$ is the probability of choosing i.i.d. sample $S\sim \Dist$ of size $m$. Let $\h_B\subseteq \h$ be the set of all bad hypothesis of $\h$, i.e.,
\begin{equation*}
\h_B = \{h\in \h:\, L_{\Dist, f}(h)>\varepsilon\}.
\end{equation*}
In addition, let $M$ be the set of all misleading samples:
\begin{equation*}
M = \{ \bigvert{S}{X}:\,\text{for some }h\in \h_B, L_S(h) = 0 \},
\end{equation*}
in other words, for every $\bigvert{S}{X}\in M$, there is a bad hypothesis $h\in \h_B$, that looks like a good hypothesis on $\bigvert{S}{X}$. By realizability assumption, we know that $L_S(h_S) = 0$. Then the event that $L_{\Dist,f}(h_S)>\varepsilon$ for a sample $S$ implies that $h_S\in \h_B$ with $L_S(h_S) = 0$ or equivalently $S\in M$. Therefore we have shown that
\begin{equation}\label{eqn:set_inclusion_expr}
\{\bigvert{S}{X}:\, L_{\Dist,f}(h_S)>\varepsilon\} \subseteq M = \bigcup_{h\in \h_B} \{\bigvert{S}{x}:\, L_S(h) = 0\}.
\end{equation}
Hence,
\begin{align*}
\Dist^m(\{\bigvert{S}{X}:\, L_{\Dist,f}(h_S)>\varepsilon\}) &\le \Dist^m(\cup_{h\in \h_B} \{\bigvert{S}{x}:\, L_S(h) = 0\})\\
&\overset{(*)}{\le} \sum_{h\in \h_B}\Dist^m(\{\bigvert{S}{x}:\, L_S(h) = 0\})\\
&= \sum_{h\in \h_B}\Dist^m(\{\bigvert{S}{x}:\, \forall i, h(x_i) = f(x_i)\}) \\
&\overset{(\dagger)}{=} \sum_{h\in \h_B}\prod_{i=1}^m\Dist(\{x_i:\, h(x_i) = f(x_i)\})= \sum_{h\in \h_B}\prod_{i=1}^m(1-L_{\Dist,f}(h))\\
&\le \sum_{h\in \h_B}\prod_{i=1}^m(1-\varepsilon) \le \sum_{h\in \h_B}\prod_{i=1}^m e^{-\varepsilon} = |\h_B|e^{-\varepsilon m}
\le |\h|e^{-\varepsilon m},
\end{align*}
where $(*)$ is obtained by applying union bound and $(\dagger)$ is obtained by using i.i.d. assumption. Using the fact that $m\ge \frac{\log(|\h|/\delta)}{\varepsilon}$, we have 
\begin{equation*}
\Dist^m(\{\bigvert{S}{X}:\, L_{\Dist,f}(h_S)>\varepsilon\})\le |\h|e^{-\varepsilon m} \le \delta
\end{equation*}
which is as required.
\end{proof}

\section{PAC Learnability}

\begin{definition}[{PAC Learnability}] \label{def:PAC_learning}
A hypothesis class $\h$ is \highlight{P}robably \highlight{A}pproximately \highlight{C}orrect (PAC) learnable if there exists a function $\mapping{m_\h}{(0,1)^2}{\mathbb{N}}$ and a learning algorithm with the following property: For every $\varepsilon,\delta\in (0,1)$, for every distribution $\Dist$ over $X$, and for every labeling function $\mapping{f}{X}{\{0,1\}}$, if the realizability assumption holds w.r.t. $\h,\Dist, f$, then when running the learning algorithm on $m\ge m_\h(\varepsilon,\delta)$ i.i.d.\,examples generated by $\Dist$ and labeled by $f$, the algorithm returns a hypothesis $h$ such that with probability of at least $1-\delta$ (over the choice of the examples), we have $L_{\Dist,f}(h)\le \varepsilon$.
\end{definition}

The definition contains two approximation parameters which are justified by the following propositions.

\begin{proposition}
No learner can guarantee to find $h$ such that $L_{\Dist, f}(h) = 0$ with probability $1$ over random samples $S$.
\end{proposition}

\begin{proof}
For some $\varepsilon\in (0,1)$ take $X = \{x_1,x_2\}$ and $\Dist(x_1) = 1 - \varepsilon$, $\Dist(x_2) = \varepsilon$. Then the probability of not seeing $x_2$ at all among $m$ i.i.d. examples is $(1-\varepsilon)^m\approx e^{-\varepsilon m}$. So, if $\varepsilon \ll 1/m$, then the learner is likely to not see $x_2$ at all, and therefore true risk is never equal to $0$.
\end{proof}

Thus we need the accuracy parameter $\varepsilon$ which determines how far the output hypothesis can be from the optimal one (this corresponds to the ``approximately correct"), i.e., for $\varepsilon\in (0,1)$, the learner tries to find a hypothesis $h$ such that $L_{\Dist, f}(h)\le \varepsilon$.

\begin{proposition}
No learner can guarantee to find $h$ such that $L_{\Dist, f}(h) \le \varepsilon,$ for $\varepsilon\in (0,1)$ with probability $1$ over random samples $S$.
\end{proposition}

\begin{proof}
The proof is similar the previous proposition. The input to the learner is randomly generated samples. Then there is always a (very small) chance to see the same example again and again.
\end{proof}

Thus, the confidence parameter $\delta$ is needed to indicate how likely the hypothesis is to meet that accuracy requirement (corresponds to the ``probably" part of ``PAC").

\begin{remark}
The function $\mapping{m_\h}{(0,1)^2}{\mathbb{N}}$ determines the \textit{sample complexity} of learning $\h$, i.e., how many examples are required to guarantee a probably approximately correct solution. Further note that if $\h$ is PAC learnable, there are many functions $m_\h$ that satisfy the requirements in Def. \ref{def:PAC_learning}. Therefore, to be precise we define the sample complexity of learning $\h$ to be the ``minimal function", in the sense that for any $\varepsilon, \delta$, the number $m_\h(\varepsilon,\delta)$ is minimal that satisfies the requirements of PAC learning with accuracy $\varepsilon$ and confidence $\delta$.
\end{remark}

\begin{corollary}
Every finite hypothesis class is PAC learnable with sample complexity 
\begin{equation*}
m_\h(\varepsilon,\delta) \le \bigg\lceil \dfrac{\log(\h/\delta)}{\varepsilon} \bigg\rceil.
\end{equation*}
\end{corollary}

The next example shows that there are some infinite hypothesis classes that are PAC learnable.

\begin{example}[{Axis aligned rectangles}]\label{ex:axis_aligned_rect}
Suppose $a_1\le b_1$, $a_2\le b_2$, define the axis aligned rectangle classifier $h_{(a_1,b_1,a_2,b_2)}$ by
\begin{equation*}
h_{(a_1,b_1,a_2,b_2)}(x_1,x_2) = 
\begin{cases}
1\qquad \text{if } a_1\le x_1\le b_1 \text{ and } a_2\le x_2\le b_2 \\
0\qquad \text{otherwise.}
\end{cases}
\end{equation*}
Let $R(a_1,b_1,a_2,b_2)$ denote the rectangle corresponding to $h_{(a_1,b_1,a_2,b_2)}$. The class of all axis aligned rectangles in $\R^2$ is defined as
\begin{equation*}
\h^2_{rec} = \{h_{(a_1,b_1,a_2,b_2)}:\, a_1\le b_1,a_2\le b_2\}.
\end{equation*}
Let $\Dist$ be some distribution over $\R^2$ and $R^* = R({a_1^*,b_1^*,a_2^*,b_2^*})$ be the rectangle that generates the labels such that $\Dist(R^*) = 1$. Let $f$ be the labeling function corresponding to the rectangle $R^*$.

We show that $\h^2_{rec}$ is PAC learnable. Define a simple ERM algorithm as follows: return the smallest rectangle enclosing all the positive examples in the training set. It is easy to see that the empirical risk of the selected rectangle over the training sample is $0$.

Let $R(S)$ be rectangle returned by ERM rule. Choose $\varepsilon\in(0,1)$ and choose $a_1\ge a_1^*,b_1\le b_1^*,a_2\ge a_2^*,b_2\le b_2^*$ such that $\Dist(R_1) = \Dist(R_2) = \Dist(R_3) = \Dist(R_4) = \varepsilon/4$ where $R_1 = R(a_1^*,a_1,a_2^*,b_2^*)$, $R_2 = R({b_1,b_1^*,a_2^*,b_2^*})$, $R_3 = R({a_1^*,b_1^*,a_2^*,a_2})$ and $R_4 = R({a_1^*,b_1^*,b_2,b_2^*})$.

Note that $L_{\Dist,f}(h_S) = \Dist(R^* - R(S))$. If $S$ contains (positive) examples in all of the rectangles $R_1,R_2,R_3,R_4$, then $R^*-R(S)\subseteq R_1\cup R_2\cup R_3\cup R_4$ and consequently $L_{\Dist,f}(h_S) \le \varepsilon$. Then we have
\begin{align*}
\Dist^m(\{\bigvert{S}{X}:\,L_{\Dist,f}(h_S)> \varepsilon\}) &\le 
\Dist^m(\{\bigvert{S}{X}:\, \bigvert{S}{X}\cap R_i = \emptyset \text{ for some }i\in \{1,2,3,4\} \})\\
&\le \sum_{i=1}^4 \Dist^m(\{\bigvert{S}{X}:\, \bigvert{S}{X}\cap R_i = \emptyset\})\\
&\le \sum_{i=1}^4 (1-\varepsilon/4)^m = 4(1-\varepsilon/4)^m \le 4e^{-m\varepsilon/4}.
\end{align*}
Setting $m\ge \frac{4\log{ 4/\delta}}{\varepsilon}$ for some $\delta\in(0,1)$ we have 
\begin{equation*}
\Dist^m(\{\bigvert{S}{X}:\,L_{\Dist,f}(h_S)> \varepsilon\}) \le  4(1-\varepsilon/4)^m \le \delta.
\end{equation*}
\end{example}

\section{General Learning Framework}

In many practical problems the realizability assumption does not hold. Furthermore, it is more realistic not to assume that the labels are fully determined by features we measure on input elements. Following these lines, we generalize the learner framework:

\begin{itemize}
\item {(Realistic Data-generation model).} Let $\Dist$ be a joint probability distribution over $X\times Y$. The distribution $\Dist$ can be seen to be composed of two components: (i) a distribution $\Dist_X$ over unlabeled domain points (called the marginal distribution) and (ii) a conditional probability over labels for each domain point $\Dist((x,y)|x)$.

\item {(Revised True Error).} We redefine the true error (or risk) of a hypothesis $h$ to be
\begin{equation*}
L_\Dist (h) \equiv \Prob_{(x,y)\sim \Dist}[h(x)\neq y] = \Dist(\{(x,y):\,h(x)\neq y\}).
\end{equation*}
\end{itemize}

\begin{proposition}[{Bayes optimal predictor}]
Given any probability distribution $\Dist$ over $X\times\{0,1\}$, the Bayes predictor $f_\Dist$ defined as
\begin{equation*}
f_\Dist(x) = \begin{cases}
1,\quad \text{if }\ \Prob[y=1|x = x]\ge 1/2\\
0\,\quad \text{otherwise},
\end{cases}
\end{equation*}
is optimal, in the sense that for every predictor $\mapping{g}{X}{\{0,1\}}$, $L_\Dist(f_\Dist)\le L_\Dist(g)$.
\end{proposition}

\begin{proof}
Fix $x\in X$ and $\Prob[y = 1|x = x] = \alpha_x$. Note that $\Prob[f_\Dist(x) = 1|x = x]$ is 1 if $\alpha_x\ge 1/2$ and 0 if $\alpha_x< 1/2$, i.e., $\Prob[f_\Dist(x) = 1|x = x] = \mathbb{I}_{[\alpha_x\ge 1/2]}$ and $\Prob[f_\Dist(x) = 0|x = x] = \mathbb{I}_{[\alpha_x<1/2]}$. Then
\begin{align*}
\Prob[f_\Dist(x)\neq y|x=x] &= \Prob[f_\Dist(x) = 1|x=x] \Prob[y=0|x=x]+ \Prob[f_\Dist(x) = 0|x=x] \Prob[y=1|x=x] \\
&= \mathbb{I}_{[\alpha_x\ge 1/2]} \Prob[y=0|x=x]+ \mathbb{I}_{[\alpha_x< 1/2]} \Prob[y=1|x=x] \\
&= \mathbb{I}_{[\alpha_x\ge 1/2]} (1-\alpha_x)+ \mathbb{I}_{[\alpha_x< 1/2]} \alpha_x \\
&= \text{min}\{\alpha_x , 1-\alpha_x\}.
\end{align*}
And similarly for $g$, we have
\begin{align*}
\Prob[g(x)\neq y|x=x] &= \Prob[g(x) = 1|x=x] \Prob[y=0|x=x]+ \Prob[g(x) = 0|x=x] \Prob[y=1|x=x] \\
&= \Prob[g(x) = 1|x=x] (1-\alpha_x)+ \Prob[g(x) = 0|x=x] \alpha_x \\
&\ge \text{min}\{\alpha_x , 1-\alpha_x\}(\Prob[g(x) = 1|x=x] + \Prob[g(x) = 0|x=x])\\
&= \text{min}\{\alpha_x , 1-\alpha_x\} = \Prob[f_\Dist(x)\neq y|x=x].
\end{align*}
The statement follows now due to the fact that the above statement is true for
every $x\in X$.
\end{proof}

We make a further generalization in the measure of success.
\begin{itemize}
\item {(Generalized Loss function).} Given any set $\h$ (that plays the role of hypothesis class) and some domain $Z$, the loss function is a mapping $\mapping{l}{\h\times Z}{\R_+}$. The true error or risk of a hypothesis $h\in \h$ is defined as
\begin{equation*}
L_\Dist(h) \coloneqq \underset{z\sim\Dist}{\mathbb{E}}\ [l(h,z)].
\end{equation*}
Similarly, we define the empirical risk to be the expected loss over a given sample $S = (z_1,\ldots,z_m)\in Z^m$, i.e.,
\begin{equation*}
L_S(h) \coloneqq \dfrac{1}{m}\sum_{i=1}^m l(h,z_i).
\end{equation*}
\end{itemize}

\begin{example}[ {\bsdnn{0-1} loss}]
The domain of $z$ is the set $X\times Y$ and the loss function is 
\begin{equation*}
    l_{0-1}(h,(x,y)) = \begin{cases}
    0\quad \text{if }h(x) = y\\
    1\quad \text{if }h(x)\neq y.
    \end{cases}
\end{equation*}
\end{example}

\section{Agnostic PAC Learnability}

\begin{definition}[{Agnostic PAC Learnability}]
A hypothesis class $\h$ is agnostic PAC learnable w.r.t. a set $Z$ and a loss function $\mapping{l}{\h\times Z}{\R_+}$, if there exists a function $\mapping{m_\h}{(0,1)^2}{\mathbb{N}}$ and a learning algorithm with the following property: For every $\varepsilon,\delta\in(0,1)$ and for every distribution $\Dist$ over $Z$, when running the algorithm on $m\ge m_\h(\varepsilon,\delta)$ i.i.d. examples generated by $\Dist$, the algorithm returns $h\in\h$ such that, with probability of at least $1-\delta$, over the choice of the $m$ training examples, we have
\begin{equation*}
    L_\Dist(h)\le \underset{h'\in\h}{\text{min}}\,L_\Dist(h')+\varepsilon,
\end{equation*}
where $L_\Dist(h) = \underset{z\sim\Dist}{\mathbb{E}}\ [l(h,z)]$.
\end{definition}

\begin{remark}
In the above definition, for every $h\in \h$, we view the function $\mapping{l(h,\cdot)}{Z}{\R_+}$ as a random variable and define $L_\Dist(h)$ to be the expectation of this random variable.
\end{remark}

\begin{remark}
Agnostic PAC learnability $\implies$ PAC learnability. So, agnostic PAC learnability is a stronger requirement than PAC learnability.
\end{remark}

\begin{definition}[{${\varepsilon}$-representative sample}]
A training set $S$ is called $\varepsilon$-representative (w.r.t. domain $Z$, hypothesis class $\h$, loss function $l$, and distribution $\Dist$) if 
\begin{equation*}
    |L_S(h)-L_\Dist(h)|\le \varepsilon,\quad \forall\, h\in\h.
\end{equation*}
\end{definition}

\begin{proposition}
Assume that a training set $S$ is $\frac{\varepsilon}{2}$-representative (w.r.t. domain $Z$, hypothesis class $\h$, loss function $l$, and distribution $\Dist$). Then, any output of $\text{ERM}_\h(S)$, namely any $h_S\in\text{argmin}_{h\in\h} L_S(h)$, satisfies
\begin{equation*}
    L_\Dist(h_S)\le \underset{h\in\h}{\text{min}}\; L_\Dist(h) + \varepsilon.
\end{equation*}
\end{proposition}

\begin{proof}
For every $h\in\h$,
\begin{equation*}
L_\Dist(h_S)\le L_S(h_S) + \varepsilon/2 \le L_S(h) + \varepsilon/2 \le L_\Dist(h) + \varepsilon/2 + \varepsilon/2 = L_\Dist(h) + \varepsilon.
\end{equation*}
\end{proof}

\begin{definition}[{Uniform Convergence}]
We say that a hypothesis class $\h$ has the uniform convergence property (w.r.t. a domain $Z$ and a loss function $l$) if there exists a function $\mapping{m_\h^{UC}}{(0,1)^2}{\mathbb{N}}$ such that for every $\varepsilon,\delta\in(0,1)$ and for every probability distribution $\Dist$ over $Z$, if $S$ is a sample of size $m\ge m_\h^{UC}(\varepsilon,\delta)$ examples drawn i.i.d. according to $\Dist$, then with probability of at least $1-\delta$ we have S is $\varepsilon$-representative.
\end{definition}

\begin{remark}
The term uniform in the above definition refers to having a fixed sample size that works for all members of $\h$ and over all possible probability distributions over the domain.
\end{remark}

\begin{corollary}
If a class $\h$ has the uniform convergence property with a function $m_\h^{UC}$, then the class is agnostic PAC learnable with the sample complexity $m_\h(\varepsilon,\delta)\le m_\h^{UC}(\varepsilon/2,\delta)$. Furthermore, in that case, the ERM$_\h$ paradigm is a successful agnostic PAC learner for $\h$.
\end{corollary}

\begin{lemma}[{Hoeffding's Inequality}]
Let $\theta_1,\ldots,\theta_m$ be a sequence of i.i.d. random variables and let $\bar{\theta} = \tfrac{1}{m} \sum_{i=1}^m \theta_i$. Assume that $\mathbb{E}(\bar{\theta}) = \mu$ and $\Prob[a\le \theta_i\le b] = 1$. Then for any $\varepsilon>0$, we have
\begin{equation*}
\Prob\Bigg[ \Bigg| \dfrac{1}{m}\sum_{i=1}^m\theta_i - \mu \Bigg|>\varepsilon \Bigg] \le 2 \exp{\bigg(\dfrac{-2m\varepsilon^2}{(b-a)^2} \bigg)}.
\end{equation*}
\end{lemma}

% \begin{proof}

% \end{proof}

\begin{proposition}
Let $\h$ be a finite hypothesis class, $Z$ be a domain, and $\mapping{l}{\h\times Z}{[0,1]}$ be a loss function. Then, $\h$ enjoys the uniform convergence property with sample complexity
\begin{equation*}
m_\h^{UC}(\varepsilon,\delta)\le \bigg\lceil \dfrac{\log(2\h/\delta)}{2\varepsilon^2} \bigg\rceil.
\end{equation*}
Furthermore, the class $\h$ is agnostic PAC learnable using the ERM algorithm with sample complexity
\begin{equation*}
m_\h(\varepsilon,\delta)\le m_\h^{UC}(\varepsilon/2,\delta)\le \bigg\lceil \dfrac{2\log(2\h/\delta)}{\varepsilon^2} \bigg\rceil.
\end{equation*}
\end{proposition}

\begin{proof}
Fix $\varepsilon,\delta\in(0,1)$. We need to find a sample of size $m$ that guarantees that for any $\Dist$, with probability of at least $1-\delta$ of the choice of $S = (z_1,\ldots,z_m)$ sampled i.i.d. from $\Dist$ we have that for all $h\in\h$, $|L_S(h)-L_\Dist(h)|\le \varepsilon$, i.e.,
\begin{equation*}
\Dist^m(\{S:\,\forall h\in\h,\,|L_S(h)-L_\Dist(h)|\le \varepsilon\}) \ge 1-\delta.
\end{equation*}
Equivalently, we need to show that 
\begin{equation*}
\Dist^m(\{S:\,\exists h\in\h,\,|L_S(h)-L_\Dist(h)|> \varepsilon\}) < \delta.
\end{equation*}
Writing
\begin{equation*}
\{S:\,\exists h\in\h,\,|L_S(h)-L_\Dist(h)|> \varepsilon\} = \bigcup_{h\in\h} \{S:\,|L_S(h)-L_\Dist(h)|> \varepsilon\},
\end{equation*}
and applying the union bound we get
\begin{equation*}
\Dist^m(\{S:\,\exists h\in\h,\,|L_S(h)-L_\Dist(h)|> \varepsilon\}) \le 
\sum_{h\in\h} \Dist^m(\{S:\,|L_S(h)-L_\Dist(h)|> \varepsilon\}).
\end{equation*}
Let $\theta_i$ be the random variable $l(h,z_i)$. For a fixed $h$ and $z_1,\ldots,z_m$ sampled i.i.d., it follows that $\theta_1,\ldots,\theta_m$ are also i.i.d. random variables. Furthermore, $L_S(h) = \frac{1}{m}\theta_i$ and $L_\Dist(h) = \mu$, where $\mu = \mathbb{E}[\theta_i]$. Since the range of $l$ is $[0,1]$, we have that $\theta_i\in [0,1]$ for $i = 1,\ldots,m$. Then by applying Hoeffding's inequality we get
\begin{equation*}
\Dist^m(\{S:\,|L_S(h)-L_\Dist(h)|> \varepsilon\}) = \Prob\Bigg[ \Bigg| \dfrac{1}{m}\sum_{i=1}^m\theta_i - \mu \Bigg|>\varepsilon \Bigg] \le 2 \exp{(-2m\varepsilon^2)}.
\end{equation*}
Then
\begin{align*}
\Dist^m(\{S:\,\exists h\in\h,\,|L_S(h)-L_\Dist(h)|> \varepsilon\}) &\le 
\sum_{h\in\h} 2 \exp{(-2m\varepsilon^2)} \\
&= 2|\h| \exp{(-2m\varepsilon^2)}.
\end{align*}
Finally, if we choose $m\ge \frac{\log{(2|\h|/\delta)}}{2\varepsilon^2}$, we obtain
\begin{equation*}
\Dist^m(\{S:\,\exists h\in\h,\,|L_S(h)-L_\Dist(h)|> \varepsilon\}) \le \delta.
\end{equation*}
\end{proof}

\begin{remark}[{``Discretization Trick"}]
While the preceding proposition only applies to finite hypothesis classes, there is a simple trick that allows us to get a very good estimate of the practical sample complexity of infinite hypothesis classes. Consider an infinite hypothesis class parameterized by $d$ parameters, such as $h(x;w) = \innerproduct{x}{w},$ for all $x\in\R^d$ and parameter $w\in\R^d$. In practice we store real numbers in a computer using floating point representation, say of $64$ bits. Then it follows that in practice, the actual size of the hypothesis class is $2^{64d}$. Applying the previous proposition, we obtain that the sample complexity of such classes is bounded by $\frac{128d+2\log{(2/\delta)}}{\varepsilon^2}$. This upper bound on the sample complexity has the deficiency of being dependent on the specific representation of real numbers used. 
\end{remark}

\section{The Bias-Complexity Trade-off}

\begin{lemma}[Modified Markov's Inequality]\label{lemma:markov_ineq}
    Let $Z$ be a random variable that takes values in $[0,1]$. Assume that $E[Z] = \mu$. Then for any $a\in (0,1)$,
    \begin{equation*}
        P[Z>a]\ge \dfrac{\mu-a}{1-a}.
    \end{equation*}
\end{lemma}

\begin{proposition}[{No-Free-Lunch}]
Let $A$ be a learning algorithm for the task of binary classification w.r.t. the $0-1$ loss over a domain $X$. Let $m$ be any number smaller than $|X|/2$, representing a training set size. Then, there exists a distribution $\Dist$ over $X\times \{0,1\}$ such that:
\begin{enumerate}[(a)]
    \item There exists a function $\mapping{f}{X}{\{0,1\}}$ with $L_\Dist(f) = 0$.
    \item With probability at least $1/7$ over the choice of $S\sim \Dist^m$ we have that $L_\Dist(A(S))\ge 1/8$.
\end{enumerate}
\end{proposition}

\begin{proof}
Let $C$ be a subset of $X$ of size $2m$. There are $T = 2^{2m}$ functions from $C$ to $\{0,1\}$. Denote these functions by $f_1,\ldots,f_T$. For each such function, let $\Dist_i$ be a distribution over $C\times \{0,1\}$ defined by 
\begin{equation*}
    \Dist_i(\{(x,y)\}) = \begin{cases}
    1/2m,\quad\text{if }y = f_i(x)\\
    0,\hspace{1.2cm}\text{otherwise}.
    \end{cases}
\end{equation*}
Clearly, $L_{\Dist_i}(f_i) = 0$. 

We will show that for every algorithm $A$ that receives a training set of $m$ examples from $C\times \{0,1\}$ and returns a function $\mapping{A(S)}{C}{\{0,1\}}$, it holds that 
\begin{equation}\label{eqn:main_res_L_D}
    \max_{i\in[T]} \underset{S\sim \Dist_i^m}{\mathbb{E}}[L_{\Dist_i}(A(S))] \ge 1/4.
\end{equation}
This means that for every algorithm, $A^\prime$, that receives a training set of $m$ examples over $X\times \{0,1\}$, such that $L_\Dist (f) = 0$ and
\begin{equation*}
    \underset{S\sim \Dist^m}{\mathbb{E}}[L_\Dist(A^\prime(S))] \ge 1/4.
\end{equation*}
The result then follows from Lemma \ref{lemma:markov_ineq}.

There are $k = (2m)^m$ possible sequences of $m$ examples from $C$. Denote these sequences by $S_1,\ldots,S_k$. Also, if $S_j = (x_1,\ldots,x_m)$ we denote by $S_j^i$ the sequence containing instances in $S_j$ labeled by the function $f_i$, i.e., $S_j^i= ((x_1,f_i(x_1)),\ldots,(x_m,f_i(x_m)))$. If the distribution is $\Dist_i$ then the possible training sets $A$ can receive are $S_1^i,\ldots,S^i_k$, and all these training sets have the same probability of being sampled. Therefore,
\begin{equation}\label{eqn:expectation_L}
    \underset{S\sim\Dist_i^m}{\mathbb{E}}[L_{\Dist_i}(A(S))] = \frac{1}{k}\sum_{j=1}^k L_{\Dist_i}(A(S_j^i)).
\end{equation}
Taking the maximum of this quantity over $i = 1,\ldots,T$, we obtain
\begin{align}
    \max_{i\in[T]} \dfrac{1}{k} \sum_{j = 1}^k L_{\Dist_i}(A(S_j^i)) &\ge \dfrac{1}{T}\sum_{i=1}^T \dfrac{1}{k} \sum_{j = 1}^k L_{\Dist_i}(A(S_j^i)) \nonumber \\
    &= \dfrac{1}{k} \sum_{j = 1}^k \dfrac{1}{T}\sum_{i=1}^T L_{\Dist_i}(A(S_j^i)) \nonumber \\
    &\ge \min_{j\in[k]} \dfrac{1}{T}\sum_{i=1}^T L_{\Dist_i}(A(S_j^i)).\label{eqn:max_L}
\end{align}
Next fix some $j\in[k]$ and denote $S_j = (x_1,\ldots,x_m)$ and let $v_1,\ldots,v_p$ be the examples in $C$ that do not appear in $S_j$. Clearly $p\ge m$, therefore, for every function $\mapping{h}{C}{\{0,1\}}$ and every $i$ we have
\begin{equation*}
    L_{\Dist_i}(h) = \dfrac{1}{2m} \sum_{x\in C}\mathbb{I}_{[h(x)\neq f_i(x)]} \ge \dfrac{1}{2m} \sum_{r=1}^p \mathbb{I}_{[h(v_r)\neq f_i(v_r)]} \ge \dfrac{1}{2p} \sum_{r=1}^p \mathbb{I}_{[h(v_r)\neq f_i(v_r)]}.
\end{equation*}
Hence,
\begin{align}
    \dfrac{1}{T}\sum_{i=1}^T L_{\Dist_i}(A(S_j^i)) &\ge \dfrac{1}{T}\sum_{i=1}^T \dfrac{1}{2p} \sum_{r=1}^p \mathbb{I}_{[A(S_j^i)(v_r)\neq f_i(v_r)]}\nonumber \\
    &= \dfrac{1}{2p} \sum_{r=1}^p \dfrac{1}{T}\sum_{i=1}^T \mathbb{I}_{[A(S_j^i)(v_r)\neq f_i(v_r)]} \nonumber\\
    &\ge \dfrac{1}{2} \min_{r\in[p]} \dfrac{1}{T}\sum_{i=1}^T \mathbb{I}_{[A(S_j^i)(v_r)\neq f_i(v_r)]}. \label{eqn:expectation_L_D}
\end{align}
Next fix some $r\in [p]$. We can partition all the functions in $f_1,\ldots, f_T$ into $T/2$ disjoint pairs, where for a pair $(f_i,f_{i^\prime})$ we have that $f_i(c)\neq f_{i^\prime}(c)$ iff $c = v_r$. Since $v_r\notin S_j$, for such a pair we must have $S_j^i = S_j^{i^\prime}$, so that $A(S_j^i) = A(S_j^{i^\prime})$ and 
\begin{equation}\label{eqn:id_S}
    \mathbb{I}_{[A(S_j^i)(v_r)\neq f_i(v_r)]} + \mathbb{I}_{[A(S_j^{i^\prime})(v_r)\neq f_{i^\prime}(v_r)]} = 1 \implies \dfrac{1}{T}\sum_{i=1}^T \mathbb{I}_{[A(S_j^i)(v_r)\neq f_i(v_r)]} = \dfrac{1}{2}.
\end{equation}
Combining \eqref{eqn:expectation_L}, \eqref{eqn:max_L}, \eqref{eqn:expectation_L_D} and \eqref{eqn:id_S} we obtain \eqref{eqn:main_res_L_D}.  
\end{proof}

\begin{corollary}
    Let $X$ be an infinite domain set and let $\h$ be the set of all functions from $X$ to $\{0,1\}$. Then, $\h$ is not PAC learnable.
\end{corollary}

\begin{proof}
    Suppose the class if PAC learnable and choose $\varepsilon<1/8$ and $\delta<1/7$. By the definition of PAC learnability, there must be some learning algorithm $A$ and an integer $m = m(\varepsilon, \delta)$, such that for any data-generating distribution over $X\times \{0,1\}$, if for some function $\mapping{f}{X}{\{0,1\}}$, $L_\Dist(f) = 0$, then with probability greater than $1-\delta$ when $A$ is applied to samples $S$ of size $m$, generated i.i.d. by $\Dist$, $L_\Dist(A(S))\le \varepsilon$. However, applying No-Free-Lunch theorem, since $|X|>2m$, for every learning algorithm (and in particular for the algorithm $A$), there exists a distribution $\Dist$ such that with probability greater than $1/7>\delta$, $L_\Dist(A(S))>1/8>\varepsilon$, which leads to a contradiction.
\end{proof}

Error Decomposition

\section{VC Dimension}

\begin{definition}[Restriction of $\h$ to $C$] 
Let $\h$ be a set of functions from $X$ to $\{0,1\}$ and let $C = \{c_1,\ldots,c_m\}\subseteq X$. The restriction of $\h$ to $C$ is the set of functions from $C$ to $\{0,1\}$ that can be derived from $\h$, i.e.,
\begin{equation*}
    \h_C = \{(h(c_1),\ldots,h(c_m))\;:\; h\in \h\},
\end{equation*}
where we represent each function from $C$ to $\{0,1\}$ as a vector in $\{0,1\}^{|C|}$.
\end{definition}

\begin{definition}[Shattering]
    A hypothesis class $\h$ shatters a finite set $C\subseteq X$ if the restriction of $\h$ to $C$ is the set of all functions from $C$ to $\{0,1\}$, i.e., $|\h_C| = 2^{|C|}$.
\end{definition}

\begin{corollary}\label{cor:no_free_lunch}
    Let $\h$ be a hypothesis class of functions from $X$ to $\{0,1\}$. Let $m$ be a training set size. Assume that there exists a set $C\subseteq X$ of size $2m$ that is shattered by $\h$. Then, for any learning algorithm $A$ there exists a distribution $\Dist$ over $X\times\{0,1\}$ and a predictor $h\in \h$ such that $L_\Dist(h) = 0$ but with probability of at least $1/7$ over the choice of $S\sim \Dist^m$ we have that $L_\Dist(A(S))\ge 1/8$.
\end{corollary}

\begin{definition}[VC-dimension]
    The VC-dimension of a hypothesis class $\h$, denoted by VCdim$(\h)$, is the maximal size of a set $C\subseteq X$ that can be shattered by $\h$. If $\h$ shatter sets of arbitraryily large size we say that $\h$ has infinite VC-dimension.
\end{definition}

\begin{proposition}
    Let $\h$ be a class of infinite VC-dimension. Then, $\h$ is not PAC learnable.
\end{proposition}

\begin{proof}
    Since $\h$ has infinite VC-dimension, for any training set size $m$, there exists a shattered set of size $2m$, and the claim follows by Cor. \ref{cor:no_free_lunch}
\end{proof}

\begin{example}
    To show that VCdim$(\h) = d$ we need to show that
    \begin{enumerate}
        \item There exists a set $C$ of size $d$ that is shattered by $\h$.
        \item Every set $C$ of size $d+1$ is not shattered by $\h$.
    \end{enumerate}
    \begin{itemize}
        \item (Threshold Functions). Let $\h$ be the set of all threshold functions over the real line, namely, $\h = \{h_a\;:\;a\in \R\}$ where $\mapping{h_a}{\R}{\{0,1\}}$ is defined as $h_a(x) = \mathbb{I}_{[x<a]}$. Take a set $C = \{c_1\}$. Now, if we take $a = c_1+1$, then we have $h_a(c_1) = 1$, and if we take $a = c_1-1$, then we have $h_a(c_1) = 0$. Therefore, $\h_C$ is the set of all functions from $C$ to $\{0,1\}$, and $\h$ shatters $C$. Now if we take $C = \{c_1,c_2\}$, where $c_1\le c_2$. No $h\in \h$ can account for the labeling $(0,1)$, because any threshold that assigns the label $0$ to $c_1$ must assign the label $0$ to $c_2$ as well. Therefore $C$ is not shattered by $\h$ showing that VCdim$(\h) = 1$.

        \item (Intervals). Let $\h$ be the class of intervals over $\R$, namely, $\h = \{h_{a,b}\;:\;a,b\in ,a<b\}$, where $\mapping{h_{a,b}}{\R}{\{0,1\}}$ is defined as $h_{a,b}(x) = \mathbb{I}_{[x\in[a,b]]}$. Take the set $C = \{1,2\}$. Then, $\h$ shatters $C$. Now take an arbitrary set $C = \{c_1,c_2,c_3\}$ and assume WLOG that $c_1\le c_2\le c_3$. Then, the labeling $(1,0,1)$ cannot be obtained by an interval and therefore $\h$ does not shatter $C$ showing that VCdim$(\h) = 2$.

        \item (Axis Aligned Rectangles). Let $\h$ be the set of all axis aligned rectangles (see Example \ref{ex:axis_aligned_rect}). Take the set $C = \{(0,1),(1,0),(2,1),(1,2)\}$. Then, $\h$ shatters $C$. Now, consider any set $C\subseteq\R^2$ of $5$ points. In $C$, take a leftmost point (whose first corrdinate is the smallest in $C$), a rightmost point, a lowest point, and a highest point. WLOG, denote $C = \{c_1,c_2,c_3,c_4,c_5\}$ and let $c_5$ be the point that was not selected. Then the labeling $(1,1,1,1,0)$ cannot be obtained by an axis aligned rectangle and therefore $\h$ does not shatter $C$ showing that VCdim$(\h) = 4$.

        % \item (Finite Classes). 
    \end{itemize}
\end{example}

\begin{theorem}[The Fundamental Theorem of Statistical Learning]
    Let $\h$ be a hypothesis class of functions from a domain $X$ to $\{0,1\}$ and let the loss function be the $0$-$1$ loss. Then, the following statements are equivalent:
    \begin{enumerate}[(a)]
        \item $\h$ has the uniform convergence property.
        \item Any ERM rule is successful agnostic PAC learner for $\h$.
        \item $\h$ is agnostic PAC learnable.
        \item $\h$ is PAC learnable.
        \item Any ERM rule is a successful PAC learner for $\h$.
        \item $\h$ has a finite VC-dimension.
    \end{enumerate}
\end{theorem}

\begin{thebibliography}{9}
\bibitem{ShaiBenDavid}
Shalev-Shwartz S., Ben-David S. (2014) Understanding Machine Learning: From Theory to Algorithms, Cambridge University Press

% \bibitem{Seq Lemma (b)}
% [Sequence Lemma (b)] \href{https://math.stackexchange.com/questions/1876224/}{math.SE}

\end{thebibliography}


\end{document}