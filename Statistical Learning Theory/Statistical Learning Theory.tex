\documentclass[11pt,a4paper]{article}
\usepackage[margin=1in]{geometry}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage[english]{babel}
\usepackage{amsthm}
\usepackage{amsfonts}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{amssymb}
\usepackage{enumerate}
\usepackage[hidelinks]{hyperref}
\usepackage{cancel}

\hypersetup{
    filecolor=magenta,      
    urlcolor=blue,
    pdftitle={Notes on Riemannian Preconditioning},
    % pdfpagemode=FullScreen,
}


\author{Jayadev Naram}
\title{Statistical Learning Theory} 

\begin{document}

\date{}
\maketitle
\tableofcontents
\newpage

\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{prop}[theorem]{Proposition}
\newtheorem{assume}{Assumption}
\newtheorem{remark}[theorem]{Remark}

\theoremstyle{definition}
\newtheorem{mydef}[theorem]{Definition}
\newtheorem{example}[theorem]{Example}

\newcommand{\R}{\mathbb{R}}
\newcommand{\A}{\mathcal{A}}
\newcommand{\M}{\mathcal{M}}
\newcommand{\N}{\mathcal{N}}
\newcommand{\h}{\mathcal{H}}
\newcommand{\T}{\mathcal{T}}
\newcommand{\Prob}{\mathbb{P}}
\newcommand{\Dist}{\mathcal{D}}
\newcommand{\perpProj}{\mathcal{P}^\perp}
\newcommand{\bb}{\mathbb{B}}
\newcommand{\Sprod}{\mathbb{S}_{xy}}
\newcommand{\highlight}[1]{\textsl{\textbf{#1}}}
\newcommand{\mapping}[3]{#1:#2\rightarrow #3}
\newcommand{\doubt}{\highlight{[??]}}
% \newcommand{\bigvert}[2]{#1{\raisebox{-.5ex}{$|$}_{#2}}}
\newcommand{\bigvert}[2]{\left.#1\right|_{#2}}
\newcommand{\sdnn}[1]{${#1}$}
\newcommand{\bsdnn}[1]{$\boldsymbol{#1}$}
\newcommand{\ifthen}[2]{\textbf{(#1)}\boldsymbol{\implies}\textbf{(#2)}}
\newcommand{\bsdn}[1]{\boldsymbol{#1}}
\newcommand{\forward}{$(\implies)$}
\newcommand{\converse}{$(\impliedby)$}
\newcommand{\Lt}[1]{\underset{#1\rightarrow 0}{Lt}}
\newcommand{\norm}[1]{\|#1\|}
\newcommand{\dparder}[2]{\dfrac{\partial #1}{\partial x_{#2}}}
\newcommand{\fparder}[2]{\frac{\partial #1}{\partial x_{#2}}}
\newcommand{\parder}[2]{\partial #1/\partial x_{#2}}
\newcommand{\parop}[1]{\dfrac{\partial}{\partial x_{#1}}}
\newcommand{\innerproduct}[2]{\langle #1, #2 \rangle}
\newcommand{\genst}{St_B(n,p)}
\newcommand{\igenst}[1]{St_{B_{#1}}(n_{#1},p)}
\newcommand{\realmat}[2]{\R^{#1\times #2}}
\newcommand{\Skew}{\mathcal{S}_{skew}(p)}
\newcommand{\Sym}{\mathcal{S}_{sym}(p)}
\newcommand{\XperpB}{X_{B^\perp}}
\newcommand{\polarRetr}{R^{polar}_X}
\newcommand{\qrRetr}{R^{QR}_X}
\newcommand{\vectransport}{\mathcal{T}}
\newcommand{\grad}{\text{grad}\,}
\newcommand{\hess}{\text{Hess}\,}

\section{Learning Framework}

\begin{itemize}
    \item \highlight{(Input).} A finite sequence $S = ((x_1,y_1)\ldots(x_m,y_m))$ called as \textit{training data} is provided to the learner, where each instance called as \textit{example} or \textit{sample} $(x_i,y_i)\in X\times Y$ for some domain set $X$ of data points and label set $Y$.
    \item \highlight{(Output).} A map $\mapping{h}{X}{Y}$ called as a \textit{predictor}, a \textit{hypothesis}, or a \textit{classifier} is the output of the learner. We use the notation $A(S)$ to denote the hypothesis that a learning algorithm $A$ returns upon receiving the training sequence $S$.
    \item \highlight{(Data-generation model).} The instances are generated by some probability distribution $\Dist$ over $X$ and the labels are assigned by some labeling function $\mapping{f}{X}{Y}$, i.e., the label y of a randomly sampled data point $x\sim \Dist$ is $y = f(x).$
    \item \highlight{(Measure of success).} The error of a hypothesis is the probability that it does not predict the correct label on a random data point generated by $\Dist$, i.e., for a hypothesis $\mapping{h}{X}{Y}$, the \textit{prediction error}, the \textit{generalization error}, or the \textit{risk} is defined to be
    \begin{equation}
    L_{\Dist, f}(h) = \underset{x\sim \Dist}{\Prob} [h(x)\neq f(x)] = \Dist (\{x:\,h(x)\neq f(x)\}).
    \end{equation}
\end{itemize}

\begin{remark}
The learner is blind to the underlying distribution $\Dist$ over $X$ and to the labeling function $f$. The only way the learner can interact is through observing the training set.
\end{remark}

\section{ERM with Finite Hypothesis Classes}

Since the learner does not know $\Dist$ and $f$, the true error is not directly available. Instead one can compute the \textit{training error} or \textit{empirical risk} which is defined to be
\begin{equation}
L_S(h) = \dfrac{|\{(x_i,y_i)\in S:\,h(x_i)\neq y_i\}|}{m}.
\end{equation}

Then, the \highlight {Empirical Risk Minimization} rule selects a hypothesis $h_S$ that minimizes the empirical risk over the training samples $S$, i.e.,
\begin{equation*}
h_S \in \underset{\mapping{h}{X}{Y}}{\text{argmin}}\; L_S(h).
\end{equation*}

We show that ERM rule might lead to undesirable hypotheses. 
\begin{remark}[\highlight{Overfitting}]
Assume that $X = [0,1]^2$ and $Y = \{0,1\}$. Let the probability distribution $\Dist$ be uniform distribution over $X$ and the labeling function $f$ determines the label to be $1$ for all data points in $[0,1/2]\times [0,1]$. Then note that area of region with labels 1 is $1/2$ and the total area of domain set is $1$. Consider the following hypothesis:
\begin{equation*}
h_S(x) = 
\begin{cases}
y_i\quad\text{if } (x_i,y_i)\in S\text{ and } x_i = x\\
0  \quad \;\text{otherwise.}
\end{cases}
\end{equation*}
Then we see that the empirical risk $L_S(h_S) = 0$, but the generalization error $L_{\Dist, f}(h_S) = 1/2$ because $h_S$ correctly labels all the negative (i.e., label $0$) examples, but incorrectly labels all but finite positive (i.e., label $1$) examples. The ERM rule can choose this hypothesis which performs no better than random guess.
\end{remark}

To overcome overfitting, we provide the learner with some prior knowledge. We restrict the search space to a finite hypothesis class denoted by $\h$ and we apply $ERM$ rule on it as before. Formally, 
\begin{equation*}
    h_S \in \underset{h\in \h}{\text{argmin}}\; L_S(h),
\end{equation*}
where $h_S$ is the ERM hypothesis. Additionally, we make the following simplifying assumption:
\begin{assume}[\highlight{The Realizability Assumption}]
There exists $h^*\in \h$ s.t. $L_{\Dist, f}(h^*) = 0$.
\end{assume}

By this assumption, we have with probability $1$ over random samples $S$ that $L_S(h^*) = 0$. This also implies that for every ERM hypothesis $h_S$ we have that $L_S(h_S) = 0$.

\begin{prop}
Let $\h$ denote a finite hypothesis class. Let $\delta \in (0,1)$ and $\epsilon >0$ and let $m$ be an integer that satisfies $m\ge \frac{\log(|\h|/\delta)}{\epsilon}.$ If for a labeling function $f$, a distribution $\Dist$, and the hypothesis class $\mathcal{H}$, the realization assumption holds. Then with probability of at least $1-\delta$ over the choice of an i.i.d. sample $S$ of size $m$, we have that for every ERM hypothesis $h_S$, $L_{\Dist, f}(h_S)\le \epsilon$.
\end{prop}

\begin{proof}
We will show that, over the choice of i.i.d. sample $S$ of size $m$, with probability not more than $\delta$, there exists a hypothesis $h_S$ that the ERM might select for which $L_{\Dist, f}(h_S)> \epsilon$. Formally, we would like to upper bound
\begin{equation}\label{eqn:upper_bound_qty}
\Dist^m(\{\bigvert{S}{X}:\, L_{\Dist,f}(h_S)>\epsilon\}),
\end{equation}
where $\bigvert{S}{X}$ is the restriction of $S$ to the domain $X$ and $\Dist^m$ is the probability of choosing i.i.d. sample $S\sim \Dist$ of size $m$. Let $\h_B\subseteq \h$ be the set of all bad hypothesis of $\h$, i.e.,
\begin{equation*}
\h_B = \{h\in \h:\, L_{\Dist, f}(h)>\epsilon\}.
\end{equation*}
In addition, let $M$ be the set of all misleading samples:
\begin{equation*}
M = \{ \bigvert{S}{X}:\,\text{for some }h\in \h_B, L_S(h) = 0 \},
\end{equation*}
in other words, for every $\bigvert{S}{X}\in M$, there is a bad hypothesis $h\in \h_B$, that looks like a good hypothesis on $\bigvert{S}{X}$. By realizability assumption, we have that $L_S(h_S) = 0$. Then the event that $L_{\Dist,f}(h_S)>\epsilon$ for a sample $S$ implies that $h_S\in \h_B$ and also $L_S(h_S) = 0$ so that $S\in M$. Therefore we have shown that
\begin{equation}\label{eqn:set_inclusion_expr}
\{\bigvert{S}{X}:\, L_{\Dist,f}(h_S)>\epsilon\} \subseteq M = \bigcup_{h\in \h_B} \{\bigvert{S}{x}:\, L_S(h) = 0\}.
\end{equation}
Hence,
\begin{align*}
\Dist^m(\{\bigvert{S}{X}:\, L_{\Dist,f}(h_S)>\epsilon\}) &\le \Dist^m(\cup_{h\in \h_B} \{\bigvert{S}{x}:\, L_S(h) = 0\})\\
&\overset{(*)}{\le} \sum_{h\in \h_B}\Dist^m(\{\bigvert{S}{x}:\, L_S(h) = 0\})\\
&= \sum_{h\in \h_B}\Dist^m(\{\bigvert{S}{x}:\, \forall i, h(x_i) = f(x_i)\}) \\
&\overset{(\dagger)}{=} \sum_{h\in \h_B}\prod_{i=1}^m\Dist(\{x_i:\, h(x_i) = f(x_i)\})= \sum_{h\in \h_B}\prod_{i=1}^m(1-L_{\Dist,f}(h))\\
&\le \sum_{h\in \h_B}\prod_{i=1}^m(1-\epsilon) \le \sum_{h\in \h_B}\prod_{i=1}^m e^{-\epsilon} = |\h_B|e^{-\epsilon m}
\le |\h|e^{-\epsilon m},
\end{align*}
where $(*)$ is obtained by applying union bound and $(\dagger)$ is obtained by using i.i.d. assumption. Using the fact that $m\ge \frac{\log(|\h|/\delta)}{\epsilon}$, we have 
\begin{equation*}
\Dist^m(\{\bigvert{S}{X}:\, L_{\Dist,f}(h_S)>\epsilon\})\le |\h|e^{-\epsilon m} \le \delta
\end{equation*}
which is as required.
\end{proof}

\section{PAC Learnability}

\begin{mydef}[\highlight{PAC Learnability}] \label{def:PAC_learning}
A hypothesis class $\h$ is \highlight{P}robably \highlight{A}pproximately \highlight{C}orrect \highlight{Learnable} if there exists a function $\mapping{m_\h}{(0,1)^2}{\mathbb{N}}$ and a learning algorithm with the following property: For every $\epsilon,\delta\in (0,1)$, for every distribution $\Dist$ over $X$, and for every labeling function $\mapping{f}{X}{\{0,1\}}$, if the realizablilty assumption holds w.r.t. $\h,\Dist, f$, then when running the learning algorithm on $m\ge m_\h(\epsilon,\delta)$ i.i.d.\,examples generated by $\Dist$ and labeled by $f$, the algorithm returns a hypothesis $h$ such that with probability of at least $1-\delta$ (over the choice of the examples), we have $L_{\Dist,f}(h)\le \epsilon$.
\end{mydef}

The definition contains two approximation parameters which are justified by the following propositions.

\begin{prop}
No learner can guarantee to find $h$ such that $L_{\Dist, f}(h) = 0$ with probability $1$ over random samples $S$.
\end{prop}

\begin{proof}
For $\epsilon\in (0,1)$ take $X = \{x_1,x_2\}$ and $\Dist(x_1) =1- \epsilon$, $\Dist(x_2) = \epsilon$. Then the probability not to see $x_2$ at all among $m$ i.i.d. examples is $(1-\epsilon)^m\approx e^{-\epsilon m}$. So, if $\epsilon \ll 1/m$, then the learner is likely to not see $x_2$ at all, therefore making error in predicting $x_2$.
\end{proof}

Thus we need the accuracy parameter $\epsilon$ which determines how far output classifier can be from the optimal one (this corresponds to the ``approximately correct"), i.e., for $\epsilon\in (0,1)$, the learner tries to find a classifier such that $L_{\Dist, f}(h)\le \epsilon$.

\begin{prop}
No learner can guarantee to find $h$ such that $L_{\Dist, f}(h) \le \epsilon,$ for $\epsilon\in (0,1)$ with probability $1$ over random samples $S$.
\end{prop}

\begin{proof}
The input to the learner is randomly generated samples. Then there is always a (very small) chance to see the same example again and again.
\end{proof}

Thus, the confidence parameter $\delta$ indicates how likely the classifier is to meet that accuracy requirement (corresponds to the ``probably" part of ``PAC").

\begin{remark}
The function $\mapping{m_\h}{(0,1)^2}{\mathbb{N}}$ determines the \highlight{sample complexity} of learning $\h$, i.e., how many examples are required to guarantee a probably approximately correct solution. Further note that if $\h$ is PAC learnable, there are many functions $m_\h$ that satisfy the requirements in Def. \ref{def:PAC_learning}. Therefore, to be precise we define the sample complexity of learning $\h$ to be the ``minimal function", in the sense that for any $\epsilon, \delta$, the number $m_\h(\epsilon,\delta)$ is minimal that satisfies the requirements of PAC learning with accuracy $\epsilon$ and confidence $\delta$.
\end{remark}

\begin{corollary}
Every finite hypothesis class is PAC learnable with sample complexity 
\begin{equation*}
m_\h(\epsilon,\delta) \le \bigg\lceil \dfrac{\log(\h/\delta)}{\epsilon} \bigg\rceil.
\end{equation*}
\end{corollary}

The next example shows that there are infinite hypothesis classes that are PAC learnable.

\begin{example}[\highlight{Axis aligned rectangles}]
Given $a_1\le b_1$, $a_2\le b_2$, define the axis aligned rectangle classifier $h_{(a_1,b_1,a_2,b_2)}$ by
\begin{equation*}
h_{(a_1,b_1,a_2,b_2)}(x_1,x_2) = 
\begin{cases}
1\qquad \text{if } a_1\le x_1\le b_1 \text{ and } a_2\le x_2\le b_2 \\
0\qquad \text{otherwise.}
\end{cases}
\end{equation*}
Let $R(a_1,b_1,a_2,b_2)$ denote the rectangle corresponding to $h_{(a_1,b_1,a_2,b_2)}$. The class of all axis aligned rectangles in $\R^2$ is defined as
\begin{equation*}
\h^2_{rec} = \{h_{(a_1,b_1,a_2,b_2)}:\, a_1\le b_1,a_2\le b_2\}.
\end{equation*}
Let $\Dist$ be some distribution over $\R^2$ and $R^* = R({a_1^*,b_1^*,a_2^*,b_2^*})$ be the rectangle that generates the labels such that $\Dist(R^*) = 1$. Let $f$ be the labeling function corresponding to the rectangle $R^*$.

We show that $\h^2_{rec}$ is PAC learnable. Define a simple ERM algorithm as follows: return the smallest rectangle enclosing all the positive examples in the training set. It is easy to note that this is indeed an ERM because the empirical risk of the selected rectangle over the training sample is $0$.

Let $R(S)$ be rectangle returned by ERM rule. Choose $\epsilon\in(0,1)$ and choose $a_1\ge a_1^*,b_1\le b_1^*,a_2\ge a_2^*,b_2\le b_2^*$ such that $\Dist(R_1) = \Dist(R_2) = \Dist(R_3) = \Dist(R_4) = \epsilon/4$ where $R_1 = R(a_1^*,a_1,a_2^*,b_2^*)$, $R_2 = R({b_1,b_1^*,a_2^*,b_2^*})$, $R_3 = R({a_1^*,b_1^*,a_2^*,a_2})$ and $R_4 = R({a_1^*,b_1^*,b_2,b_2^*})$.

Note that $L_{\Dist,f}(h_S) = \Dist(R^* - R(S))$. If $S$ contains (positive) examples in all of the rectangles $R_1,R_2,R_3,R_4$, then $R^*-R(S)\subseteq R_1\cup R_2\cup R_3\cup R_4$ and consequently $L_{\Dist,f}(h_S) \le \epsilon$. Then we have
\begin{align*}
\Dist^m(\{\bigvert{S}{X}:\,L_{\Dist,f}(h_S)> \epsilon\}) &\le 
\Dist^m(\{\bigvert{S}{X}:\, \bigvert{S}{X}\cap R_i = \emptyset \text{ for some }i\in \{1,2,3,4\} \})\\
&\le \sum_{i=1}^4 \Dist^m(\{\bigvert{S}{X}:\, \bigvert{S}{X}\cap R_i = \emptyset\})\\
&\le \sum_{i=1}^4 (1-\epsilon/4)^m = 4(1-\epsilon/4)^m \le 4e^{-m\epsilon/4}.
\end{align*}
Setting $m\ge \frac{4\log{ 4/\delta}}{\epsilon}$ for some $\delta\in(0,1)$ we have 
\begin{equation*}
\Dist^m(\{\bigvert{S}{X}:\,L_{\Dist,f}(h_S)> \epsilon\}) \le  4(1-\epsilon/4)^m \le \delta.
\end{equation*}
\end{example}

\section{General Learning Framework}

In many practical problems the realizability assumption does not hold. Furthermore, it is more realistic not to assume that the labels are fully determined by features we measure on input elements. Following these lines, we generalize the learner framework:

\begin{itemize}
\item \highlight{(Realistic Data-generation model).} Let $\Dist$ be a joint probability distribution over $X\times Y$. The distribution $\Dist$ can be seen to be composed of two components: (i) a distribution $\Dist_X$ over unlabeled domain points (called the marginal distribution) and (ii) a conditional probability over labels for each domain point $\Dist((x,y)|x)$.

\item \highlight{(Revised True Error).} We redefine the true error (or risk) of a hypothesis $h$ to be
\begin{equation*}
L_\Dist (h) \equiv \Prob_{(x,y)\sim \Dist}[h(x)\neq y] = \Dist(\{(x,y):\,h(x)\neq y\}).
\end{equation*}
\end{itemize}

\begin{prop}[\highlight{Bayes optimal predictor}]
Given any probability distribution $\Dist$ over $X\times\{0,1\}$, the Bayes predictor $f_\Dist$ defined as
\begin{equation*}
f_\Dist(x) = \begin{cases}
1,\quad \text{if }\ \Prob[y=1|x = x]\ge 1/2\\
0\,\quad \text{otherwise},
\end{cases}
\end{equation*}
is optimal, in the sense that for every predictor $\mapping{g}{X}{\{0,1\}}$, $L_\Dist(f_\Dist)\le L_\Dist(g)$.
\end{prop}

\begin{proof}
Fix $x\in X$ and $\Prob[y = 1|x = x] = \alpha_x$. Note that $\Prob[f_\Dist(x) = 1|x = x]$ is 1 if $\alpha_x\le 1/2$ and 0 if $\alpha_x< 1/2$, i.e., $\Prob[f_\Dist(x) = 1|x = x] = \mathbb{I}_{[\alpha_x\ge 1/2]}$ and $\Prob[f_\Dist(x) = 0|x = x] = \mathbb{I}_{[\alpha_x<1/2]}$. Then
\begin{align*}
\Prob[f_\Dist(x)\neq y|x=x] &= \Prob[f_\Dist(x) = 1|x=x] \Prob[y=0|x=x]+ \Prob[f_\Dist(x) = 0|x=x] \Prob[y=1|x=x] \\
&= \mathbb{I}_{[\alpha_x\ge 1/2]} \Prob[y=0|x=x]+ \mathbb{I}_{[\alpha_x< 1/2]} \Prob[y=1|x=x] \\
&= \mathbb{I}_{[\alpha_x\ge 1/2]} (1-\alpha_x)+ \mathbb{I}_{[\alpha_x< 1/2]} \alpha_x \\
&= \text{min}\{\alpha_x , 1-\alpha_x\}.
\end{align*}
And similarly for $g$, we have
\begin{align*}
\Prob[g(x)\neq y|x=x] &= \Prob[g(x) = 1|x=x] \Prob[y=0|x=x]+ \Prob[g(x) = 0|x=x] \Prob[y=1|x=x] \\
&= \Prob[g(x) = 1|x=x] (1-\alpha_x)+ \Prob[g(x) = 0|x=x] \alpha_x \\
&\ge \text{min}\{\alpha_x , 1-\alpha_x\}(\Prob[g(x) = 1|x=x] + \Prob[g(x) = 0|x=x])\\
&= \text{min}\{\alpha_x , 1-\alpha_x\}.
\end{align*}
The statement follows now due to the fact that the above is true for
every $x\in X$.
\end{proof}

We make a further generalization in the measure of success.
\begin{itemize}
\item \highlight{(Generalized Loss function).} Given any set $\h$ (that plays the role of hypotheses) and some domain $Z$, the loss functions are functions $\mapping{l}{\h\times Z}{\R_+}$. The true error or risk of a hypothesis $h\in \h$ is defined as
\begin{equation*}
L_\Dist(h) \equiv \mathbb{E}_{z\in\Dist}[l(h,z)].
\end{equation*}
Similarly, we define the empirical risk to be the expected loss over a given sample $S = (z_1,\ldots,z_m)\in Z^m$, i.e.,
\begin{equation*}
L_S(h) \equiv \dfrac{1}{m}\sum_{i=1}^m l(h,z_i).
\end{equation*}
\end{itemize}

\begin{example}[\highlight {\bsdnn{0-1} loss}]
The random variable $z$ ranges over the set of pairs $X\times Y$ and the loss function is 
\begin{equation*}
    l_{0-1}(h,(x,y)) = \begin{cases}
    0\quad \text{if }h(x) = y\\
    1\quad \text{if }h(x)\neq y.
    \end{cases}
\end{equation*}
\end{example}

\section{Agnostic PAC Learnability}

\begin{mydef}[\highlight{Agnostic PAC Learnability}]
A hypothesis class $\h$ is agnostic PAC learnable w.r.t. a set $Z$ and a loss function $\mapping{l}{\h\times Z}{\R_+}$, if there exists a function $\mapping{m_\h}{(0,1)^2}{\mathbb{N}}$ and a learning algorithm with the following property: For every $\epsilon,\delta\in(0,1)$ and for every distribution $\Dist$ over $Z$, when running the algorithm on $m\ge m_\h(\epsilon,\delta)$ i.i.d. examples generated by $\Dist$, the algorithm returns $h\in\h$ such that, with probability of at least $1-\delta$, over the choice of the $m$ training examples, we have
\begin{equation*}
    L_\Dist(h)\le \underset{h'\in\h}{\text{min}}\,L_\Dist(h')+\epsilon,
\end{equation*}
where $L_\Dist(h) = \mathbb{E}_{z\in\Dist}[l(h,z)]$.
\end{mydef}

\begin{remark}
In the above definition, for every $h\in \h$, we view the function $\mapping{l(h,\cdot)}{Z}{\R_+}$ as a random variable and define $L_\Dist(h)$ to be the expectation of this random variable.
\end{remark}

\begin{remark}
Agnostic PAC learnability $\implies$ PAC learnability. So, agnostic PAC learnability is a stronger requirement than PAC learnability.
\end{remark}

\begin{mydef}[\highlight{\bsdnn{\epsilon}-representative sample}]
A training set $S$ is called $\epsilon$-representative (w.r.t. domain $Z$, hypothesis class $\h$, loss function $l$, and distribution $\Dist$) if 
\begin{equation*}
    |L_S(h)-L_\Dist(h)|\le \epsilon,\quad \forall\, h\in\h.
\end{equation*}
\end{mydef}

\begin{prop}
Assume that a training set $S$ is $\frac{\epsilon}{2}$-representative (w.r.t. domain $Z$, hypothesis class $\h$, loss function $l$, and distribution $\Dist$). Then, any output of $ERM_\h(S)$, namely any $h_S\in\text{argmin}_{h\in\h} L_S(h)$, satisfies
\begin{equation*}
    L_\Dist(h_S)\le \underset{h\in\h}{\text{min}}\; L_\Dist(h) + \epsilon.
\end{equation*}
\end{prop}

\begin{proof}
For every $h\in\h$,
\begin{equation*}
L_\Dist(h_S)\le L_S(h_S) + \epsilon/2 \le L_S(h) + \epsilon/2 \le L_\Dist(h) + \epsilon/2 + \epsilon/2 = L_\Dist(h) + \epsilon.
\end{equation*}
\end{proof}

\begin{mydef}[\highlight{Uniform Convergence}]
We say that a hypothesis class $\h$ has the uniform convergence property (w.r.t. a domain $Z$ and a loss function $l$) if there exists a function $\mapping{m_\h^{UC}}{(0,1)^2}{\mathbb{N}}$ such that for every $\epsilon,\delta\in(0,1)$ and for every probability distribution $\Dist$ over $Z$, if $S$ is a sample of size $m\ge m_\h^{UC}(\epsilon,\delta)$ examples drawn i.i.d. according to $\Dist$, then with probability of at least $1-\delta$ we have S is $\epsilon$-representative.
\end{mydef}

\begin{remark}
The term uniform in the above definition refers to having a fixed sample size that works for all members of $\h$ and over all possible probability distributions over the domain.
\end{remark}

\begin{corollary}
If a class $\h$ has the uniform convergence property with a function $m_\h^{UC}$, then the class is agnostic PAC learnable with the sample complexity $m_\h(\epsilon,\delta)\le m_\h^{UC}(\epsilon/2,\delta)$. Furthermore, in that case, the ERM$_\h$ paradigm is a successful agnostic PAC learner for $\h$.
\end{corollary}

\begin{lemma}[\highlight{Hoeffding's Inequality}]
Let $\theta_1,\ldots,\theta_m$ be a sequence of i.i.d. random variables and let $\bar{\theta} = \tfrac{1}{m} \sum_{i=1}^m \theta_i$. Assume that $\mathbb{E}(\bar{\theta}) = \mu$ and $\Prob[a\le \theta_i\le b] = 1$. Then for any $\epsilon>0$, we have
\begin{equation*}
\Prob\Bigg[ \Bigg| \dfrac{1}{m}\sum_{i=1}^m\theta_i - \mu \Bigg|>\epsilon \Bigg] \le 2 \exp{\bigg(\dfrac{-2m\epsilon^2}{(b-a)^2} \bigg)}.
\end{equation*}
\end{lemma}

% \begin{proof}

% \end{proof}

\begin{prop}
Let $\h$ be a finite hypothesis class, $Z$ be a domain, and $\mapping{l}{\h\times Z}{[0,1]}$ be a loss function. Then, $\h$ enjoys the uniform convergence property with sample complexity
\begin{equation*}
m_\h^{UC}(\epsilon,\delta)\le \bigg\lceil \dfrac{\log(2\h/\delta)}{2\epsilon^2} \bigg\rceil.
\end{equation*}
Furthermore, the class $\h$ is agnostic PAC learnable using the ERM algorithm with sample complexity
\begin{equation*}
m_\h(\epsilon,\delta)\le m_\h^{UC}(\epsilon/2,\delta)\le \bigg\lceil \dfrac{2\log(2\h/\delta)}{\epsilon^2} \bigg\rceil.
\end{equation*}
\end{prop}

\begin{proof}
Fix $\epsilon,\delta\in(0,1)$. We need to find a sample of size $m$ that guarantees that for any $\Dist$, with probability of at least $1-\delta$ of the choice of $S = (z_1,\ldots,z_m)$ sampled i.i.d. from $\Dist$ we have that for all $h\in\h$, $|L_S(h)-L_\Dist(h)|\le \epsilon$, i.e.,
\begin{equation*}
\Dist^m(\{S:\,\forall h\in\h,\,|L_S(h)-L_\Dist(h)|\le \epsilon\}) \ge 1-\delta.
\end{equation*}
Equivalently, we need to show that 
\begin{equation*}
\Dist^m(\{S:\,\exists h\in\h,\,|L_S(h)-L_\Dist(h)|> \epsilon\}) < \delta.
\end{equation*}
Writing
\begin{equation*}
\{S:\,\exists h\in\h,\,|L_S(h)-L_\Dist(h)|> \epsilon\} = \bigcup_{h\in\h} \{S:\,|L_S(h)-L_\Dist(h)|> \epsilon\},
\end{equation*}
and applying the union bound we get
\begin{equation*}
\Dist^m(\{S:\,\exists h\in\h,\,|L_S(h)-L_\Dist(h)|> \epsilon\}) \le 
\sum_{h\in\h} \Dist^m(\{S:\,|L_S(h)-L_\Dist(h)|> \epsilon\}).
\end{equation*}
Let $\theta_i$ be the random variable $l(h,z_i)$. For a fixed $h$ and $z_1,\ldots,z_m$ sampled i.i.d., it follows that $\theta_1,\ldots,\theta_m$ are also i.i.d. random variables. Furthermore, $L_S(h) = \frac{1}{m}\theta_i$ and $L_\Dist(h) = \mu$, where $\mu = \mathbb{E}[\theta_i]$. Since the range of $l$ is $[0,1]$, we have that $\theta_i\in [0,1]$ for $i = 1,\ldots,m$. Then by applying Hoeffding's inequality we get
\begin{equation*}
\Dist^m(\{S:\,|L_S(h)-L_\Dist(h)|> \epsilon\}) = \Prob\Bigg[ \Bigg| \dfrac{1}{m}\sum_{i=1}^m\theta_i - \mu \Bigg|>\epsilon \Bigg] \le 2 \exp{(-2m\epsilon^2)}.
\end{equation*}
Then
\begin{align*}
\Dist^m(\{S:\,\exists h\in\h,\,|L_S(h)-L_\Dist(h)|> \epsilon\}) &\le 
\sum_{h\in\h} 2 \exp{(-2m\epsilon^2)} \\
&= 2|\h| \exp{(-2m\epsilon^2)}.
\end{align*}
Finally, if we choose $m\ge \frac{\log{(2|\h|/\delta)}}{2\epsilon^2}$, we obtain
\begin{equation*}
\Dist^m(\{S:\,\exists h\in\h,\,|L_S(h)-L_\Dist(h)|> \epsilon\}) \le \delta.
\end{equation*}
\end{proof}

\begin{remark}[\highlight{``Discretization Trick"}]
While the preceding proposition only applies to finite hypothesis classes, there is a simple trick that allows us to get a very good estimate of the practical sample complexity of infinite hypothesis classes. Consider an infinite hypothesis class parameterized by $d$ parameters, such as $h(x;w) = \innerproduct{x}{w},$ for all $x\in\R^d$ and parameter $w\in\R^d$. In practice we store real numbers in a computer using floating point representation, say of $64$ bits. Then it follows that in practice, the actual size of the hypothesis class is $2^{64d}$. Applying the previous proposition, we obtain that the sample complexity of such classes is bounded by $\frac{128d+2\log{(2/\delta)}}{\epsilon^2}$. This upper bound on the sample complexity has the deficiency of being dependent on the specific representation of real numbers used. 
\end{remark}

\section{The Bias-Complexity Trade-off}

\begin{prop}[\highlight {No-Free-Lunch}]
Let $A$ be a learning algorithm for the task of binary classification w.r.t. the $0-1$ loss over a domain $X$. Let $m$ be any number smaller than $|X|/2$, representing a training set size. Then, there exists a distribution $\Dist$ over $X\times \{0,1\}$ such that:
\begin{enumerate}[(a)]
    \item There exists a function $\mapping{f}{X}{\{0,1\}}$ with $L_\Dist(f) = 0$.
    \item With probability at least $1/7$ over the choice of $S\sim \Dist^m$ we have that $L_\Dist(A(S))\ge 1/8$.
\end{enumerate}
\end{prop}

\begin{proof}
Let $C$ be a subset of $X$ of size $2m$. There are $T = 2^{2m}$ functions from $C$ to $\{0,1\}$. Denote these functions by $f_1,\ldots,f_T$. For each such function, let $\Dist_i$ be a distribution over $C\times \{0,1\}$ defined by 
\begin{equation*}
    \Dist_i(\{(x,y)\}) = \begin{cases}
    1/2m\quad\text{if }y = f_i(x)\\
    0\quad\text{otherwise}.
    \end{cases}
\end{equation*}
Clearly, $L_{\Dist_i}(f_i) = 0$.

We will show that for every algorithm $A$ that receives a training set of $m$ examples from $C\times \{0,1\}$ and returns a function $\mapping{A(S)}{C}{\{0,1\}}$, it holds that 
\begin{equation*}
    \underset{i = 1,\ldots,T}{max}\; \underset{S\sim \Dist_i^m}{\mathbb{E}}[L_{\Dist_i}(A(S))] \ge 1/4.
\end{equation*}
There are $k = (2m)^m$ possible sequences of $m$ examples from $C$. Denote these sequences by $S_1,\ldots,S_k$. Also, if $S_j = (x_1,\ldots,x_m)$ we denote by $S_j^i$ the sequence containing instances in $S_j$ labeled by the function $f_i$, i.e., $S_j^i= ((x_1,f_i(x_1)),\ldots,(x_m,f_i(x_m)))$. If the distribution is $\Dist_i$ then the possible training sets $A$ can receive are $S_1^i,\ldots,S^i_k$, and all these training sets have the same probability of being sampled. Therefore,
\begin{equation*}
    \underset{S\sim\Dist_i^m}{\mathbb{E}}[L_{\Dist_i}(A(S))] = \frac{1}{k}\sum_{j=1}^k L_{\Dist_i}(A(S_j^i)).
\end{equation*}
\end{proof}

% \newpage

\begin{thebibliography}{9}
\bibitem{ShaiBenDavid}
Shalev-Shwartz S., Ben-David S. (2014) Understanding Machine Learning: From Theory to Algorithms, Cambridge University Press

% \bibitem{Seq Lemma (b)}
% [Sequence Lemma (b)] \href{https://math.stackexchange.com/questions/1876224/}{math.SE}

\end{thebibliography}


\end{document}