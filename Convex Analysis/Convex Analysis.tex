\documentclass[11pt,a4paper]{article}
\usepackage[margin=1in]{geometry}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage[english]{babel}
\usepackage{amsthm}
\usepackage{amsfonts}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{amssymb}
\usepackage{enumerate}
\usepackage[hidelinks]{hyperref}
\usepackage{cancel}

\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,      
    urlcolor=blue,
    pdftitle={Convex Analysis},
    pdfpagemode=FullScreen,
}


\author{Jayadev Naram}
\title{Convex Analysis} 

\begin{document}

\date{}
\maketitle
\tableofcontents
\newpage

\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[subsection]
\newtheorem{corollary}{Corollary}[theorem]

\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{assume}{Assumption}

\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{example}[theorem]{Example}
\newtheorem{remark}[theorem]{Remark}

\newcommand{\R}{\mathbb{R}}
\newcommand{\B}{\mathbb{B}}
\newcommand{\A}{\mathcal{A}}
\newcommand{\M}{\mathcal{M}}
\newcommand{\N}{\mathcal{N}}
\newcommand{\h}{\mathcal{H}}
\newcommand{\T}{\mathcal{T}}
\newcommand{\Prob}{\mathbb{P}}
\newcommand{\Dist}{\mathcal{D}}
\newcommand{\perpProj}{\mathcal{P}^\perp}
\newcommand{\bb}{\mathbb{B}}
\newcommand{\Sprod}{\mathbb{S}_{xy}}
\newcommand{\highlight}[1]{\underline{\textit{\textbf{#1}}}}
\newcommand{\mapping}[3]{#1:#2\rightarrow #3}
\newcommand{\doubt}{\highlight{[??]}}
% \newcommand{\bigvert}[2]{#1{\raisebox{-.5ex}{$|$}_{#2}}}
\newcommand{\bigvert}[2]{\left.#1\right|_{#2}}
\newcommand{\sdnn}[1]{${#1}$}
\newcommand{\bsdnn}[1]{$\boldsymbol{#1}$}
\newcommand{\ifthen}[2]{\textbf{(#1)}\boldsymbol{\implies}\textbf{(#2)}}
\newcommand{\bsdn}[1]{\boldsymbol{#1}}
\newcommand{\forward}{$(\implies)$\ }
\newcommand{\converse}{$(\impliedby)$\ }
\newcommand{\Lt}[1]{\underset{#1\rightarrow 0}{Lt}}
\newcommand{\norm}[1]{\|#1\|}
\newcommand{\dparder}[2]{\dfrac{\partial #1}{\partial x_{#2}}}
\newcommand{\fparder}[2]{\frac{\partial #1}{\partial x_{#2}}}
\newcommand{\parder}[2]{\partial #1/\partial x_{#2}}
\newcommand{\parop}[1]{\dfrac{\partial}{\partial x_{#1}}}
\newcommand{\innerproduct}[2]{\langle #1, #2 \rangle}
\newcommand{\metric}[2]{[#1, #2]}
\newcommand{\genst}{St_B(n,p)}
\newcommand{\igenst}[1]{St_{B_{#1}}(n_{#1},p)}
\newcommand{\realmat}[2]{\R^{#1\times #2}}
\newcommand{\Skew}{\mathcal{S}_{skew}(p)}
\newcommand{\Sym}{\mathcal{S}_{sym}(p)}
\newcommand{\XperpB}{X_{B^\perp}}
\newcommand{\polarRetr}{R^{polar}_X}
\newcommand{\qrRetr}{R^{QR}_X}
\newcommand{\vectransport}{\mathcal{T}}
\newcommand{\grad}{\mathrm{grad}\ }
\newcommand{\hess}{\mathrm{Hess}\ }

\section{Convex Sets and Convex Functions}

\subsection{Affine Sets}

Let $\R^n$ denote the $n$-dimensional Euclidean real vector space with the inner product defined for any $x,y\in \R^n$ as $\innerproduct{x}{y} = x^Ty = \sum_{i=1}^n x_i y_i$, where $x = [x_1,\ldots, x_n]^T$ and $y = [y_1,\ldots, y_n]^T$ are the coordinates of $x$ and $y$ respectively. Let the norm and the metric on $\R^n$ be defined as $\|x\| = \sqrt{\innerproduct{x}{x}}$ and $d(x,y) = \|x-y\|$, respectively.

\begin{definition}
    If $x$ and $y$ are different points in $\R^n$, the set of points of the form 
    \begin{equation*}
        (1-\lambda) x + \lambda y = x + \lambda (y-x), \qquad \lambda \in \R,
    \end{equation*}
    is called the \highlight{line} through $x$ and $y$. A subset $M$ of $\R^n$ is called an \highlight{affine set} if $(1 - \lambda) x + \lambda y \in M$ for every $x,y\in M$ and $\lambda \in \R$.
\end{definition}

The formal geometry of affine sets may be developed from the theorems of linear algebra about subspaces of $\R^n$.

\begin{theorem}\label{thm:subspace_affine}
    $M$ is a subspace of $\R^n$ iff $M$ is an affine set containing origin.
\end{theorem}

\begin{proof}
    \forward Every subspace contains $0$, and being closed under addition and scalar multiplication, a subspace is in particular an affine set.

    \noindent\converse Suppose $M$ is an affine set containing $0$. For every $x\in M$ and $\lambda\in \R$, we have
    \begin{equation*}
        \lambda x = (1-\lambda) 0 + \lambda x \in M,
    \end{equation*} 
    so $M$ is closed under scalar multiplication. Now, if $x, y\in M$, we have
    \begin{equation*}
        \tfrac{1}{2}(x+y) = \tfrac{1}{2} x + (1-\tfrac{1}{2}) y \in M,
    \end{equation*}
    and hence 
    \begin{equation*}
        x+y = 2(\tfrac{1}{2}(x+y)) \in M.
    \end{equation*}
    Thus $M$ is also closed under addition and is a subspace.
\end{proof}

\begin{definition}
    For $M\subseteq \R^n$ and $a\in \R^n$, the \highlight{translate} of $M$ by $a$ is defined to be the set
    \begin{equation*}
        M + a = \{x+a\;:\;x\in M\}.
    \end{equation*}
    An affine set $M$ is said to be \highlight{parallel} to an affine set $L$ if $M = L + a$ for some $a$.
\end{definition}

\begin{proposition}\label{prop:affine_translate}
    A translate of an affine set is another affine set.
\end{proposition}

\begin{proof}
    Suppose $M$ is an affine set and consider $L = M + a$, for some $a\in R^n$. For any points $x,y\in M$, $x+a,y+a\in L$, then we have
    \begin{equation*}
        \lambda (x+a) + (1-\lambda) (y+a) = \underbrace{\lambda x + (1-\lambda) y}_{\in\ M} + a \in L.
    \end{equation*}
\end{proof}

\begin{proposition}
    The relation ``$M$ parallel to $L$'' between two affine subsets $M, L\subseteq \R^n$ is an equivalence relation on the collection of affine subsets of $\R^n$.
\end{proposition}

\begin{proof}
    Consider affine sets $M,L,N\subseteq R^n$ and $a,b\in R^n$.
    \begin{enumerate}[i]
        \item Reflexive: $M = M+0$, so $M$ is parallel to $M$.
        \item Symmetric: Suppose $M = L+a = \{x+a\;:\; x\in L \}$. Then it is easy to see that $L = M + (-a)$, i.e., $L$ is parallel to $M$.
        \item Transistive: Suppose $M = L + a$ and $L = N + b$. Then $M = N + (a+b)$, thus $M$ is parallel to $N$.
    \end{enumerate}
\end{proof}

\begin{remark}
    From the equivalence relation developed in the previous proposition, if $L_1$ and $L_2$ both are parallel to $M$, then $L_1$ is parallel to $L_2$.
\end{remark}

% \begin{remark}
%     Note that this definition of parallelism is more restrictive than the everyday one, in that it does not include the idea of a line being parallel to a plane. One has to speak of a line which is parallel to another line within a given plane, and so forth.
% \end{remark}

\begin{theorem}\label{thm:unique_subspace_parallel_to_affine}
    Every non-empty affine set $M$ is parallel to a unique subspace $L$. This $L$ is given by
    \begin{equation*}
        L = M - M = \{x - y\;:\; x,y\in M\}.
    \end{equation*}
\end{theorem}

\begin{proof}
    Let us show first that $M$ cannot be parallel to two different subspaces. Subspaces $L_1$ and $L_2$ parallel to $M$ would be parallel to each other, so that $L_2 = L_1 + a$ for some $a$. Since $0\in L_2$, we would then have $-a\in L_1$, and hence $a\in L_1$. But then $L_1 \supseteq L_1 + a = L_2$. By similar argument $L_2\supseteq L_1$, so $L_1 = L_2$. This establishes the uniqueness. 
    
    Now observe that, for any $y\in M$, $M-y = M+(-y)$ is a translate of $M$ containing $0$. We considered $M$ to be an affine set and from Prop. \ref{prop:affine_translate} its translate is also an affine set, in this case containing $0$. Then, by Theorem \ref{thm:subspace_affine} and what we have just proved, this affine set must be the unique subspace parallel to $M$. Since $L = M-y$ no matter which $y\in M$ is chosen, we actually have $L = M - M$.  
\end{proof}

\begin{definition}
    The \highlight{dimension} of a non-empty affine set is defined as the dimension of the subspace parallel to it.
\end{definition}

By convention, the dimension of $\emptyset$ is $-1$, and affine sets of dimension $0, 1$ and $2$ are points, lines and planes respectively. An $(n-1)$-dimensional affine set in $\R^n$ is called a \highlight{hyperplane}.

\begin{theorem}\label{thm:affine_set_representation}
    Given $b\in\R^n$ and an $m\times n$ real matrix $B$, the set
    \begin{equation*}
        M = \{x\in \R^n\;:\; Bx = b\}
    \end{equation*}
    is an affine set in $\R^n$. Moreover, every affine set may be represented in this way.
\end{theorem}

\begin{proof}
    \forward If $x,y\in M$ and $\lambda\in \R$, then we have
    \begin{equation*}
        B((1-\lambda) x + \lambda y) = (1-\lambda) Bx + \lambda By = (1-\lambda) b + \lambda b = b,
    \end{equation*}
    so $(1-\lambda) x + \lambda y\in M$. Thus the given $M$ is affine.

    \noindent\converse Starting with an arbitrary non-empty affine set $M$ other than $\R^n$ itself, let $L$ be the subspace parallel to $M$. Let $b_1,\ldots, b_m$ be a basis for $L^{\perp}$, the orthogonal complement of $L$. Then
    \begin{equation*}
        L = \{x\;:\; \innerproduct{x}{b_i} = 0,\ i = 1,\ldots, m\} = \{x\;:\; Bx = 0\},
    \end{equation*}
    where $B$ is the $m\times n$ matrix whose rows are $b_1,\ldots, b_m$. Since $M$ is parallel to $L$, there exists an $a\in \R^n$ such that
    \begin{equation*}
        M = L+a = \{x\;:\; B(x-a) = 0\} = \{x\;:\; Bx = b\},
    \end{equation*}
    where $b = Ba$.
\end{proof}

\begin{corollary}
    Given $\beta\in \R$ and a non-zero $b\in \R^n$, the set
    \begin{equation*}
        H = \{ x\in \R^n \;:\; \innerproduct{x}{b} = \beta \}
    \end{equation*}
    is a hyperplane in $\R^n$. Moreover, every hyperplane may be represented in this way, with $b$ and $\beta$ unique up to a common non-zero multiple. The vector $b$ is called a \highlight{normal} to the hyperplane $H$. Every other normal to $H$ is either a positive or a negative scalar multiple of $b$.
\end{corollary}

% \begin{proof}
%     An $n-1$ dimensional subspace $L$ can be represented as $L = \{x\;:\; \innerproduct{x}{b} = 0\}$, where $b\in \R^n$ is the basis vector of the orthogonal complement of $L$. Since hyperplanes are translates of $n-1$ dimensional subspaces of $\R^n$, for some $a\in \R^n$, consider the hyperplane $H$ defined as
%     \begin{equation*}
%         H = L + a = \{x+a\;:\; \innerproduct{x}{b} = 0\} = \{y\;:\; \innerproduct{y-a}{b} = 0\} = \{y\;:\; \innerproduct{y}{b} = \beta\},
%     \end{equation*}  
%     where $\beta = \innerproduct{a}{b}$. This establishes the existence of such representation for a hyperplane. The uniqueness of this representation follows the uniqueness of the subspace parallel to the hyperplane and the vector $a$ from Theorem \ref{thm:unique_subspace_parallel_to_affine} and the unique orthogonal complement of this subspace, unique up to a scalar multiple of $b$ and consequently, $\beta$ with the same multiple.
% \end{proof}


% \begin{corollary}
%     Every affine subset of $\R^n$ is an intersection of a finite collection of hyperplanes.
% \end{corollary}

\begin{lemma}\label{lem:intersection_affine_sets}
    The intersection of an arbitrary collection of affine sets is affine.
\end{lemma}

\begin{proof}
    Consider an arbitrary collection of affine set $\{M_i\}$ and define $M = \cap_i M_i$. Let $x,y\in M$ and $\lambda \in \R$, then we have $(1-\lambda)x + \lambda y\in M_i$ for each $i$ and $(1-\lambda)x + \lambda y\in M$, thus $M$ is affine.
\end{proof}

\begin{definition}
    Given any $S\subseteq \R^n$, the \highlight{affine hull} of $S$, denoted by aff $S$, is the intersection of the collection of affine sets containing $S$.
\end{definition}

\begin{proposition}\label{prop:affine_hull_affine_comb}
    For any $S\subseteq \R^n$, we have 
    \begin{equation*}
        \mathrm{aff}\ S = \{ \lambda_1 x_1 + \cdots + \lambda_m x_m \;:\; x_i\in S \ \text{and}\ \lambda_1+\cdots+\lambda_m = 1 \}\equiv C.
    \end{equation*}
\end{proposition}

\begin{proof}
    \forward For any $x\in S$, we have the trivial combination $x = 1x\in C$ so that $S\subseteq C$. Suppose $x_1,x_2\in C$ so that $x = \sum_{i=1}^m \mu_i x_i$ and $y = \sum_{i=m+1}^n \mu_i x_i$ with $\sum_{i = 1}^m\mu_i = 1$, $\sum_{i=m+1}^n \mu_i = 1$ and $x_i\in S$ for $i = 1,\ldots,n$. Then for any $\lambda\in\R$, we have
    \begin{equation*}
        (1-\lambda) x + \lambda y = (1-\lambda) \sum_{i=1}^m \mu_i x_i + \lambda \sum_{i=m+1}^n \mu_i x_i,
    \end{equation*}
    such that the sum of the coefficients is
    \begin{equation*}
        (1-\lambda) \sum_{i=1}^m \mu_i + \lambda \sum_{i=m+1}^n \mu_i = (1-\lambda) (1) + \lambda (1) = 1.
    \end{equation*}
    This shows that $(1-\lambda)x + \lambda y\in C$ so that $C$ is an affine set containing $S$, thus, aff $S\subseteq C$.

    \noindent \converse Suppose $x\in C$, so that 
    \begin{equation*}
        x = \lambda_1 x_1+\cdots +\lambda_m x_m\ \text{where}\ \lambda_1+\cdots +\lambda_m = 1.
    \end{equation*}
    We show that $x\in$ aff $S$ by induction. The base case $m = 2$ holds from the definition of affine set. Assume that for $m>2$, $x$ belongs to aff $S$. Then consider the case of $m$-point combination in $x$. There is some index $j$ such that $\lambda_j\neq 1$, otherwise $\lambda_1+\cdots+\lambda_m = m\neq 1$ which is a contradiction. We can assume WLOG $j=1$, then we have
    \begin{equation*}
        x = \sum_{i=1}^m \lambda_i x_i = \lambda_1 x_1 + (1-\lambda_1)\sum_{i=2}^m\dfrac{\lambda_i}{1-\lambda_1} x_i\ \text{with}\ \sum_{i=2}^m\dfrac{\lambda_i}{1-\lambda_1} = 1.
    \end{equation*}
    This shows that $y = \sum_{i=2}^m\dfrac{\lambda_i}{1-\lambda_1}x_i\in$ aff $S$ by the induction hypothesis and similarly $x = \lambda_1 x_1 + (1-\lambda_1) y\in$ aff $S$.
\end{proof}

\begin{definition}
    A set of $m+1$ points $b_0,b_1,\ldots, b_m$ is said to be \highlight{affinely independent} if aff$\ \{b_0,b_1,\ldots, b_m\}$ is $m$-dimensional.
\end{definition}

\begin{proposition}
    The points $b_0,b_1,\ldots, b_m$ are affinely independent iff $b_1-b_0, \ldots, b_m-b_0$ are linearly independent. 
\end{proposition}

\begin{proof}
    By definition, the points $b_0,b_1,\ldots, b_m$ are affinely independent iff $M = $ aff$\ \{b_0,b_1,\ldots,$ $b_m\}$ is $m$-dimensional. But then
    \begin{equation*}
        M = \mathrm{aff}\ \{b_0,b_1,\ldots, b_m\} = L + b_0, 
    \end{equation*}
    where $L = \mathrm{aff}\{0,b_1-b_0,\ldots, b_m-b_0\}$ is the smallest subspace parallel to $M$, by Theorem \ref{thm:subspace_affine} and \ref{thm:unique_subspace_parallel_to_affine}. The dimension of $L$ is $m$ iff these vectors are linearly independent. 
\end{proof}

% \begin{remark}[Barycentric coordinate system on affine sets]
%     When the points $b_0,b_1,\ldots,b_m$ are affinely independent, any point $x$ in $M = $ aff$\ \{b_0,b_1,\ldots,b_m\}$ can be uniquely written in the form given in \eqref{eqn:affine_combination}. Then the scalars $\lambda_0,\lambda_1,\ldots,\lambda_m$ are called the \highlight{coordinates} of $x$ and this system is called the \highlight{barycentric coordinate system} for the affine set $M$. 
% \end{remark}

\begin{definition}
    A map $\mapping{T}{\R^n}{\R^m}$ is called an \highlight{affine transformation} if
    \begin{equation*}
        T((1-\lambda)x + \lambda y) = (1-\lambda) Tx + \lambda Ty,
    \end{equation*}
    for every $x,y\in\R^n$ and $\lambda\in\R$.
\end{definition}

\begin{theorem}\label{thm:affine_linear_relationship}
    The affine transformations from $\R^n$ to $\R^m$ are the mappings $T$ of the form $Tx = Ax+a$, where $A$ is a linear transformation and $a\in \R^n$.
\end{theorem}

\begin{proof}
    \forward Suppose $T$ is affine and define the map $Ax = Tx - T0$, where $0\in\R^n$ is the origin. We show that $A$ is linear. Note that
    \begin{align*}
        A((1-\lambda)x + \lambda y) &= T((1-\lambda)x + \lambda y) - T0 \\
        &= (1-\lambda) Tx + \lambda Ty - T0 \\
        &= (1-\lambda) Tx + \lambda Ty - (1-\lambda)T0 - \lambda T0 \\
        &= (1-\lambda) (Tx-T0) + \lambda (Ty-T0) \\
        &= (1-\lambda) Ax + \lambda Ay.
    \end{align*}
    This shows that $A$ is affine. Also note that $A0 = T0 - T0 = 0$. Now, we proceed to show the linearity of $A$ using these two facts.
    \begin{equation}\label{eqn:linear_1}
        A(\lambda x) = A((1-\lambda)0 + \lambda x) = (1-\lambda)A0 + \lambda Ax = \lambda Ax,
    \end{equation} 
    and
    \begin{align}\label{eqn:linear_2}
        A(\tfrac{1}{2}(x+&y)) = A(\tfrac{1}{2}x + (1-\tfrac{1}{2})y) = \tfrac{1}{2}Ax + \tfrac{1}{2}Ay,\nonumber \\
        A(x+y) &= A(2(\tfrac{1}{2}(x+y))) = 2 A(\tfrac{1}{2}(x+y)) = Ax + Ay.
    \end{align}
    From \eqref{eqn:linear_1} and \eqref{eqn:linear_2}, we have proved that $A$ is linear.

    \noindent\converse If $Tx = Ax+a$, where $A$ is linear, one has
    \begin{equation*}
        T((1-\lambda)x+\lambda y) = (1-\lambda)Ax + \lambda Ay + a = (1-\lambda) Tx + \lambda Ty,
    \end{equation*}
    thus $T$ is affine.
\end{proof}

\begin{proposition}
    Consider an affine transformation $T$ on $\R^n$ defined as $Tx = Ax+a$, where $A$ is a linear transformation on $\R^n$ and $a\in\R^n$. If $A^{-1}$ exists, then $T^{-1}$ exists and it is affine on $\R^n$ which is given by
    \begin{equation*}
        T^{-1}(x) = A^{-1}(x-a).
    \end{equation*}
\end{proposition}

\begin{proof}
    First we verify that $T^{-1}$ defined above is the inverse of $T$. For any $x\in\R^n$, we have
    \begin{align*}
        T^{-1}(T(x)) &= T^{-1}(Ax+a) = A^{-1}(Ax+a-a) = x \\
        T(T^{-1}(x)) &= T(A^{-1}(x-a)) = A(A^{-1}(x-a)) + a = x.
    \end{align*}
% 
    Now we show that $T^{-1}$ is affine. For any $x,y\in \R^n$, we have
    \begin{align*}
        T^{-1}((1-\lambda)x + \lambda y) &= A^{-1}((1-\lambda)x + \lambda y - a)\\
        &= (1-\lambda)A^{-1}x + \lambda A^{-1} y - A^{-1} a \\
        &= (1-\lambda)A^{-1}(x-a) + \lambda A^{-1} (y-a) \\
        &= (1-\lambda)T^{-1}x + \lambda T^{-1} y.
    \end{align*}
\end{proof}

\begin{proposition}
    If a mapping $\mapping{T}{\R^n}{\R^m}$ is affine, then the image set $TM = \{Tx\;:\;x\in M\}$ is affine in $\R^n$ for every affine set $M$ in $\R^n$. In particular, then, affine transformations preserve affine hulls:
    \begin{equation*}
        T(\mathrm{aff}\ S) = \mathrm{aff}\ (TS).
    \end{equation*}
\end{proposition}

\begin{proof}
    Let $M\subseteq \R^n$ be an affine set. Then for any $x,y\in M$ and $\lambda\in\R$, we have $T(x),T(y)\in TM$ and since $T$ is affine, we get 
    \begin{equation*}
        (1-\lambda)T(x) + \lambda T(y) = T((1-\lambda)x + \lambda y).
    \end{equation*}
    But $(1-\lambda)x + \lambda y\in M$, so that $(1-\lambda)T(x) + \lambda T(y)\in TM$. Thus, $TM$ is affine.
\end{proof}

\begin{theorem}
    Let $\{b_0,b_1,\ldots,b_m\}$ and $\{b_0^\prime,b_1^\prime,\ldots, b_m^\prime\}$ be affinely independent sets in $\R^n$. Then there exists a one-to-one affine transformation $T$ on $\R^n$, such that $Tb_i = b_i^\prime$ for $i = 0,\ldots,m$. If $m = n$, $T$ is unique.
\end{theorem}

\begin{proof}
    Enlarging the given affinely independent sets if necessary, we can reduce the question to the case when $m=n$. Then, there exists a unique one-to-one linear transformation $A$ on $\R^n$ carrying the basis $b_1-b_0,\ldots,b_n-b_0$ of $\R^n$ onto the basis $b_1^\prime-b_0^\prime,\ldots,b_n^\prime-b_0^\prime$. The desired affine transformation is then given by $Tx = Ax + a$, where $a = b_0^\prime - Ab_0$.
\end{proof}

\begin{corollary}\label{cor:affine_sets_one_to_one_map}
    Let $M_1$ and $M_2$ be any two affine sets in $\R^n$ of the same dimension. Then there exists a one-to-one affine transformation $T$ on $\R^n$ such that $TM_1 = M_2$.
\end{corollary}

\begin{proof}
    Any $m$-dimensional affine set can be expressed as the affine hull of an affinely independent set of $m+1$ points, and affine hulls are preserved by affine transformations.
\end{proof}

\begin{proposition}\label{prop:graph_affine_transformation}
    The graph of an affine transformation $\mapping{T}{\R^n}{\R^m}$ is an affine subset of $\R^{m+n}$.
\end{proposition}

\begin{proof}
    Let $G = \{z\in\R^{m+n}\;:\; z = (x,y),\ Tx = y\}$ denote the graph of $T$. From Theorem \ref{thm:affine_linear_relationship}, we know that $Tx = Ax + a$ for some linear transformation $A$ and $a\in\R^n$. Then consider a linear map $Bz = Ax - y$, for $z = (x,y)\in\R^{m+n}$. Then for all $z\in G$, we have $Bz = b$ with $b = -a$, i.e., $G = \{z\in\R^{m+n}\;:\; Bz = b\}$. Then by Theorem \ref{thm:affine_set_representation}, $G$ is affine subset of $\R^{m+n}$. 
\end{proof}


\subsection{Convex Sets and Cones}

\begin{definition}
    If $x$ and $y$ are different points in $\R^n$, the set of points of the form 
    \begin{equation*}
        (1-\lambda)x + \lambda y, \quad 0\le \lambda\le 1,
    \end{equation*}
    is called the \highlight{(closed) line segment} between $x$ and $y$. A subset $C$ of $\R^n$ is called a \highlight{convex set} if $(1-\lambda)x + \lambda y\in C$ for every $x,y\in C$ and $0\le \lambda \le 1$.  
\end{definition}

\begin{remark}
    Affine sets are convex sets but not all convex sets are affine sets. For example consider solid cubes in $\R^3$. 
\end{remark}

\begin{example}[Half-spaces]
    For any non-zero $b\in\R^n$ and any $\beta\in\R$, the sets 
    \begin{equation*}
        \{x\;:\; \innerproduct{x}{b}\le \beta\}, \qquad \{x\;:\; \innerproduct{x}{b}\ge \beta\},
    \end{equation*} 
    are called \highlight{closed half-spaces}. The sets
    \begin{equation*}
        \{x\;:\; \innerproduct{x}{b}< \beta\}, \qquad \{x\;:\; \innerproduct{x}{b}> \beta\},
    \end{equation*}
    are called \highlight{open half-spaces}. All four sets are convex and correspond to the same hyperplane $H = \{x\;:\; \innerproduct{x}{b} = \beta\}$.
\end{example}

\begin{lemma}\label{lem:intersection_convex_sets}
    The intersection of an arbitrary collection of convex sets is convex.
\end{lemma}

Proof similar to that of Lemma \ref{lem:intersection_affine_sets}.

\begin{corollary}\label{cor:intersection_convex_sets_affine_map}
    Let $b_i\in\R^n$ and $\beta_i\in\R$ for $i\in I$, where $I$ is an arbitrary index set. Then the set 
    \begin{equation*}
        C = \{x\in\R^n\;:\; \innerproduct{x}{b_i}\le \beta_i,\ \forall\ i\in I\}
    \end{equation*}
    is convex.
\end{corollary}

Corollary \ref{cor:intersection_convex_sets_affine_map} will be generalized by Corollary \ref{cor:intersection_convex_sets_convex_map}.

\begin{remark}\label{rem:all_sym}
    The conclusion of the corollary would still be valid if some of the inequalities $\le$ were replaced by $\ge,<,>$ or $=$. Thus, given any system of simultaneous linear inequalities and equations in $n$ variables, the set $C$ of solutions is a convex set $\R^n$.
\end{remark}

\begin{definition}
    A set which can be expressed as the intersection of finitely many closed half spaces of $\R^n$ is called a \highlight{polyhedral} convex set.
\end{definition}

\begin{definition}
    A vector sum
    \begin{equation*}
        \lambda_1 x_1+\cdots + \lambda_m x_m
    \end{equation*}
    is called a \highlight{convex combination} of $x_1,\ldots,x_m$ if the coefficients $\lambda_i$ are all nonnegative and $\lambda_1+\cdots+\lambda_m = 1$.
\end{definition}

\begin{definition}
    The intersection of all the convex sets containing a given subset $S$ of $\R^n$ is called the \highlight{convex hull} of $S$ and is denoted by conv $C$. 
\end{definition}

\begin{remark}
    The convex hull of $S$ is a convex set by Lemma \ref{lem:intersection_convex_sets}, the unique smallest one containing $S$.
\end{remark}

\begin{theorem}\label{thm:convex_hull_convex_combinations}
    For any $S\subseteq \R^n$, conv $S$ consists of all the convex combinations of the elements of $S$.
\end{theorem}

The proof is same as that of Prop. \ref{prop:affine_hull_affine_comb} with the additional nonnegativity constraint added throughout.

\begin{corollary}
    A subset $C$ of $\R^n$ is convex iff $C$ contains all the convex combinations of its elements.
\end{corollary}

\begin{corollary}
    The convex hull of a finite subset $\{b_0,\ldots, b_m\}$ of $\R^n$ consists of all the vectors of the form $\lambda_0 b_0 + \cdots +\lambda_m b_m$ with $\lambda_i \ge 0$, $i = 0,\ldots, m$, $\lambda_0+\cdots + \lambda_m = 1$.
\end{corollary}

\begin{definition}
    A set which is the convex hull of finitely many points is called a \highlight{polytope}. If $\{b_0,\ldots,b_m\}$ is affinely independent, its convex hull is called an \highlight{m-dimensional simplex}, and $b_0,\ldots,b_m$ are called the \highlight{vertices} of the simplex.
\end{definition}

% In terms of barycentric coordinates on aff $\{b_0,\ldots,b_m\}$, each point of simplex is uniquely expressible as a convex combination of the vertices. 

\begin{definition}
    The \highlight{dimension of a convex set} $C$ is defined to be the dimension of the affine hull of $C$.
\end{definition}

\begin{remark}
    The dimension of an affine hull or simplex as already defined agrees with its dimension as a convex set.
\end{remark}

\begin{theorem}\label{thm:convex_set_dimension_simplices}
    The dimension of a convex set $C$ is the maximum of the dimensions of the various simplices included in $C$.
\end{theorem}

\begin{proof}
    The convex hull of any subset of $C$ is included in $C$ by the convexity of $C$. The maximum dimension of the various simplices included in $C$ is thus the largest $m$ such that $C$ contains an affinely independent set of $m+1$ elements. Let $\{b_0,\ldots,b_m\}$ be such a set with $m$ maximal, and let $M$ be its affine hull. Then dim $M = m$ and $M\subseteq \mathrm{aff}\ C$. Furthermore, $C\subseteq M$, for if $C-M$ contained an element $b$, the set of $m+2$ elements $b_0,\ldots,b_m,b$ in $C$ would be affinely independent, contrary to the maximality of $m$. Since aff $C$ is the smallest affine set which includes $C$, it follows that aff $C\subseteq M$, or equivalently aff $C = M$ and hence dim $C = m$.
\end{proof}

\begin{definition}
    A subset $K$ of $\R^n$ is called a \highlight{cone} if it closed under positive scalar multiplication, i.e., $\lambda x\in K$ when $x\in K$ and $\lambda > 0$. A \highlight{convex cone} is a cone which is a convex set.
\end{definition}

\begin{example}[Convex cone]\label{eg:nonneg_notation}
    Subspaces of $\R^n$; open and closed half-spaces corresponding to a hyperplane passing through the origin, i.e., $\beta = 0$; the \highlight{nonnegative orthant} of $\R^n$
    \begin{equation*}
        \{x = (\xi_1,\ldots,\xi_n)\;:\; \xi_1\ge 0,\ldots, \xi_n\ge 0\},
    \end{equation*}
    and the \highlight{positive orthant}
    \begin{equation*}
        \{x = (\xi_1,\ldots,\xi_n)\;:\; \xi_1> 0,\ldots, \xi_n > 0\}.
    \end{equation*}
    It is customary to write $x\ge x^\prime$ for $x,x^\prime\in\R^n$, if $x-x^\prime$ belongs to the nonnegative orthant, i.e., 
    \begin{equation*}
        \xi_i\ge \xi_i^\prime\quad \text{ for }\quad i = 1,\ldots,n.
    \end{equation*}
    In this notation, the nonnegative orthant consits of the vectors $x$ such that $x\ge 0.$
\end{example}

\begin{lemma}\label{lem:intersection_convex_cones}
    The intersection of an arbitrary collection of convex cones is a convex cone.
\end{lemma}

\begin{corollary}
    Let $b_i\in\R^n$ for $i\in I$, where $I$ is an arbitrary index set. Then 
    \begin{equation*}
        K = \{x\in\R^n\;:\; \innerproduct{x}{b_i}\le 0,\ i\in I\}
    \end{equation*}
    is a convex cone.
\end{corollary}

Similar remarks to Remark \ref{rem:all_sym} hold for the above corollary.
% 
The following characterization of convex cones highlights an analogy between convex cones and subspaces.

\begin{theorem}\label{thm:convex_cone_conic_combinations}
    A subset of $\R^n$ is a convex cone iff it is closed under addition and positive scalar multiplication.
\end{theorem}

\begin{proof}
    \forward Let $K$ be a convex cone. By definition it is closed under positive scalar multiplication, so we need to show that it is closed under addition. Let $x,y\in K$, then by the convexity of $K$, $z = (1/2)(x+y)\in K$, and $x+y = 2z \in K$ since $K$ is a cone.
    
    \noindent\converse Suppose $K$ is closed under addition and positive scalar multiplication, and let $0<\lambda<1$. Then, the vectors $(1-\lambda)x$ and $\lambda y$ belong $K$ for $x,y\in K$, and hence $(1-\lambda)x+\lambda y\in K$. 
\end{proof}

\begin{corollary}\label{cor:convex_cone_conic_combinations}
    A subset of $\R^n$ is a convex cone iff it contains all the positive linear combinations of its elements.
\end{corollary}

\begin{corollary}\label{cor:convex_cone_conic_combinations_2}
    Let $S$ be an arbitrary subset of $\R^n$, and let $K$ be the set of all positive linear combinations of $S$. Then $K$ is the smallest convex cone which includes $S$.
\end{corollary}

\begin{corollary}\label{cor:convex_cone_conic_combinations_3}
    Let $C$ be a convex set, and let 
    \begin{equation*}
        K = \{\lambda x\;:\; \lambda > 0, x\in C\}.
    \end{equation*}
    Then $K$ is the smallest convex cone which includes $C$.
\end{corollary}

\begin{theorem}\label{thm:convex_cones_subspaces}
    Let $K$ be a convex cone containing $0$. Then there is the smallest subspace containing $K$, namely
    \begin{equation*}
        K - K = \{x-y\;:\;x\in K, y\in K\} = \mathrm{aff}\ K,
    \end{equation*}
    and there is the largest subspace contained within $K$, namely $(-K)\cap K$.
\end{theorem}

\begin{proof}
    To show that each of these sets are subspaces, we first show that they are convex cones. Then by Theorem \ref{thm:convex_cone_conic_combinations}, it suffices to show that they contain $0$ and are closed under multiplication by $-1$. 
    
    Consider the set $K-K = \{x-y\;:\; x,y\in K\}$. For any $z_1,z_2\in K-K$ and $\lambda_1,\lambda_2>0$, we have $z_1 = x_1-y_1$ and $z_2 = x_2-y_2$ for some $x_1,x_2,y_1.y_2\in K$. Then we have
    \begin{equation*}
        \lambda_1 z_1+\lambda_2 z_2 = (\lambda_1 x_1 + \lambda_2 x_2) - (\lambda_1 y_1 + \lambda_2 y_2).
    \end{equation*}
    But $\lambda_1 x_1 + \lambda_2 x_2, \lambda_1 y_1 + \lambda_2 y_2\in K$ as $K$ is a convex cone, thus $\lambda z_1 +\lambda_2 z_2\in K-K$. By Cor. \ref{cor:convex_cone_conic_combinations}, $K-K$ is also a convex cone. For every $x\in K$, $x-x = 0 \in K-K$. For every $z\in K-K$, there are $x,y\in K$ such that $z = x-y$, but then we also have $-z = y-x\in K-K$, showing that $K-K$ is a subspace. Since $0\in K-K$, $K-0\subseteq K-K$, i.e., $K-K$ contains $K$. Let $L$ be a subspace containing $K$, then for any $z = x-y\in K-K$, $x,y\in K$, we have $x-y\in L$ or equivalently $z\in L$ showing that $K-K\subseteq L$, i.e., $K-K$ is the smallest subspace containing $K$. Since $K$ is a set containing $0$, its affine hull aff $K$ must also contain $0$, which by Theorem \ref{thm:subspace_affine} is a subspace. But we have proved that $K-K$ is the smallest subspace containing $K$, hence $K-K = \mathrm{aff}\ K$.

    Now consider the set $(-K)\cap K$. By Lemma \ref{lem:intersection_convex_cones} $(-K)\cap K$ is also a convex cone containing $0$, since $0\in K$ and $0\in (-K)$. For any $x\in (-K)\cap K$, we have $x\in K$ and $x\in (-K)$, i.e., $-x\in K$, which shows that $-x\in K$ and $-x\in (-K)$ or $-x\in (-K)\cap K$. This shows that $(-K)\cap K$ is a subspace. Since $K\subseteq K$ and $K\cap (-K)\subseteq K$, $(-K)\cap K$ is a subspace contained in $K$. Suppose $M$ is a subspace contained in $K$ and let $x\in M$. Since $M$ is a subspace $-x\in M$, i.e., $x\in (-K)\cap K$ as $M\subseteq K$. Thus $(-K)\cap K$ is the largest subspace contained in $K$.  
\end{proof}

\begin{definition}\label{def:convex_cone_generated_by_a_set}
    Let $S$ be an arbitrary subset of $\R^n$. The convex cone obtained by adjoining the origin to the set of all positive linear combinations of $S$ is called as the \highlight{convex cone} \highlight{generated by \bsdnn{S}} and is denoted by cone $S$.
\end{definition}

\begin{remark}\label{remark:convex_cone_generated_by_a_set}
    The convex cone generated by $S$ is not, under our terminology, the same as the smallest convex cone containing $S$ unless the latter cone happens to contain the origin. If $S\neq\emptyset$, cone $S$ consists of all non-negative (rather than positive) linear combinations of elements of $S$. Clearly, cone $S$ = conv $($ray $S)$, where ray $S$ is the union of the origin and the various rays (half-lines of the form $\{\lambda y \;:\; \lambda\ge 0\}$) generated by the non-zero vectors $y\in S$.
\end{remark}

\begin{definition}
    A vector $x^*$ is said to be \highlight{normal} to a convex set $C$ at a point $a$, where $a\in C$, if $x^*$ does not make an acute angle with any line segment in $C$ with $a$ as endpoint, i.e., if $\innerproduct{x-a}{x^*}\le 0$ for every $x\in C$. The set of all vectors $x^*$ normal to $C$ at $a$ is called the \highlight{normal cone} to $C$ at $a$. The \highlight{normal cone map} associated with the convex set $C$ is a set-valued map $N_C:\R^n\rightrightarrows \R^n$ which is defined as
    \begin{equation*}
        N_C(a) = 
        \begin{cases}
			\{x^*\in\R^n\;:\; \innerproduct{x-a}{x^*}\le 0\ \forall\ x\in C\}, & \text{if }a\in C\\
            \emptyset, & \text{if } a\notin C.
		 \end{cases}
    \end{equation*}
\end{definition}

At this point, the normal cone map definition only helps in simplfying notation in the following proposition. But it will be used more than that in context of Variational Analysis.

\begin{proposition}
    For a convex set $C$ and $a\in C$, $N_C(a)$ is a convex cone.
\end{proposition}

\begin{proof}
    Suppose that $x^*,y^*\in N_C(a)$ and $\lambda,\mu >0$. Then $\innerproduct{x-a}{x^*}\le 0$ and $\innerproduct{x-a}{y^*}\le 0$, which implies that
    \begin{equation*}
        \innerproduct{x-a}{\lambda x^*+\mu y^*} \le 0,\ \text{i.e.,}\ \lambda x^* +\mu y^* \in N_C(a), 
    \end{equation*}
    where we have used Theorem \ref{thm:convex_cone_conic_combinations}.
\end{proof}

\begin{definition}
    The \highlight{barrier cone} of a convex set $C$ is defined as the set of all vectors $x^*$ such that, for some $\beta\in\R$, $\innerproduct{x}{x^*}\le \beta$ for every $x\in C$.
\end{definition}

\begin{proposition}
    The barrier cone of a convex set is a convex cone.
\end{proposition}

\begin{proof}
    Suppose $C$ is a convex set and $x^*,y^*$ belong to the barrier cone of $C$ and $\lambda,\mu>0$. Then there are some $\beta,\gamma\in \R$ such that $\innerproduct{x}{x^*}\le \beta$ and $\innerproduct{x}{y^*}\le \gamma$ for every $x\in C$. Using this, we see that for all $x\in C$
    \begin{equation*}
        \innerproduct{x}{\lambda x^*+\mu y^*}\le \lambda \beta + \mu \gamma \in \R,
    \end{equation*}
    which shows that $\lambda x^*+\mu y^*$ also belongs to the barrier cone of $C$.
\end{proof}

\subsection{The Algebra of Convex Sets}

\begin{theorem}
    Let $C$ be a convex set in $\R^n$. Then so is every translate $C+a$ and every \highlight{scalar multiple} $\lambda C = \{\lambda x\;:\; x\in C\}$, where $a\in \R^n$ and $\lambda \in \R$.
\end{theorem}

\begin{proof}
    Let $x+a,y+a\in C+a$ and $0\le \mu\le 1$. Then using convexity of $C$, we get
    \begin{equation*}
        (1-\mu) (x+a) + \mu (y+a) = ((1-\mu) x + \mu y) + a\in C+a.
    \end{equation*}
    Similarly, let $\lambda x,\lambda y\in \lambda C$ and $0\le \mu\le 1$. Then using convexity of $C$, we get
    \begin{equation*}
        (1-\mu) \lambda x + \mu \lambda y = \lambda ((1-\mu) x + \mu y) \in \lambda C.
    \end{equation*}
\end{proof}

\begin{remark}[Symmetric Convex Set]
    The \highlight{symmetric reflection} of $C$ across origin is $-C = (-1)C.$ A convex set if said to be \highlight{symmetric} if $-C = C$. Such a set (if non-empty) must contain the origin, since it must contain along with each vector $x$, not only $-x$, but the entire line segment between $x$ and $-x$. The non-empty convex cones which are symmetric are the subspaces (Theorem \ref{thm:convex_cones_subspaces}).
\end{remark}

\begin{theorem}\label{thm:convex_sets_sum}
    If $C_1$ and $C_2$ are convex sets in $\R^n$, then so is their sum $C_1+C_2$, where 
    \begin{equation*}
        C_1+C_2 = \{x_1+x_2\;:\;x_1\in C_1,x_2\in C_2\}.
    \end{equation*}
\end{theorem}

\begin{proof}
    Let $x_1+x_2,x_1^\prime +x_2^\prime \in C_1+C_2$ and $0\le \lambda\le 1$. Then using convexity of $C_1$ and $C_2$, we obtain
    \begin{equation*}
        (1-\lambda) (x_1+x_2) + \lambda (x_1^\prime +x_2^\prime) = ((1-\lambda)x_1 + \lambda x_1^\prime) + ((1-\lambda)x_2 + \lambda x_2^\prime) \in C_1+C_2.
    \end{equation*}
\end{proof}

\begin{example}
    If $C_1$ is any convex set and $C_2$ is the nonnegative orthant (see Example \ref{eg:nonneg_notation}), then 
    \begin{equation*}
        C_1+C_2 = \{x_1+x_2\;:\; x_1\in C_1,x_2\ge 0\} = \{x\;:\;\exists x_1\in C_1,x_1\le x\}.
    \end{equation*}
    Thus, $C_1+C_2$ is convex by Theorem \ref{thm:convex_sets_sum} when $C_1$ is convex.
\end{example}

\begin{theorem}\label{thm:convex_set_eq_def_1}
    A set $C$ is a convex set iff $(1-\lambda) C+ \lambda C\subseteq C$, where $0\le\lambda\le 1$.
\end{theorem}

\begin{proposition}
    Let $C_1,\ldots,C_m$ be convex sets, then so is the linear combination
    \begin{equation*}
        C = \lambda_1 C_1 + \cdots + \lambda_m C_m,
    \end{equation*}
    where $\lambda_1,\ldots, \lambda_m\in \R$.
\end{proposition}

\begin{proof}
    Suppose $x = \lambda_1 x_1+\cdots +\lambda_m x_m, x^\prime = \lambda_1 x_1^\prime+\cdots +\lambda_m x_m^\prime$ with $x_i,x_i^\prime\in C_i$ so that $x,x^\prime \in C$ and assume $0\le \lambda\le 1$. Then
    \begin{equation*}
        (1-\lambda) x + \lambda x^\prime = \lambda_1 ((1-\lambda) x_1 + \lambda x_1^\prime) + \cdots + \lambda_m ((1-\lambda) x_m + \lambda x_m^\prime) \in C.
    \end{equation*}
\end{proof}

\begin{remark}[Set Algebra]
    The following algebraic laws are valid for the addition and scalar multiplication of sets even without convexity being involved:
    \begin{align}
        C_1+ C_2 &= C_2+C_1,\nonumber\\
        (C_1+C_2)+C_3 &= C_1+(C_2+C_3),\nonumber\\
        \lambda_1(\lambda_2 C) &= (\lambda_1\lambda_2) C,\nonumber\\
        \lambda(C_1+C_2) &= \lambda C_1 +\lambda C_2,\nonumber\\
        (\lambda_1 +\lambda_2)C &\subseteq \lambda_1 C + \lambda_2 C,\label{eqn:partial_scalar_addition}
    \end{align}
    where $\lambda,\lambda_1,\lambda_2\in \R$. The set $\{0\}$ is the identity element for the addition operation. Additive inverses do not exist for sets containing more than one point; the best one can say in general is that $0\in [C+(-C)]$ when $C\neq \emptyset$. As can be noted from \eqref{eqn:partial_scalar_addition}, to see that the reverse inclusion is not true in general, consider $C = \{1,2\}\subseteq \R$ and $\lambda_1 = 1,\lambda_2 = 2$. Then $3C = \{3,6\}\neq C + 2C = \{3,4,5,6\}$. However, under the convexity assumption along with $\lambda_1,\lambda_2\ge 0$, the next theorem shows that we can retain the reverse inclusion.
\end{remark}

\begin{theorem}
    A set $C$ is a convex set iff $(\lambda_1+\lambda_2)C = \lambda_1 C + \lambda_2 C$, where $\lambda_1,\lambda_2\ge 0$.
\end{theorem}

\begin{proof}
    \forward The inclusion $(\lambda_1 +\lambda_2)C \subseteq \lambda_1 C + \lambda_2 C$ is true whether $C$ were convex or not. Now consider the reverse inclusion. Assume that $\lambda_1+\lambda_2>0$ and define the set 
    \begin{equation*}
        D\equiv \tfrac{\lambda_1}{\lambda_1+\lambda_2} C + \tfrac{\lambda_2}{\lambda_1+\lambda_2}C.
    \end{equation*}
    Let $x\in D$, so that there exists $x_1,x_2\in C$ such that 
    \begin{equation*}
        x = \tfrac{\lambda_1}{\lambda_1+\lambda_2}x_1 + \tfrac{\lambda_2}{\lambda_1+\lambda_2}x_2 = \tfrac{\lambda_1}{\lambda_1+\lambda_2}x_1 + \Big(1-\tfrac{\lambda_1}{\lambda_1+\lambda_2}\Big)x_2 \in C,
    \end{equation*}
    hence $D \subseteq C$. On multiplying through by $\lambda_1+\lambda_2$, we get $\lambda_1 C +\lambda_2 C \subseteq (\lambda_1+\lambda_2)C$.

    \noindent\converse The distributive law implies that $\lambda C + (1-\lambda) C \subseteq C$ when $0\le \lambda\le 1$. Then by Theorem \ref{thm:convex_set_eq_def_1}, this implies that $C$ is a convex set. 
\end{proof}

\begin{remark}
    It follows from this theorem, for instance, that $C+C = 2C$, $C+C+C = 3C$, and so forth, when $C$ is convex.
\end{remark}

\begin{proposition}
    Given an arbitrary family of non-empty convex sets $\{C_i\;:\;i\in I\}$ in $\R^n$, there is a unique largest convex set included in all of them, namely $\bigcap_{i\in I}C_i$, and a unique smallest convex set including all of them, namely conv $(\bigcup_{i\in I} C_i)$.
\end{proposition}

\begin{remark}\label{remark:convex_sets_complete_lattice}
    From the above proposition, the collection of convex subsets in $\R^n$ is a complete lattice under the natural partial order corresponding to inclusion.
\end{remark}

\begin{theorem}\label{thm:convex_hull_finite_convex_combinations}
    Let $\{C_i\;:\;i\in I\}$ be an arbitrary collection of non-empty convex sets of $\R^n$, and let $C$ be the convex hull of the union of the collection, i.e., $C =$ conv $(\bigcup_{i\in I}C_i)$. Then
    \begin{equation*}
        C = \bigcup\Big\{\sum_{i\in I}\lambda_i C_i\Big\},
    \end{equation*}
    where the sum is taken over all finite convex combinations (i.e., over all nonnegative choices of the coefficients $\lambda_i$ such that only finitely many are non-zero and these add up to $1$.)
\end{theorem}

\begin{proof}
    By Theorem \ref{thm:convex_hull_convex_combinations}, $C$ is the set of all convex combinations 
    \begin{equation*}
        x = \mu_1 y_1+\cdots + \mu_m y_m,
    \end{equation*}
    where $y_1,\ldots,y_m\in \bigcup_{i\in I} C_i, \sum_{i=1}^m \mu_i = 1$, and $m<\infty$ as $x\in\R^n$. Actually, we can get $C$ just by taking those combinations in which the coefficients are non-zero and vectors are taken from different sets $C_i$. Indeed, vectors with zero coefficients can be omitted from the combination, and if two of the vectors with positive coefficients belong to the same set $C_i$, say $y_1$ and $y_2$, then the term $\mu_1 y_1+\mu_2 y_2$ in $x$ can be replaced by $\mu y$, where $\mu = \mu_1+\mu_2$ and $y = (\mu_1/\mu)y_1 + (\mu_2/\mu)y_2\in C_i$ so that we have
    \begin{equation*}
        x = \mu y + \sum_{i=3}^m \mu_i x_i,\quad \mu+\sum_{i=3}^m \mu_i = \sum_{i=1}^m \mu_i = 1. 
    \end{equation*}
    Thus by Theorem \ref{thm:convex_hull_convex_combinations}, $C$ is the union of the finite convex combinations of the form 
    \begin{equation*}
        \mu_1 C_{i_1} + \cdots + \mu_m C_{i_m},
    \end{equation*} 
    where the indices $i_1,\ldots,i_m$ are all different. 
\end{proof}

\begin{theorem}\label{thm:convex_set_under_linear_transformation}
    Let $\mapping{A}{\R^n}{\R^m}$ be a linear transformation. Then $AC$, image of $C$ under $A$, is a convex set in $\R^m$ for every convex set $C$ in $\R^n$, and $A^{-1}D$, the inverse image of $D$ under $A$, is a convex set in $\R^n$ for every convex set $D$ in $\R^m$.
\end{theorem}

\begin{proof}
    Let $C$ be a convex subset of $\R^n$ and let $x,y\in C$ so that $Ax,Ay\in AC$. Then for $0\le \lambda \le 1$, we have 
    \begin{equation*}
        \lambda Ax + (1-\lambda) A y = A(\lambda x+(1-\lambda)y) \in AC.
    \end{equation*}

    \noindent Now, suppose that $D$ is a convex subset of $\R^m$ and let $A^{-1}x,A^{-1}y\in A^{-1}D$. Then for $0\le \lambda\le 1$, we have
    \begin{equation*}
        \lambda A^{-1}x + (1-\lambda)A^{-1}y = A^{-1}(\lambda x + (1-\lambda) y) \in A^{-1}D.
    \end{equation*}
\end{proof}

\begin{corollary}
    The orthogonal projection of a convex set $C$ on a subspace $L$ is convex.
\end{corollary}

% \begin{proof}
%     The orthogonal projector map onto $L$ is a linear transformation.
% \end{proof}

\begin{theorem}\label{thm:direct_sum_convex}
    Let $C$ and $D$ be convex sets in $\R^m$ and $\R^p$, respectively. Then, the direct sum
    \begin{equation*}
        C\oplus D = \{x = (y,z)\;:\; y\in C, z\in D\}
    \end{equation*}
    is a convex set in $\R^{m+p}$.
\end{theorem}

\begin{remark}[Direct sum and cartesian product]\label{remark:direct_sum_cartesian_product}
    When dealing with finite collection of sets, the direct sum and the cartesian product of the collection coincide. So the previous result also applies for cartesian product of $C$ and $D$.
\end{remark}

\begin{remark}
    Suppose $C,D\subseteq\R^n$. Then the direct sum $C\oplus D$ coincides with the ordinary sum $C+D$, if each $x\in C+D$ can be expressed uniquely in the form $x = y+z$, $y\in C$ and $z\in D$. This happens iff the symmetric convex sets $C-C$ and $D-D$ have only the zero vector of $\R^n$ in common. (It can be shown that then $\R^n$ may be expressed as a direct sum of two subspaces, one containing $C$ and the other containing $D$.)
\end{remark}

% \begin{theorem}
%     Let $C_1$ and $C_2$ be convex sets in $\R^{m+p}$, and let $C$ be the set of vectors $x = (y,z)$ (where $y\in\R^m$ and $z\in \R^p$) such that there exists vectors $z_1$ and $z_2$ with $(y,z_1)\in C_1$ and $(y,z_2)\in C_2$ and $z_1+z_2 = z$. Then $C$ is a convex set in $\R^{m+p}$.
% \end{theorem}

\begin{remark}[Algebra of Convex cones]
    The sets $K_1+K_2$, conv $(K_1\cup K_2)$, $K_1\oplus K_2$, $AK$, $A^{-1}K$ and $\lambda K$ are convex cones when $K_1$, $K_2$ and $K$ are convex cones. Positive scalar multiplication is a trivial operation for cones: one has $\lambda K = K$ for every $\lambda >0$.
\end{remark}

\begin{theorem}
    If $K_1$ and $K_2$ are convex cones containing origin, then
    \begin{equation*}
        K_1+K_2 = \mathrm{conv}\ (K_1\cup K_2).
    \end{equation*} 
\end{theorem}

\begin{proof}
    By Theorem \ref{thm:convex_hull_finite_convex_combinations}, conv $(K_1\cup K_2)$ is the union over $\lambda\in [0,1]$ of $(1-\lambda)K_1 + \lambda K_2$, i.e.,
    \begin{equation*}
        \mathrm{conv}\ (K_1\cup K_2) = \bigcup_{\lambda\in [0,1]} (1-\lambda)K_1 + \lambda K_2.
    \end{equation*}
    The latter set is $K_1+K_2$ when $0<\lambda<1$ (since $\lambda K = K$ for a cone with $\lambda>0$), $K_1$ when $\lambda = 0$, $K_2$ when $\lambda = 1$. Since $0\in K_1$ and $0\in K_2$, $K_1+K_2$ includes both $K_1$ and $K_2$. Thus conv $(K_1\cup K_2)$ coincides with $K_1+K_2$.
\end{proof}

\subsection{Convex Functions}

\begin{definition}
    Let $\mapping{f}{S\subseteq \R^n}{\R\cup \{\pm \infty\}}$ be a function. The set 
    \begin{equation*}
        \{(x,\mu)\;:\; x\in S,\mu\in \R,\mu\ge f(x)\}
    \end{equation*}
    is called the \highlight{epigraph} of $f$ and is denoted by epi $f$. We define $f$ to be a \highlight{convex function} on $S$ if epi $f$ is convex as a subset of $\R^{n+1}$. A \highlight{concave function} on $S$ is a function whose negative is convex. An \highlight{affine function} on $S$ is a function which is finite, convex and concave.
\end{definition}

\begin{definition}\label{def:effective_domain}
    The \highlight{effective domain} of a convex function $f$ on $S$, which we denote by dom $f$, is the projection on $\R^n$ of the epigraph of $f$:
    \begin{equation*}
        \mathrm{dom}\ f = \{x\;:\;\exists \mu,(x,\mu)\in\mathrm{epi}\ f\} = \{x\;:\;f(x)<+\infty\}.
    \end{equation*}
\end{definition}

\begin{remark}
    % This is a convex set in $\R^n$, since it is the image of the the convex set epi $f$ under a linear transformation. Its dimension is called the \highlight{dimension} of $f$ (Theorem \ref{thm:convex_set_under_linear_transformation}).
% 
    Since a convex function $f$ on $S$ can always be extended to a convex function on all of $\R^n$ by setting $f(x) = +\infty$ for $x\notin S$, we shall henceforth refer by a ``convex function'' a  ``convex function with possibly infinte values which is defined throughout the space of $\R^n$.'' This leads to extened arithmetic rules involving $\pm \infty$ where the only forbidden calculations are $\infty -\infty$ and $-\infty + \infty$.
\end{remark}

\begin{definition}
    A convex function $f$ is said to be \highlight{proper} if its epigraph is non-empty and contains no vertical lines, i.e., $f(x)<+\infty$ for at least one $x$ and $f(x)>-\infty$ for every $x$. A convex function which is not proper is \highlight{improper}.
\end{definition}

% \begin{remark}
%     Thus $f$ is proper iff the convex set $C = $ dom $f$ is non-empty and the restriction of $f$ to $C$ is finite.
% \end{remark}

\begin{theorem}\label{thm:convex_function_def}
    Let $f$ be a function from $C$ to $(-\infty,+\infty]$, where $C$ is a convex set. Then $f$ is convex on $C$ iff 
    \begin{equation*}
        f((1-\lambda)x+\lambda y) \le (1-\lambda) f(x) + \lambda f(y), \quad 0<\lambda < 1,
    \end{equation*}
    for every $x,y\in C$.
\end{theorem}

\begin{proof}
    By definition, $f$ is convex on $C$ iff 
    \begin{equation*}
        (1-\lambda)(x,\mu) + \lambda (y,\nu) = ((1-\lambda)x+\lambda y,(1-\lambda)\mu+\lambda \nu) \in \mathrm{epi}\ f
    \end{equation*}
    whenever $(x,\mu)$ and $(y,\nu)$ belong to epi $f$ and $0\le\lambda\le 1$. By setting $\mu = f(x)$ and $\nu = f(y)$, we get the required result.
\end{proof}

\begin{corollary}\label{cor:convex_function_def_1}
    Let $f$ be a function from $\R^n$ to $[-\infty,+\infty]$. Then $f$ is convex on $C$ iff 
    \begin{equation*}
        f((1-\lambda)x+\lambda y) < (1-\lambda) \alpha + \lambda \beta, \quad 0<\lambda < 1,
    \end{equation*}
    whenever $f(x)<\alpha$ and $f(y)<\beta$.
\end{corollary}

\begin{corollary}[Jensen's Inequality]\label{cor:convex_function_def_2}
    Let $f$ be a function from $\R^n$ to $(-\infty,+\infty]$. Then $f$ is convex iff 
    \begin{equation*}
        f(\lambda_1 x_1 + \cdots + \lambda_m x_m)\le \lambda_1 f(x_1) + \cdots + \lambda_m f(x_m)
    \end{equation*}
    whenever $\lambda_1\ge 0,\ldots,\lambda_m\ge 0$, $\lambda_1 + \cdots + \lambda_m = 1$.
\end{corollary}

\begin{remark}
    The inequality in Theorem \ref{thm:convex_function_def} is often taken as the definition of the convexity of a function $f$ from a convex set $C$ to $(-\infty,+\infty]$. This approach causes difficulties, however, when $f$ can have both $+\infty$ and $-\infty$ among its values, since the expression $\infty -\infty$ could arise. One could instead use the condition in Corollary \ref{cor:convex_function_def_1} as the definition of convexity in the general case, but the definition given at the beginning of this section seems preferable because it emphasizes the geometry which is fundamental to the theory of convex functions.
\end{remark}

\begin{theorem}\label{thm:convex_function_second_order}
    Let $f$ be a twice continuously differentiable real-valued function on an open interval $(\alpha,\beta)$. Then $f$ is convex iff its second derivative $f^{\prime\prime}$ is nonnegative throughout $(\alpha,\beta)$.
\end{theorem}

\begin{proof}
    \converse Suppose first that $f^{\prime\prime}$ is nonnegative on $(\alpha,\beta)$. Then $f^\prime$ is non-decreasing on $(\alpha,\beta)$. For $\alpha<x<y<\beta$, $0<\lambda<1$ and $z = (1-\lambda)x+\lambda y$, we have $f^\prime(x)\le f^\prime (z)\le f^\prime(y)$ and
    \begin{equation*}
        f(z) - f(x) = \int_x^z f^\prime (t)dt \le f^\prime(z)(z-x), 
    \end{equation*}
    \begin{equation*}
        f(y) - f(z) = \int_z^y f^\prime (t)dt \ge f^\prime(z)(y-z). 
    \end{equation*}
    Since $z-x = \lambda(y-x)$ and $y-z = (1-\lambda)(y-x)$, we have
    \begin{align*}
        f(z)&\le f(x) + \lambda f^\prime(z)(y-x),\\
        f(z)&\le f(y) - (1-\lambda) f^\prime(z)(y-x).
    \end{align*}
    Multiplying the two inequalities by $(1-\lambda)$ and $\lambda$ respectively and adding them together, we get
    \begin{equation*}
        f(z) = f((1-\lambda)x + \lambda y) \le (1-\lambda)f(x) + \lambda f(y).
    \end{equation*}
    \\
    \forward Suppose that $f^{\prime\prime}$ were not nonnegative on $(\alpha,\beta)$. Then $f^{\prime\prime}$ would be negative on a certain subinterval $(\alpha^\prime,\beta^\prime)$ by continuity. Then by similar arguments as above, we would have
    \begin{equation*}
        f(z) - f(x) > f^\prime (z)(z-x),
    \end{equation*} 
    \begin{equation*}
        f(y) - f(z) < f^\prime (z)(y-z),
    \end{equation*} 
    and hence 
    \begin{equation*}
        f((1-\lambda)x + \lambda y) > (1-\lambda)f(x) + \lambda f(y).
    \end{equation*}
    Thus, $f$ would not be convex on $(\alpha,\beta)$.
\end{proof}

\begin{example}\label{ex:convex_functions}
    Here are some functions on $\R$ convexity is a consequence of Theorem \ref{thm:convex_function_second_order}.
    \begin{enumerate}
        \item $f(x) = e^{\alpha x}$, where $-\infty < \alpha < +\infty$.
        \item $f(x) = x^p$ if $x\ge 0$ and $f(x) = \infty$ if $x<0$, where $1\le p< \infty$.
        \item $f(x) = -x^p$ if $x\ge 0$, $f(x) = \infty$ if $x\le 0$, where $0\le p\le 1$.
        \item $f(x) = x^p$ if $x> 0$ and $f(x) = \infty$ if $x\le 0$, where $p\in (-\infty,0]\cup [1,\infty)$.
        \item $f(x) = (\alpha^2-x^2)^{-1/2}$ if $|x|<\alpha$, $f(x) = \infty$ if $|x|\ge\alpha$, where $\alpha>0$.
        \item $f(x) = -\log{x}$ if $x>0$, $f(x) = \infty$ if $x\le 0$.
    \end{enumerate}
\end{example}

\begin{theorem}
    Let $f$ be a twice continuously differentiable real-valued function on an open convex set $C$ in $\R^n$. Then $f$ is convex iff its Hessian matix Hess $f(x)$ is positive semi-definite for every $x\in C$.
\end{theorem}

\begin{proof}
    The convexity of $f$ on $C$ is equivalent to the convexity of the restriction of $f$ to each line segment in $C$. This is the same as the convexity of the function $g(\lambda) = f(y+\lambda z)$ on the open real interval $\{\lambda\;:\;y+\lambda z\in C\}$ for each $y\in C$ and $z\in \R^n$. Note that 
    \begin{equation*}
        g^{\prime\prime}(\lambda) = \innerproduct{z}{\mathrm{Hess}\ f(y+\lambda z)z}.
    \end{equation*}
    Thus, by Theorem \ref{thm:convex_function_second_order}, $g$ is convex for each $y\in C$ and $z\in \R^n$ iff $\innerproduct{z}{\mathrm{Hess}\ f(y+\lambda z)z}\ge 0$ for every $z\in \R^n$ and $y\in C$.
\end{proof}

\begin{remark}[Euclidean Norm and distance]\label{remark:euclidean_norm_distance}
    Consider the Euclidean norm on $\R^n$ 
    \begin{equation*}
        \|x\| = \innerproduct{x}{x}^{1/2} = (x_1^2+\ldots+x_n^2)^{1/2}.
    \end{equation*}
    Suppose $x,y\in \R^n$ and $0\le\lambda\le 1$. Then
    \begin{equation*}
        \|(1-\lambda)x+\lambda y\| \le (1-\lambda)\|x\| + \lambda \|y\|,
    \end{equation*}
    which shows the convexity of Euclidean norm. Next, consider the Euclidean distance on $\R^n$
    \begin{equation*}
        d(x,y) = \|x-y\|.
    \end{equation*}
    Suppose $(x,y),(x^\prime,y^\prime)\in \R^{2n}$ and $0\le \lambda\le 1$. Then
    \begin{equation*}
        d((1-\lambda)(x,y) + \lambda (x^\prime,y^\prime)) = \|(1-\lambda)x -(1-\lambda) y + \lambda x^\prime -\lambda y^\prime\| \le (1-\lambda) d(x,y) + \lambda d(x^\prime,y^\prime),
    \end{equation*}
    which shows the convexity of Euclidean distance.
\end{remark}

\begin{definition}
    For a set $C$ in $\R^n$ the \highlight{indicator function} $\delta_C(x)$ of $C$ is defined as 
    \begin{equation*}
        \delta_C(x) = 
        \begin{cases}
			0, & \text{if }x\in C\\
            +\infty, & \text{if } x\notin C.
        \end{cases}
    \end{equation*}
\end{definition}

\begin{remark}
    The epigraph of the indicator function is $C\times [0,\infty)$ which is a ``half-cylinder with cross-section $C$''. 
\end{remark}

\begin{proposition}
    A set $C$ in $\R^n$ is a convex set iff $\delta_C$ is a convex function in $\R^n$. 
\end{proposition}

\begin{proof}
    The set $C\subseteq\R^n$ is convex iff $C\times [0,\infty)\subseteq\R^{n+1}$ is convex by Theorem \ref{thm:direct_sum_convex} and Remark \ref{remark:direct_sum_cartesian_product}. But then epi $\delta_C$ is convex which means that $\delta_C$ is convex.
\end{proof}

\begin{definition}\label{def:support_function}
    The \highlight{support function} $\delta_C^*$ of a convex set $C$ in $\R^n$ is defined by
    \begin{equation*}
        \delta_C^*(x) = \sup\{\innerproduct{x}{y}\;:\;y\in C\}.
    \end{equation*}
\end{definition}

\begin{definition}\label{def:gauge_function}
    The \highlight{gauge function} $\gamma_C$ of a convex set $C$ in $\R^n$ is defined by 
    \begin{equation*}
        \gamma_C(x) = \inf\{\lambda\ge 0\;:\; x\in \lambda C\},\quad C\neq \emptyset.
    \end{equation*}
\end{definition}

\begin{definition}\label{def:distance_function}
    The (Euclidean) \highlight{distance function} $d_C$ is defined by
    \begin{equation*}
        d_C(x) = \inf\{\|x-y\|\;:\;y\in C\}.
    \end{equation*}
\end{definition}

\begin{remark}
    The convexity of these functions (in Def. \ref{def:support_function}, \ref{def:gauge_function} and \ref{def:distance_function}) on $\R^n$ could be verified now directly, but we shall wait until the next section, where the convexity can be shown to follow from general principles.
\end{remark}

\begin{theorem}\label{thm:convex_level_sets}
    For any convex function $f$ and any $\alpha\in [-\infty,+\infty]$, the level sets $\{x\;:\;f(x)<\alpha\}$ and $\{x\;:\;f(x)\le\alpha\}$ are convex.
\end{theorem}

\begin{proof}
    In the case of strict inequality the result is immediate from Corollary \ref{cor:convex_function_def_1}, with $x = y$ and $\beta = \alpha$. The convexity of $\{x\;:\;f(x)\le \alpha\}$ then follows from the fact that it is the intersection of the convex sets $\{x\;:\;f(x)\le \mu\}$ for $\mu>\alpha$, i.e.,
    \begin{equation*}
        \{x\;:\;f(x)\le \alpha\} = \bigcap_{\mu>\alpha} \{x\;:\;f(x)< \mu\}.
    \end{equation*}
    A more geometric way of seeing this convexity is to observe that $\{x\;:\;f(x)\le \alpha\}$ is the projection on $\R^n$ of the intersection of epi $f$ and the horizontal hyperplane $\{(x,\mu)\;:\;\mu=\alpha\}$ in $\R^{n+1}$, so that $\{x\;:\;f(x)\le \alpha\}$ can be regarded as a horizontal cross-section of epi $f$.
\end{proof}

\begin{corollary}\label{cor:intersection_convex_sets_convex_map}
    Let $f_i$ be a convex function on $\R^n$ and $\alpha_i$ be a real number for each $i\in I$, where $I$ is an arbitrary index set. Then
    \begin{equation*}
        C = \{x\;:\;f_i(x)\le \alpha,\ \forall i\in I\}
    \end{equation*}
    is a convex set.
\end{corollary}

\begin{proof}
    Like Corollary \ref{cor:intersection_convex_sets_affine_map}.
\end{proof}

\begin{definition}\label{def:postively_homogeneous}
    A function $f$ on $\R^n$ is said to be \highlight{positively homogeneous} (of degree $1$) if for every $x$ one has
    \begin{equation*}
        f(\lambda x) = \lambda f(x), \quad 0<\lambda<\infty.
    \end{equation*}
\end{definition}

\begin{remark}
    Positive homogeneity is equivalent to the epigraph being a cone in $\R^{n+1}$. 
\end{remark}

\begin{theorem}
    A positively homogeneous function $f$ from $\R^n$ to $(-\infty,+\infty]$ is convex iff
    \begin{equation}\label{eqn:subadditivity}
        f(x+y) \le f(x) + f(y)
    \end{equation}
    for every $x,y\in\R^n$.
\end{theorem}

\begin{proof}
    This is implied by Theorem \ref{thm:convex_cone_conic_combinations}, because the subadditivity condition of $f$ in \eqref{eqn:subadditivity} is equivalent to epi $f$ being closed under addition.
\end{proof}

\begin{corollary}
    If $f$ is a positively homogeneous proper convex function, then
    \begin{equation*}
        f(\lambda_1 x_1+\cdots+\lambda_m x_m) \le \lambda_1 f(x_1) + \cdots + \lambda_m f(x_m)
    \end{equation*}
    whenever $\lambda_1>0,\ldots,\lambda_m>0$.
\end{corollary}

\begin{corollary}
    If $f$ is a positively homogeneous proper convex function, then $f(-x) \ge -f(x)$ for every $x$.
\end{corollary}

\begin{proof}
    We have $f(x)+f(-x) \ge f(x-x) = f(0) \ge 0$.
\end{proof}

\begin{theorem}
    A positively homogeneous proper convex function $f$ is linear on a subspace $L$ iff $f(-x) = -f(x)$ for every $x\in L$. The converse is true if merely $f(-b_i) = -f(b_i)$ for all the vectors in some basis $b_1,\ldots,b_m$ for $L$.
\end{theorem}

\begin{proof}
    \forward Linearity of $f$ implies $f(-x)=-f(x)$ for every $x\in L$. 
    
    \noindent\converse Assume that $f$ is positively homogeneous proper convex function and that $f(-b_i) = -f(b_i)$ for all the vectors in some basis $b_1,\ldots,b_m$ for $L$. Then $f(\lambda_i b_i) = \lambda_i f(b_i)$ for every $\lambda_i \in \R$, not just for $\lambda_i>0$. For any $x = \lambda_1 b_1+\cdots+\lambda_m b_m\in L$ we have
    \begin{align*}
        f(\lambda_1 b_1) + \cdots + f(\lambda_m b_m) \ge f(x) \ge -f(-x) &\ge -(f(-\lambda_1 b_1) + \cdots + f(-\lambda_m b_m)) \\
        &= f(\lambda_1 b_1) + \cdots + f(\lambda_m b_m),
    \end{align*}
    and hence 
    \begin{equation*}
        f(x) = f(\lambda_1 b_1)+ \cdots + f(\lambda_m b_m) = \lambda_1 f(b_1) + \cdots + \lambda_m f(b_m).
    \end{equation*}
    Thus $f$ is linear on $L$.
\end{proof}

\subsection{Functional Operations}

\begin{theorem}\label{thm:composite_convex_function}
    Let $f$ be a convex function from $\R^n$ to $(-\infty,+\infty]$, and let $\varphi$ be a convex function from $\R$ to $(-\infty,+\infty]$ which is non-decreasing. Then $h(x) = \varphi(f(x))$ is convex on $\R^n$ (where one sets $\varphi(+\infty) = +\infty$).
\end{theorem}

\begin{proof}
    For any $x,y\in \R^n$ and $0<\lambda<1$, from Theorem \ref{thm:convex_function_def} we have
    \begin{equation*}
        f((1-\lambda)x+\lambda y)\le (1-\lambda)f(x)+\lambda f(y).
    \end{equation*}
    Applying $\varphi$ to both sides of this inequality, we retain the inequality sign due to the non-decreasing property of $\varphi$ to get
    \begin{equation*}
        h((1-\lambda)x+\lambda y)\le \varphi((1-\lambda)f(x)+\lambda f(y)) \le (1-\lambda)h(x)+\lambda h(y).
    \end{equation*}
\end{proof}

\begin{example}
    Motivated from Example \ref{ex:convex_functions} we give some examples of convex function from $\R^n$ to $\R$ as a consequence of Theorem \ref{thm:composite_convex_function}.
    \begin{enumerate}
        \item $h(x) = e^{f(x)}$ is a proper convex function when $f$ is proper convex.
        \item $h(x) = f(x)^p$ is convex for $p\ge 1$ when $f$ is convex and nonnegative.
        \item $h(x) = 1/g(x)$ is convex on $C = \{x\;:\; g(x)>0\}$ when $g$ is concave. To see this, take $f = -g$ and $\varphi(\xi) = -1/\xi$ for $\xi<0$, and $\varphi(\xi)=+\infty$ for $\xi\ge 0$.
        \item $h(x) = \lambda f(x)+\alpha$ is a proper convex function when $f$ is proper convex and $\lambda>0$.
    \end{enumerate}
\end{example}

\begin{theorem}\label{thm:sum_of_convex_functions}
    If $f_1$ and $f_2$ are proper convex functions on $\R^n$, then $f_1+f_2$ is convex.
\end{theorem}

\begin{remark}
    The properness in the hypothesis of Theorem \ref{thm:sum_of_convex_functions} is to avoid $\infty-\infty$ when $f_1+f_2$ is formed. Also note that the dom $f_1+f_2 =$ dom $f_1\cap$ dom $f_2$, where dom $f$ is the effective domain defined in Def. \ref{def:effective_domain}. Thus, $f_1+f_2$ is improper is when the intersection of the effective domains is empty. 
\end{remark}

\begin{corollary}
    A linear combination $\lambda_1 f_1+\cdots + \lambda_m f_m$ of proper convex functions with nonnegative coefficients is convex.
\end{corollary}

\begin{theorem}\label{thm:convex_set_inf_convex_function}
    Let $F$ be any convex set in $\R^{n+1}$, and let 
    \begin{equation*}
        f(x) = \inf\{\mu\;:\;(x,\mu)\in F\}.
    \end{equation*}
    Then $f$ is a convex function on $\R^n$.
\end{theorem}

\begin{proof}
    Let $(x,\mu),(y,\gamma)\in F$ so that $((1-\lambda)x+\lambda y, (1-\lambda)\mu+\lambda \gamma)\in F$. This implies that
    \begin{equation}\label{eqn:}
        f((1-\lambda)x+\lambda y) \le (1-\lambda)\mu+\lambda \gamma,
    \end{equation} 
    such that $f(x)\le \mu$ and $f(y)\le \gamma$. Then by Cor. \ref{cor:convex_function_def_1}, $f$ is convex.
\end{proof}

\begin{remark}
    It is noteworthy to compare epi $f$ with $F$. Now suppose $(x,\mu)\in F$, then $f(x)\le \mu$ by definition of $f$, so that $(x,\mu)\in \mathrm{epi}\ f$. This shows that $F\subseteq \mathrm{epi}\ f$. 
\end{remark}

\begin{proposition}\label{prop:f_inf_epi_f}
    Let $f$ be a function on $\R^n$. Then
    \begin{equation*}
        f(x) = \inf \{\mu\;:\;(x,\mu)\in \mathrm{epi}\ f\}.
    \end{equation*}
\end{proposition}

\begin{proof}
    We need to show that $f(x)$ is the greatest lower bound of the set $M = \{\mu\;:\;(x,\mu)\in \mathrm{epi}\ f\}$. Note that for all $(x,\mu)\in \mathrm{epi}\ f$, we have $f(x)\le \mu$ from the definition of the epigraph. For any $\epsilon>0$, we have $\mu = f(x)\in M$ such that $\mu-\epsilon<f(x)$ showing that $f(x)$ is the greatest lower bound of $M$.
\end{proof}

\begin{definition}
    The \highlight{infimal convolution} of $f_1,\ldots,f_m$ (functions on $\R^n$) is the function on $\R^n$ defined as 
    \begin{equation*}
        (f_1\square \cdots \square f_m)(x) = \inf\{f_1(x_1)+\cdots + f_m(x_m)\;:\;x_i\in R^n,x_1+\cdots+x_m = x\}.
    \end{equation*} 
\end{definition}

\begin{remark}
    This terminology arises from the fact that, when only two functions are involved, $\square$ can be expressed by
    \begin{equation*}
        (f\square g)(x) = \inf_{y} \{f(x-y)+g(y)\},
    \end{equation*}
    and this is analogous to the classical formula for integral convolution.
\end{remark}

\begin{proposition}
    Let $f_1,\ldots,f_m$ be proper convex functions on $\R^n$, then $f_1\square \cdots \square f_m$ is a convex function on $\R^n$.
\end{proposition}

\begin{proof}
    Let $F_i =$ epi $f_i$ and $F = F_1+\cdots + F_m$. Then $F$ is a convex set in $\R^{n+1}$. By definition, $(x,\mu)\in F$ iff there exists $x_i\in \R^n$, $\mu_i\in \R$, such that $\mu_i\ge f_i(x_i)$, $\mu = \mu_1+\cdots+\mu_m$ and $x = x_1+\cdots + x_m$. Thus $f$ defined in the theorem is the convex function obtained from $F$ by the construction in Theorem \ref{thm:convex_set_inf_convex_function}.
\end{proof}

\begin{remark}
    We note few properties of infimal convolution operator below. Consider two function $f,g$ on $\R^n$.
    \begin{itemize}
        \item If $g = \delta_{\{a\}}$ for a certain poinr $a\in \R^n$, then $(f\square \delta_{\{a\}})(x) = f(x-a)$, which is the function whose graph is obtained by translating the graph of $f$ horizontally by $a$.
        \item The effective domain of $f\square g$ is the sum of dom $f$ and dom $g$.
        \item Taking $f$ to be the Euclidean norm and $g$ to be the indicator function of a convex set $C$, we get
        \begin{equation*}
            (f\square g)(x) = \inf_y \{\|x-y\|+\delta_C(y)\} = \inf_{y\in C} \|x-y\| = d_C(x).
        \end{equation*}
        This establishes the convexity of the distance function $d_C$ defined in Def. \ref{def:distance_function}.
        \item Properness of convex functions is not always preserved by infimal convolution, since the infimum in the definition may be $-\infty$. Nor is infimal convolution of improper functions defined by this formula, because of the rule of avoiding $\infty-\infty$. However, $f_1\square f_2$ can be defined for any functions $f_1$ and $f_2$ from $\R^n$ to $[-\infty,+\infty]$ directly in terms of addition of epigraphs:
        \begin{equation*}
            (f_1\square f_2)(x) = \inf\{\mu\;:\; (x,\mu)\in (\mathrm{epi}\ f_1+\mathrm{epi}\ f_2)\}.
        \end{equation*}
        \item As an operation on the collection of all functions from $\R^n$ to $[-\infty,+\infty]$, infimal convolution is commutative, associative and convexity-preserving. The function $\delta_{\{0\}}$ acts as the identity element for this operation. 
    \end{itemize}
\end{remark}

\begin{definition}
    We define \highlight{right scalar multiple}, denoted by $f\lambda$,  for any function $f$ on $\R^n$ and $0\le\lambda<\infty$ to be the following function
    \begin{equation*}
        (f\lambda)(x) = \inf\{\mu\;:\;(x,\mu)\in \lambda (\mathrm{epi}\ f)\}.
    \end{equation*}
\end{definition}

\begin{remark}
    We note few properties of the right scalar multiple operator below. Let $f$ be a function on $\R^n$ and $0\le\lambda<\infty$.
    \begin{itemize}
        \item If $f$ is convex, then $\lambda(\mathrm{epi}\ f)$ is a convex set. From the definition of $f\lambda$ and Theorem \ref{thm:convex_set_inf_convex_function}, we see that $f\lambda$ is convex. 
        \item From the definition, when $\lambda>0$ we have
        \begin{align}
            (f\lambda)(x) &= \inf\{\mu\;:\;(x,\mu)\in \lambda (\mathrm{epi}\ f)\} \nonumber\\
            &= \inf\{\lambda (\mu/\lambda)\;:\;(x/\lambda,\mu/\lambda)\in (\mathrm{epi}\ f)\}\nonumber\\
            &\overset{(\dagger)}{=} \lambda f(\lambda^{-1}x),\label{eqn:f_lambda_alternate_def}
        \end{align}
        where we used Prop. \ref{prop:f_inf_epi_f} to obtain $(\dagger)$. When $\lambda = 0$, $(f0)(x) = \delta_{\{0\}}(x)$ for $f\not\equiv +\infty$, otherwise $f0 = f$, for $f\equiv +\infty$.
        \item Consider the epigraph of $f\lambda$ for $\lambda>0$
        \begin{align}
            \mathrm{epi}\ (f\lambda) &= \{(x,\mu)\;:\; \mu\ge (f\lambda)(x)\}\nonumber \\
            &= \{(x,\mu)\;:\; \mu\ge \lambda f(\lambda^{-1} x)\}\nonumber \\
            &= \lambda\{(\lambda^{-1} x,\lambda^{-1} \mu)\;:\; \lambda^{-1}\mu\ge f(\lambda^{-1} x)\}\nonumber \\
            &= \lambda (\mathrm{epi}\ f).\label{eqn:epi_f_lambda}
        \end{align}
        \item The function $f$ is positively homogeneous iff $f\lambda = f$ for each $\lambda>0$. This is evident from \eqref{eqn:f_lambda_alternate_def} and Def. \ref{def:postively_homogeneous}.
    \end{itemize}
\end{remark}

\begin{remark}
    Let $h$ be a convex function on $\R^n$ and define a function $f$ as follows:
    \begin{equation*}
        f(x) = \inf \{\mu\;:\;(x,\mu)\in \mathrm{cone}(\mathrm{epi}\ h)\}.
    \end{equation*}
    Then epi $f \overset{\textcolor{red}{(?)}}{=} \mathrm{cone}\ (\mathrm{epi}\ h) = \bigcup_{\lambda\ge 0} \lambda (\mathrm{epi}\ h)$. Thus, epi $f$ is a convex cone in $\R^{n+1}$ containing origin. This shows that $f$ is positively homogeneous convex function with the property that $f(0)\le 0$ and $f(x)\le h(x)$ for all $x\in$ dom $h$. From the definition of $f$ it is also true that $f$ is the \textit{greatest} such function satisfying these conditions. We call this $f$ to be the \highlight{positively homogeneous convex function generated by \bsdnn{h}}. Since $\mathrm{cone}\ (\mathrm{epi}\ h) = \bigcup_{\lambda\ge 0} \lambda (\mathrm{epi}\ h)$, using the right scalar multiple definition here we also have
    \begin{equation*}
        f(x) = \inf\{(h\lambda)(x)\;:\;\lambda\ge 0\},\quad\text{when}\ h\not\equiv +\infty.
    \end{equation*}

    \noindent Consider the function $h(x) = \delta_C(x)+1$ for a non-empty convex set in $\R^n$. Then the positively homogeneous convex function generated by $h$ at $x\in R^n$ is 
    \begin{equation*}
        \inf\{(h\lambda)(x)\;:\;\lambda\ge 0\} = \inf \{\delta_{\lambda C}(x)+\lambda\;:\; \lambda\ge 0\} = \inf\{\lambda\ge 0\;:\; x\in \lambda C\} = \gamma_C(x).
    \end{equation*}
    Thus the gauge function defined in Def. \ref{def:gauge_function} is positively homogeneous convex function on $\R^n$.
\end{remark}

\begin{theorem}\label{thm:pointwise_sup}
    The pointwise supremum of an arbitrary collection of convex functions is convex.
\end{theorem}

\begin{proof}
    Let $f_i$ be an arbitrary collection of convex sets indexed by $I$. Then the pointwise supremum of this collection is defined as
    \begin{equation*}
        f(x) = \sup \{f_i(x)\;:\; i\in I\}.
    \end{equation*}
    It is easy to set that epi $f = \bigcap_{i\in I} \mathrm{epi}\ f_i$. Then by the definition of convexity and Lemma \ref{lem:intersection_convex_cones}, $f$ is convex.
\end{proof}

\begin{remark}
    Here are few applications of Theorem \ref{thm:pointwise_sup}.
    \begin{itemize}
        \item The convexity of the support function $\delta^*_C$ defined in Def. \ref{def:support_function} is implied by Theorem \ref{thm:pointwise_sup}, because this function is by definition the pointwise supremum of the collection of linear function $\innerproduct{\cdot}{y}$ as $y$ ranges over $C$.
        \item Consider the function $f$ which assigns to each $x = (\xi_1,\ldots,\xi_n)$ the greatest of the components $\xi_i$ of $x$. This function is convex by Theorem \ref{thm:pointwise_sup}, because it is the pointwise supremum of the linear function $\innerproduct{x}{e_j}$, $j = 1,\ldots,n$, where $e_j$ is the $j$th column of the $n\times n$ identity matrix. The function $f$ is also positively homogeneous and in fact $f$ is the support function of the simplex
        \begin{equation*}
            C = \{y = (\eta_1,\ldots,\eta_n)\;:\;\eta_j\ge 0,\eta_1+\cdots+\eta_n = 1\}.
        \end{equation*}
        as for an $x\in \R^n$, $\innerproduct{x}{y}\le \max \{\xi_j\;:\;j=1,\ldots,n\}$ which is attained by setting $y = e_j$ when $\xi_j$ is maximum.
        \item The \highlight{Tchebycheff norm} or the $\ell_{\infty}$ norm on $\R^n$ is defined as
        \begin{equation*}
            k(x) = \max \{|\xi_j|\;:\;j = 1,\ldots,n\},
        \end{equation*}
        which is a convex function by Theorem \ref{thm:pointwise_sup}. This function is the support function of the convex set 
        \begin{equation*}
            D = \{y=(\eta_1,\ldots,\eta_n)\;:\; |\eta_1|+\cdots+|\eta_n| = 1\},
        \end{equation*}
        which can been following similar arguments to the previous case and at the same time the gauge of the $n$-dimensional cube 
        \begin{equation*}
            E = \{x = (\xi_1,\ldots,\xi_n)\;:\;-1\le \xi_j\le 1, j = 1,\ldots,n\}.
        \end{equation*}
        This can be seen by noting that 
        \begin{equation*}
            \inf \{\lambda\ge 0\;:\;|\xi_j|\le \lambda,\ j = 1,\ldots,n\} = \max \{|\xi_j|\;:\;j=1,\ldots,n\} = k(x).
        \end{equation*}
    \end{itemize}
\end{remark}

\begin{definition}
    The {convex hull} of a non-convex function $g$, denoted by conv $g$, is the \textit{greatest convex function majorized by} $g$ which is defined as 
    \begin{equation*}
        \mathrm{conv}\ g = \inf \{\mu\;:\;(x,\mu)\in \mathrm{conv}(\mathrm{epi}\ g)\}.
    \end{equation*}
    The convex hull of an arbitrary collection of functions $\{f_i\;:\;i\in I\}$ on $\R^n$ is the \textit{greatest convex function} $f$ defined as 
    \begin{equation*}
        f(x) = \inf \{\mu\;:\;(x,\mu)\in \mathrm{conv}(\cup_{i\in I}\ \mathrm{epi}\ f_i)\}.
    \end{equation*}
\end{definition}

\begin{remark}
    Another way to represent the convex hull of $g$ is as follows. By Theorem \ref{thm:convex_hull_convex_combinations} $(x,\mu)\in \mathrm{conv}(\mathrm{epi}\ g)$ iff there exist $(x_i,\mu_i)\in \mathrm{epi}\ g$ and $\lambda_i\ge 0$ for $i = 1,\ldots,m$ such that 
    \begin{equation*}
        (x,\mu) = \lambda_1 (x_1,\mu_1) + \cdots + \lambda_m (x_m,\mu_m) = (\lambda_1 x_1 + \cdots + \lambda_m x_m,\lambda_1\mu_1+\cdots+\lambda_m\mu_m),
    \end{equation*}
    and $\lambda_1+\cdots+\lambda_m = 1$. Then
    \begin{equation*}
        \mathrm{conv}\ g = \inf\{\lambda_1g(x_1)+\cdots+\lambda_mg(x_m)\;:\; \lambda_1x_1+\cdots+\lambda_mx_m = x, \lambda_1+\cdots+\lambda_m = 1\},
    \end{equation*}
    provided $g$ does not take on the value $-\infty$, so that the summation is unambiguous.
\end{remark}

\begin{theorem}
    Let $\{f_i\;:\;i\in I\}$ be an arbitrary collection of proper convex functions on $\R^n$, and let $f$ be the convex hull of the collection. Then
    \begin{equation*}
        f(x) = \inf\bigg\{\sum_{i\in I} \lambda_i f_i(x_i)\;:\;\sum_{i\in I}\lambda_i x_i = x\bigg\},
    \end{equation*}
    where the infimum is taken over all representation of $x$ as a convex combinations of elements $x_i$, such that only finitely many coefficients $\lambda_i$ are non-zero. (The formula is also valid if one actually restricts $x_i$ to lie in dom $f_i$.)
\end{theorem}

\begin{proof}
    By definition $f(x)$ is the infimum of the values of $\mu$ such that $(x,\mu)\in F$, where $F$ is the convex hull of the union of convex sets $C_i = \mathrm{epi}\ f_i$. Each $C_i$ is non-empty as they are assumed to be proper. By Theorem \ref{thm:convex_hull_finite_convex_combinations}, $(x,\mu)\in F$ iff $(x,\mu)$ can be expressed as a finite combination of the form
    \begin{equation*}
        (x,\mu) = \sum_{i\in I} \lambda_i (x_i,\mu_i) = \bigg(\sum_{i\in I}\lambda_i x_i, \sum_{i\in I}\lambda_i \mu_i \bigg),
    \end{equation*}
    where $(x_i,\mu_i)\in C_i$ (only finitely many of the coefficients being non-zero). Thus $f(x)$ is the infimum of $\sum_{i\in I}\lambda_i \mu_i$ over all expressions of $x$ as a finite convex combinations $\sum_{i\in I}\lambda_i x_i$ with $\mu_i\ge f_i(x_i)$ for every $i$. This is same as the infimum in the theorem statement.
\end{proof}

\begin{remark}
    Similar to Remark \ref{remark:convex_sets_complete_lattice} for convex sets, the collection of all convex functions on $\R^n$, regarded as a partially ordered set relative to the pointwise ordering (where $f\ge g$ iff $f(x)\ge g(x)$ for every $x$), is a complete lattice. The greatest lower bound of a family of convex functions is the convex hull of the collection, while the least upper bound is $\sup\{f_i\;:\;i\in I\}$. 
\end{remark}

\begin{theorem}\label{thm:convex_function_linear_transformation}
    Let $A$ be a linear transformation from $\R^n$ to $\R^m$. Then, for each convex function $g$ on $\R^m$, the function $gA$ defined by
    \begin{equation*}
        (gA)(x) = g(Ax)
    \end{equation*}
    is convex on $\R^n$. For each convex function $h$ on $\R^n$, the function $Ah$ defined by
    \begin{equation*}
        (Ah)(y) = \inf\{h(x)\;:\;Ax = y\}
    \end{equation*}
    is convex on $\R^m$.  
\end{theorem}

\begin{proof}
    Let $x,y\in \R^n$ and $0\le \lambda\le 1$. Then 
    \begin{equation*}
        (gA)((1-\lambda)x+\lambda y) = g((1-\lambda)Ax+\lambda Ay) \overset{(\dagger)}{=} (1-\lambda)(gA)(x) + \lambda (gA)(y),
    \end{equation*}
    where we used the convexity of $g$ to obtain $(\dagger)$. This shows the convexity of $gA$.

    \noindent The convexity of $f = Ah$ also follows from applying Theorem \ref{thm:convex_set_inf_convex_function} to the image $F$ of the epigraph of $h$ under the linear transformation $(x,\mu)\rightarrow (Ax,\mu)$ from $\R^{n+1}$ to $\R^{m+1}$.
\end{proof}

\begin{remark}
    The function $Ah$ in Theorem \ref{thm:convex_function_linear_transformation} is called the \highlight{image of \bsdnn{h} under \bsdnn{A}}, while $gA$ is called the \highlight{inverse image of \bsdnn{g} under \bsdnn{A}}. This terminology is suggested by the case where $g$ and $h$ are indicators of convex sets.
\end{remark}

\begin{proposition}
    Let $h$ be any map on $\R^n$ and $A$ is a linear transformation on $\R^n$. If $A$ is non-singular, then $Ah = hA^{-1}$.
\end{proposition}

\begin{proof}
    Consider the expressions of $Ah$ 
    \begin{equation*}
        (Ah)(y) = \inf\{h(x)\;:\;Ax = y\} = \inf \{h(A^{-1}y)\} \overset{(\dagger)}{=} h(A^{-1}y) = (hA^{-1})(y),
    \end{equation*}
    where we used the uniqueness of $A^{-1}y$ in $(\dagger)$.
\end{proof}

\newpage 

\section{Topological Properties}

\subsection{Relative Interiors of Convex Sets}

\begin{proposition}
    Let $f$ be a real-valued continuous convex function on $\R^n$. Then $\{x\;:\;f(x)<\alpha\}$ is an open convex level set and $\{x\;:\;f(x)\le \alpha\}$ is a closed convex level set in $\R^n$.
\end{proposition}

\begin{proof}
    The level sets can be written as $f^{-1}(I)$ and $f^{-1}(C)$ respectively, where $I = (\alpha,\infty)$ is open and $C = [\alpha,\infty)$ is closed in $\R$. Then by continuity of $f$, we have that $f^{-1}(I)$ is open and $f^{-1}(C)$ is closed in $\R^n$. The convexity of the level sets follows from Theorem \ref{thm:convex_level_sets}. 
\end{proof}

We shall denote by $\mathbb{B}$ the (closed) \highlight{unit ball} in $\R^n$:
\begin{equation*}
    \mathbb{B} = \{x\;:\;\|x\|\le 1\} = \{x\;:\; d(x,0)\le 1\}.
\end{equation*}
% This is a closed convex set as it is a level set of Euclidean norm, which is continuous and convex. Similarly, the interior of $\mathbb{B}$ ($\mathrm{int}\ \mathbb{B}$) is the set $\{x\;:\;\|x\|<1\}$ is an open convex set. 
For any $a\in \R^n$, the ball with radius $\varepsilon>0$ and center $a$ is given by $a+\varepsilon \mathbb{B}$. For any set $C$ in $\R^n$, the set of points $x$ whose distance from $C$ does not exceed $\varepsilon$ is 
\begin{equation*}
    \{x\;:\;\exists\ y\in C, d(x,y)\le \varepsilon\} = \bigcup \{y+\varepsilon \mathbb{B}\;:\;y\in C\} = C + \varepsilon \mathbb{B}.
\end{equation*}

\begin{proposition}\label{prop:closure_interior_formulae}
    Let $C$ be a subset of $\R^n$. Then the closure $\mathrm{cl}\ C$ and interior $\mathrm{int}\ C$ of $C$ can be expressed as 
    \begin{enumerate}[(i)]
        \item $\mathrm{int}\ C = \{x\;:\; \exists\ \varepsilon>0,x+\varepsilon \mathbb{B}\subseteq C\}\equiv C_0,$ 
        \item $\mathrm{cl}\ C = \bigcap \{C+\varepsilon \mathbb{B}\;:\; \varepsilon>0\} \equiv C^0$. 
    \end{enumerate}
\end{proposition}

\begin{proof}
    \begin{enumerate}[(i)]
        \item \converse Suppose $x\in C_0$, so that for some $\varepsilon >0$, we have $x+\varepsilon\mathbb{B} \subseteq C$. Then $x+\varepsilon (\mathrm{int}\ \mathbb{B})\subseteq C$, hence $x\in \mathrm{int}\ C$. 

        \forward Suppose $x\in \mathrm{int}\ C$, so that for some $2 \varepsilon > 0$, we have $\{y\;:\;d(x,y)<2\varepsilon\}\subseteq C$. Then $x+\varepsilon \mathbb{B}\subseteq C$, which shows that $x\in C_0$. This shows that $C_0 = \mathrm{int}\ C$.

        \item \converse Suppose $x\in C^0$, so that for every $\varepsilon>0$, we have $x\in C+\varepsilon \mathbb{B}$. Then for every $\varepsilon>0$, $x\in C+\varepsilon (\mathrm{int}\ \mathbb{B})$. This means that there is some $y\in C$ for every $\varepsilon>0$ such that $x\in y+\varepsilon(\mathrm{int}\ \mathbb{B})$, hence $x\in \mathrm{cl}\ C$.

        \forward Suppose $x\in \mathrm{cl}\ C$, so that for every $2\varepsilon>0$, there is a $y\in C$ such that $d(x,y)<2\varepsilon$. Then for every $\varepsilon>0$, there is a $y\in C$ such that $x\in y+\varepsilon\mathbb{B}$, i.e., $x\in C+\varepsilon\mathbb{B}$. Hence $x\in C^0$. This shows that $C^0 = \mathrm{cl}\ C$.    
    \end{enumerate}
\end{proof}

\begin{definition}
    The \highlight{relative interior} of a convex set $C$ in $\R^n$, which we denote by $\mathrm{ri}\ C$, is defined as the interior which results when $C$ is regarded as a subset of its affine hull $\mathrm{aff}\ C$, i.e.,
    \begin{equation*}
        \mathrm{ri}\ C = \{x\in \mathrm{aff}\ C\;:\; \exists\ \varepsilon>0, (x+\varepsilon \mathbb{B})\cap (\mathrm{aff}\ C)\subseteq C\}.
    \end{equation*}
    The set difference $(\mathrm{cl}\ C) - (\mathrm{ri}\ C)$ is called the \highlight{relative boundary} of $C$. Naturally, $C$ is said to be \highlight{relatively open} if $\mathrm{ri}\ C = C$.
\end{definition}

\begin{remark}
    A pitfall to be noted is that, while inclusion $C_1\subseteq C_2$ implies $\mathrm{cl}\ C_1\subseteq \mathrm{cl}\ C_2$ and $\mathrm{int}\ C_1\subseteq \mathrm{int}\ C_2$, it does not in general imply $\mathrm{ri}\ C_1\subseteq \mathrm{ri}\ C_2$. For example, if $C_2 = \{(x_1,x_2)\;:\;|x_1|\le 1,|x_2|\le 1\}$ is a rectangle in $\R^2$ and $C_1 = \{(x_1,x_2)\;:\;x_1 = 1,|x_2|\le 1\}$ is an edge of $C_2$. Then $\mathrm{ri}\ C_1 = \{(x_1,x_2)\;:\;x_1 = 1,|x_2|< 1\}$ and $\mathrm{ri}\ C_2 = \{(x_1,x_2)\;:\;|x_1|< 1,|x_2|<1\}$, which are both non-empty but disjoint.
\end{remark}

\begin{remark}
    For an $n$-dimensional convex set of $\R^n$, $\mathrm{aff}\ C = \R^n$ by definition, so $\mathrm{ri}\ C = \mathrm{int}\ C$. An affine set is relatively open by definition. Every affine set is at the same time closed. This is clear from the fact that an affine set is an intersection of hyperplanes (Theorem \ref{thm:affine_set_representation}), and every hyperplane $H$ can be expressed as a level set of a continuous function.
\end{remark}

\begin{remark}\label{remark:simplying_ri_cl}
    Closures and relative interiors are preserved under one-to-one affine transformations of $\R^n$ onto itself as such a transformation preserves affine hulls and is continuous in both directions. One should keep this in mind as a useful device for simplfying proofs. For example, if $C$ is an $m$-dimensional convex set in $\R^n$, there exists by Cor. \ref{cor:affine_sets_one_to_one_map} a one-to-one affine transformation $T$ of $\R^n$ onto itself which carries $\mathrm{aff}\ C$ onto the subspace 
    \begin{equation*}
        L = \{x = (\xi_1,\ldots,x_n)\;:\;\xi_{m+1} = \ldots = \xi_{n} = 0\}.
    \end{equation*}
    This $L$ can be regarded as a copy of $\R^n$. It is often possible in this manner to reduce a question about general convex sets to the case where the convex set is of full dimension, i.e., has the whole space as its affine hull. 
\end{remark}

\begin{theorem}\label{thm:rel_int_line_seg}
    Let $C$ be a convex set in $\R^n$. Let $x\in \mathrm{ri}\ C$ and $y\in \mathrm{cl}\ C$. Then $(1-\lambda)x+\lambda y$ belongs to $\mathrm{ri}\ C$ (and hence in particular to $C$) for $0\le \lambda <1$.
\end{theorem}

\begin{proof}
    In view of Remark \ref{remark:simplying_ri_cl}, we can limit attention to the case where $C$ is $n$-dimensional, so that $\mathrm{ri}\ C = \mathrm{int}\ C$. Let $0\le \lambda<1$. We must show that $(1-\lambda)x+\lambda y+\varepsilon \mathbb{B}\subseteq C$ for some $\varepsilon>0$. We have $y\in C+\varepsilon\mathbb{B}$ for every $\varepsilon>0$, so that 
    \begin{equation*}
        (1-\lambda)x+\lambda y+\varepsilon \mathbb{B}\subseteq (1-\lambda)x+\lambda (C+\varepsilon\mathbb{B})+\varepsilon \mathbb{B} = (1-\lambda)[x +\varepsilon(1+\lambda)(1-\lambda)^{-1}\mathbb{B}]+\varepsilon C.
    \end{equation*}
    When $\varepsilon$ is sufficiently small, $x +\varepsilon(1+\lambda)(1-\lambda)^{-1}\mathbb{B}\subseteq C$, since $x\in\mathrm{int}\ C$, so that 
    \begin{equation*}
        (1-\lambda)x+\lambda y+\varepsilon \mathbb{B}\subseteq (1-\lambda)C+\varepsilon C = C.
    \end{equation*}
\end{proof}

\begin{theorem}\label{thm:convexity_of_ri_cl}
    Let $C$ be any convex set in $\R^n$. Then $\mathrm{cl}\ C$ and $\mathrm{ri}\ C$ are convex sets in $\R^n$ having the same affine hull, and hence the same dimension, as $C$. (In particular, if $C\neq \emptyset$ then $\mathrm{ri}\ C \neq \emptyset$.)
\end{theorem}

\begin{proof}
    By the convexity of $C$ and $\mathbb{B}$, the set $C+\varepsilon\mathbb{B}$ is convex for any $\varepsilon$. The intersection of the collection of these sets for $\varepsilon>0$ is $\mathrm{cl}\ C$, hence it is convex. The affine hull of $\mathrm{cl}\ C$ is atleast as large as that of $C$, and since $\mathrm{cl}\ C\subseteq \mathrm{aff}\ C$ it must actually coincide with $\mathrm{aff}\ C$.

    The convexity of $\mathrm{ri}\ C$ is a corollary of the preceding theorem (take $y$ to be in $\mathrm{ri}\ C$). To complete the proof, it is enough to show that, in the case where $C$ is $n$-dimensional with $n>0$, the interior of $C$ is not empty. An $n$-dimensional convex set contains an $n$-dimensional simplex (Theorem \ref{thm:convex_set_dimension_simplices}). We shall show that such a simplex $S$ has non-empty interior. Applying an affine transformation if necessary, we can assume that the vertices of $S$ are the vectors $(0,\ldots,0)$, $(1,0,\ldots, 0)$, $\ldots$, $(0,\ldots, 0, 1)$ so that
    \begin{align*}
        S &= \{(\xi_1,\ldots, \xi_n)\;:\;\xi_{j}\ge 0, \xi_0+\xi_1+\ldots+\xi_n = 1\}\\
        &= \{(\xi_1,\ldots, \xi_n)\;:\;\xi_{j}\ge 0, \xi_1+\ldots+\xi_n \le 1\}.
    \end{align*}
    But this simplex does have a non-empty interior, namely 
    \begin{equation*}
        \mathrm{int}\ S = \{(\xi_1,\ldots, \xi_n)\;:\;\xi_{j}> 0, \xi_1+\ldots+\xi_n < 1\}.
    \end{equation*}
    Hence $\mathrm{int}\ C\neq \emptyset$ as claimed.
\end{proof}

\begin{remark}
    For any set $C$ in $\R^n$, convex or not, the laws
    \begin{equation*}
        \mathrm{cl}\ (\mathrm{cl}\ C) = \mathrm{cl}\ C,\qquad \mathrm{ri}\ (\mathrm{ri}\ C) = \mathrm{ri}\ C
    \end{equation*}
    are valid. However, under convexity, the next theorem shows the validity of the complementary laws.
\end{remark}

\begin{theorem}
    For any convex set $C$ in $\R^n$, $(i)\ \mathrm{cl}\ (\mathrm{ri}\ C) = \mathrm{cl}\ C$ and $(ii)\ \mathrm{ri}\ (\mathrm{cl}\ C) = \mathrm{ri}\ C$.
\end{theorem}

\begin{proof}
    \begin{enumerate}[(i)]
        \item \forward Trivially, $\mathrm{cl}\ (\mathrm{ri}\ C) \subseteq \mathrm{cl}\ C$, since $\mathrm{ri}\ C\subseteq C$.
        
        \converse Given any $y\in \mathrm{cl}\ C$ and any $x\in \mathrm{ri}\ C$ (such an $x$ exists by the last theorem when $C\neq \emptyset$), the line segment between $x$ and $y$ lies entirely in $\mathrm{ri}\ C$ except perhaps for $y$ (Theorem \ref{thm:rel_int_line_seg}). Then by Prop. \ref{prop:closure_interior_formulae}(ii), $y\in \mathrm{cl}\ (\mathrm{ri}\ C)$.

        \item \converse The inclusion $\mathrm{ri}\ (\mathrm{cl}\ C)\supseteq \mathrm{ri}\ C$ holds, since $\mathrm{cl}\ C\supseteq C$ and the affine hulls of $\mathrm{cl}\ C$ and $C$ coincide.
        
        \forward Now let $z\in \mathrm{ri}\ (\mathrm{cl}\ C)$. We shall show that $z\in \mathrm{ri}\ C$. Let $x$ be any point of $\mathrm{ri}\ C$. (We can suppose that $x\neq z$, for otherwise $z\in \mathrm{ri}\ C$ trivially.) Consider the line through $x$ and $z$. For values $\mu>1$ with $\mu-1$ sufficiently small, the point
        \begin{equation*}
            y = (1-\mu) x+ \mu z = z - (\mu-1)(x-z)
        \end{equation*}
        on this line still belongs to $\mathrm{ri}\ (\mathrm{cl}\ C)$ and hence to $\mathrm{cl}\ C$. For such a $y$, we can express $z$ in terms of $(1-\lambda)x+\lambda y$ with $0<\lambda <1$ (specifically with $\lambda=1/\mu$). By Theorem \ref{thm:rel_int_line_seg}, $z\in \mathrm{ri}\ C$.
    \end{enumerate}
\end{proof}

\begin{corollary}\label{cor:cl_ri_convex_sets}
    Let $C_1$ and $C_2$ be convex sets in $\R^n$. Then $\mathrm{cl}\ C_1 = \mathrm{cl}\ C_2$ iff $\mathrm{ri}\ C_1 = \mathrm{ri}\ C_2$. These conditions are equivalent to the condition that $\mathrm{ri}\ C_1\subseteq C_2\subseteq \mathrm{cl}\ C_1$.
\end{corollary}

\begin{corollary}
    If $C$ is a convex set in $\R^n$, then every open set which meets $\mathrm{cl}\ C$ also meets $\mathrm{ri}\ C$.
\end{corollary}

\begin{proof}
    
\end{proof}

\begin{corollary}
    If $C_1$ is a convex subset of the relative boundary of a non-empty convex set $C_2$ in $\R^n$, then $\mathrm{dim}\ C_1<\mathrm{dim}\ C_2$.
\end{corollary}

\begin{proof}
    Suppose $C_1$ had the same dimension as $C_2$. Since $C_1\subseteq \mathrm{cl}\ C_2-\mathrm{ri}\ C_2\subseteq\mathrm{cl}\ C_2\subseteq\mathrm{aff}\ C_2$. But then $\mathrm{aff}\ C_1 = \mathrm{aff}\ C_2$ which implies that $C_1$ has an interior point relative to $\mathrm{aff}\ C_2$. Such point could not be in $\mathrm{cl}\ (\mathrm{ri}\ C_2)$, since $\mathrm{ri}\ C_2$ is disjoint from $C_1$, and hence it could not be in $\mathrm{cl}\ (\mathrm{ri}\ C_2) = \mathrm{cl}\ C_2$ which leads to a contradiction.
\end{proof}

\begin{theorem}\label{thm:ri_characterization_convex_sets}
    Let $C$ be a non-empty convex set in $\R^n$. Then $z\in \mathrm{ri}\ C$ iff for every $x\in C$, there exists a $\mu>1$ such that $(1-\mu) x + \mu z$ belongs to $C$.
\end{theorem}

\begin{proof}
    \forward For any $x\in C$, since $z\in \mathrm{ri}\ C$, there exists some $y\in C$ such that for $\lambda<1$, we have $z = (1-\lambda)+\lambda y$. By rewriting, we see that $y = (1-\mu)x+\mu z$ for $\mu =1/\lambda>1$. 

    \noindent \converse Suppose $z$ satisfies the condition for every $x\in C$. Since $\mathrm{ri}\ C\neq \emptyset$ by Theorem \ref{thm:convexity_of_ri_cl}, there exists a point $x\in \mathrm{ri}\ C$. Let $y$ be the corresponding point $(1-\mu)x+\mu z$ in $C$, $\mu >1$. Then $z = (1-\lambda)x+\lambda y$, where $0<\lambda = \mu^{-1}<1$. Hence $z\in \mathrm{ri}\ C$ by Theorem \ref{thm:rel_int_line_seg}. 
\end{proof}

\begin{corollary}
    Let $C$ be a convex set in $\R^n$. Then $z\in \mathrm{int}\ C$ iff for every $y\in \R^n$, there exists some $\varepsilon>0$ such that $z+\varepsilon y\in C$.
\end{corollary}

\begin{theorem}\label{thm:intersection_of_ri_cl}
    Let $C_i$ be a convex set in $\R^n$ for $i\in I$ (an index set). Suppose that the sets $\mathrm{ri}\ C_i$ have at least one point in common. Then 
    \begin{equation*}
        \mathrm{cl}\ \bigg(\bigcap_{i\in I} C_i\bigg) = \bigcap_{i\in I} (\mathrm{cl}\ C_i).
    \end{equation*}
    If $I$ is finite, then also 
    \begin{equation*}
        \mathrm{ri}\ \bigg(\bigcap_{i\in I} C_i\bigg) = \bigcap_{i\in I} (\mathrm{ri}\ C_i).
    \end{equation*}
\end{theorem}

\begin{proof}
    \forward The set $\bigcap_{i} (\mathrm{cl}\ C_i)$ is an intersection of closed sets so it is closed. Since $C_i\subseteq \mathrm{cl}\ C_i$, we have $\bigcap_{i} C_i \subseteq \bigcap_{i} (\mathrm{cl}\ C_i)$. Then by definition of closure, we have $\mathrm{cl}\ \Big(\bigcap_{i} C_i\Big) \subseteq \bigcap_{i} (\mathrm{cl}\ C_i)$.

    \noindent \converse Fix any $x\in \bigcap_{i} (\mathrm{ri}\ C_i) \subseteq \bigcap_{i} (\mathrm{cl}\ C_i)$. Given any $y\in \bigcap_{i} (\mathrm{cl}\ C_i)$, the vector $(1-\lambda)x+\lambda y\in \bigcap_{i} (\mathrm{cl}\ C_i)$ for $0\le\lambda\le 1$ by the convexity of $\bigcap_{i} (\mathrm{cl}\ C_i)$. But it also belongs $\bigcap_i (\mathrm{ri}\ C_i)$ for $0\le \lambda<1$ by Theorem \ref{thm:rel_int_line_seg}, and $y$ is the limit of this vector as $\lambda\uparrow 1$ so that it belongs to the closure of $\bigcap_i (\mathrm{ri}\ C_i)$. It follows that 
    \begin{equation*}
        \bigcap_{i} (\mathrm{cl}\ C_i)\subseteq \mathrm{cl}\ \Big(\bigcap_{i} (\mathrm{ri}\ C_i)\Big).
    \end{equation*}
    Now note that $\mathrm{ri}\ C_i\subseteq C_i$ and hence $\mathrm{cl}\ \Big(\bigcap_{i} (\mathrm{ri}\ C_i)\Big) \subseteq \mathrm{cl}\ \Big(\bigcap_{i} C_i\Big)$. 

    \noindent \forward By the previous result, we have shown that 
    \begin{equation*}
        \bigcap_{i} (\mathrm{cl}\ C_i)\subseteq \mathrm{cl}\ \Big(\bigcap_{i} (\mathrm{ri}\ C_i)\Big) \subseteq \mathrm{cl}\ \Big(\bigcap_{i} C_i\Big) \subseteq \bigcap_{i} (\mathrm{cl}\ C_i),
    \end{equation*}
    or in other words, $\bigcap_i (\mathrm{ri}\ C_i)$ and $\bigcap_i C_i$ have the same closure. By Corollary \ref{cor:cl_ri_convex_sets}, these two sets must also have the same relative interior. Therefore
    \begin{equation*}
        \mathrm{ri}\ \Big(\bigcap_{i} C_i\Big) = \mathrm{ri}\ \Big(\bigcap_i (\mathrm{ri}\ C_i)\Big) \subseteq \bigcap_{i} (\mathrm{ri}\ C_i).
    \end{equation*}

    \noindent \converse Suppose $I$ is finite and take $z\in \bigcap_i \mathrm{ri}\ C_i$. By Theorem \ref{thm:ri_characterization_convex_sets}, any line segment in $\bigcap_i C_i$ with $z$ as endpoint can be prolonged slightly beyond $z$ in each of the sets $C_i$. The intersection of these prolonged segments, since there are only finitely many of them, is a prolongation in $\bigcap_i C_i$ of the original segment. Thus $z\in \mathrm{ri}\ \Big(\bigcap_i C_i\Big)$ by the criterion of Theorem \ref{thm:ri_characterization_convex_sets}.
\end{proof}

\begin{remark}
    The results in Theorem \ref{thm:intersection_of_ri_cl} can fail when the sets $\mathrm{ri}\ C_i$ do not have a point in common. Take for example, $C_1$ is the positive orthant in $\R^2$ with the origin adjoined, and $C_2$ is the horizontal axis of $\R^2$. Note the convexity of these sets. In this case $\mathrm{ri}\ C_1$ is the positive orthant and $\mathrm{ri}\ C_2 = C_2$ which has no points in common. As a result, the intersection of closures, which is the nonnegative segment of the horizontal axis, not equal to the closure of the intersection, which is simply the origin. 

    Additionally, the finiteness if $I$ in the second formula is also necessary. Consider $C_\alpha = [0,1+\alpha]$ with $\alpha>0$. We see that 
    \begin{equation*}
        \mathrm{ri}\ \Big(\bigcap_{\alpha>0} C_\alpha\Big) = \mathrm{ri}\ [0,1] = (0,1).   
    \end{equation*}
    But 
    \begin{equation*}
        \bigcap_{\alpha>0} (\mathrm{ri}\ [0,1+\alpha]) = \bigcap_{\alpha>0} (0,1+\alpha) = (0,1] \neq \mathrm{ri}\ \Big(\bigcap_{\alpha>0} C_\alpha\Big).
    \end{equation*}
\end{remark}

\begin{corollary}\label{cor:intersection_ri_cl}
    Let $C$ be a convex set and let $M$ be an affine set (such as a line or a hyperplane) which contains a point of $\mathrm{ri}\ C$. Then 
    \begin{equation*}
        \mathrm{ri}\ (M\cap C) = M \cap \mathrm{ri}\ C,\qquad \mathrm{cl}\ (M\cap C) = M \cap \mathrm{cl}\ C.
    \end{equation*}
\end{corollary}

\begin{corollary}
    Let $C_1$ be a convex set and $C_2$ be a convex set contained in $\mathrm{cl}\ C_1$, but not entirely contained in the relative boundary of $C_1$. Then $\mathrm{ri}\ C_2\subseteq \mathrm{ri}\ C_1$.
\end{corollary}

\begin{proof}
    The hypothesis implies $\mathrm{ri}\ C_2$ has a point in common with $\mathrm{ri}\ C_1$. Hence
    \begin{equation*}
        \mathrm{ri} C_2\cap \mathrm{ri}\ C_1 = \mathrm{ri} C_2\cap \mathrm{ri}\ (\mathrm{cl}\ C_1) = \mathrm{ri} (C_2\cap \mathrm{cl}\ C_1) = \mathrm{ri} C_2,
    \end{equation*}
    i.e., $\mathrm{ri}\ C_2\subseteq \mathrm{ri}\ C_1$.
\end{proof}

\begin{theorem}\label{thm:ri_cl_linear_transformation}
    Let $C$ be a convex set in $\R^n$, and let $A$ be a linear transformation from $\R^n$ to $\R^m$. Then
    \begin{equation*}
        \mathrm{ri}\ (AC) = A(\mathrm{ri}\ C),\qquad \mathrm{cl}\ (AC) \supseteq A(\mathrm{cl}\ C).
    \end{equation*}
\end{theorem}

\begin{proof}
    The closure inclusion merely reflects the fact that a linear transformation is continuous; it does not depend on $C$ being convex.

    \noindent \forward From the closure inclusion, we note that
    \begin{equation*}
        \mathrm{cl}\ A(\mathrm{ri}\ C) \supseteq A(\mathrm{cl}\ (\mathrm{ri}\ C)) = A(\mathrm{cl}\ C) \supseteq AC\supseteq A(\mathrm{ri}\ C).
    \end{equation*}
    This implies that $AC$ has the same closure as $A(\mathrm{ri}\ C)$, which is $A(\mathrm{cl}\ C)$, and hence also the same relative interior by Corollary \ref{cor:cl_ri_convex_sets}. Therefore $\mathrm{ri}\ (AC) \subseteq \mathrm{ri}\ (A(\mathrm{ri}\ C))\subseteq A(\mathrm{ri}\ C)$.

    \noindent \converse Suppose $z\in A(\mathrm{ri}\ C)$. We shall use Theorem \ref{thm:ri_characterization_convex_sets} to show that $z\in \mathrm{ri}\ (AC)$. Let $x$ be any point of $AC$. Choose any element $z'\in \mathrm{ri}\ C$ and $x'\in C$, such that $Az' = z$ and $Ax' = x$. There exists some $\mu>1$ such that the vector $(1-\mu)x'+\mu z'$ belongs to $C$. The image of this vector under $A$ is $(1-\mu)x+\mu z$. Thus, for the same $\mu>1$, $(1-\mu)x+\mu z$ belongs to $AC$. Therefore $z\in \mathrm{ri}\ (AC)$.
\end{proof}

\begin{corollary}\label{cor:ri_scalar_multiple}
    For any convex set $C$ and any real number $\lambda$, $\mathrm{ri}\ (\lambda C) = \lambda (\mathrm{ri}\ C)$.
\end{corollary}

\begin{remark}
    It is elementary that, for the direct sum $C_1\oplus C_2$ in $\R^{m+p}$ of convex sets $C_1\subseteq \R^m$ and $C_2\subseteq \R^p$, one has
    \begin{align*}
        \mathrm{ri}\ (C_1\oplus C_2) &= \mathrm{ri}\ C_1\oplus \mathrm{ri}\ C_2,\\
        \mathrm{cl}\ (C_1\oplus C_2) &= \mathrm{cl}\ C_1\oplus \mathrm{cl}\ C_2.
    \end{align*}
    This remark also applies to the cartesian product due to Remark \ref{remark:direct_sum_cartesian_product}.
\end{remark}

\begin{corollary}
    For any convex sets $C_1$ and $C_2$ in $\R^{n}$,
    \begin{align*}
        \mathrm{ri}\ (C_1+ C_2) &= \mathrm{ri}\ C_1+ \mathrm{ri}\ C_2,\\
        \mathrm{cl}\ (C_1+ C_2) &\supseteq \mathrm{cl}\ C_1+ \mathrm{cl}\ C_2.
    \end{align*}
\end{corollary}

\begin{proof}
    Note that $C_1+C_2 = A(C_1\oplus C_2)$, where $A$ is the addition linear transformation from $\R^{2n}$ to $\R^n$.
\end{proof}

\begin{theorem}\label{thm:ri_cl_inverse_linear_transformation}
    Let $A$ be a linear transformation from $\R^n$ to $\R^m$. Let $C$ be a convex set in $\R^m$ such that $A^{-1}(\mathrm{ri}\ C)\neq \emptyset$. Then 
    \begin{equation*}
        \mathrm{ri}\ (A^{-1}C) = A^{-1}(\mathrm{ri}\ C),\qquad \mathrm{cl}\ (A^{-1}C) = A^{-1}(\mathrm{cl}\ C).
    \end{equation*}
\end{theorem}

\begin{proof}
    Let $D = \R^n\oplus C$, and let $M$ be the graph of $A$. Then $M$ is an affine set by Prop. \ref{prop:graph_affine_transformation} (in fact a subspace by Theorem \ref{thm:subspace_affine}), and $M$ contains a point of $\mathrm{ri}\ D$. Let $P$ be the projection $(x,y)\rightarrow x$ from $\R^{m+n}$ to $\R^n$. Then $A^{-1}C = P(M\cap D)$. Calculating with the rules in Theorem \ref{thm:ri_cl_linear_transformation} and Corollary \ref{cor:intersection_ri_cl}, we get 
    \begin{align*}
        \mathrm{ri}\ (A^{-1}C) &= P(\mathrm{ri}\ (M\cap D)) = P(M\cap \mathrm{ri}\ D) = A^{-1}(\mathrm{ri}\ C),\\
        \mathrm{cl}\ (A^{-1}C) &\supseteq P(\mathrm{cl}\ (M\cap D)) = P(M\cap \mathrm{cl}\ D) = A^{-1}(\mathrm{cl}\ C).
    \end{align*}
    The remaining inclusion $\mathrm{cl}\ (A^{-1}C)\subseteq A^{-1}(\mathrm{cl}\ C)$ is implied by the continuity of $A$.
\end{proof}

\begin{remark}
    Consider the case where $m = n = 2$, $C$ is the positive orthant of $\R^2$ with origin adjoined, a convex set in $\R^2$, and $A$ maps $(\xi_1,\xi_2)$ onto $(\xi_1,0)$. In this case, $\mathrm{ri}\ C$ is the positive orthant and $A^{-1}(\mathrm{ri}\ C)$ is empty. But $A^{-1}C$ is the vertical axis of $\R^2$ and its relative interior is itself, which is clearly not empty. 
\end{remark}

\begin{remark}
    The class of relatively open convex sets is preserved under finite intersections, scalar multiplication, addition, and taking images or inverse images under linear (or affine transformations), according to the results above.
\end{remark}

\begin{theorem}
    Let $C$ be a convex set in $\R^{m+p}$. For each $y\in \R^m$, let $C_y$ be the set of vectors $z\in \R^p$ such that $(y,z)\in C$. Let $D = \{y\;:\;C_y\neq\emptyset\}$. Then $(y,z)\in \mathrm{ri}\ C$ iff $y\in \mathrm{ri}\ D$ and $z\in \mathrm{ri}\ C_y$.
\end{theorem}

\begin{proof}
    The projection $(y,z)\rightarrow y$ carries $C$ onto $D$, and hence $\mathrm{ri}\ C$ onto $\mathrm{ri}\ D$ by Theorem \ref{thm:ri_cl_linear_transformation}. For any given $y\in \mathrm{ri}\ D$ and the affine set $M = \{(y,z)\;:\;z\in \R^p\}$, the points of $\mathrm{ri}\ C$ projecting onto $y$ are the points of 
    \begin{equation*}
        M\cap \mathrm{ri}\ C \overset{(\dagger)}{=} \mathrm{ri}\ (M\cap C) = \{(y,z)\;:\;z\in \mathrm{ri}\ C_y\},
    \end{equation*}
    where $(\dagger)$ is due to Corollary \ref{cor:intersection_ri_cl}. Thus, for any given $y\in \mathrm{ri}\ D$, we have $(y,z)\in \mathrm{ri}\ C$ iff $z\in \mathrm{ri}\ C_y$, and this proves the result.
\end{proof}

\begin{corollary}
    Let $C$ be a non-empty convex set in $\R^n$, and let $K$ be a convex cone in $\R^{n+1}$ generated by $\{(1,x)\;:\;x\in C\}$. Then $\mathrm{ri}\ K$ consists of the pairs $(\lambda, \lambda x)$ such that $\lambda>0$ and $x\in \mathrm{ri}\ C$.
\end{corollary}

\begin{corollary}
    Let $C$ be a non-empty convex set in $\R^n$, and let $K$ be a convex cone in $\R^{n}$ generated by $C$. Then $\mathrm{ri}\ K$ consists of the vectors $\lambda x$ such that $\lambda>0$ and $x\in \mathrm{ri}\ C$.
\end{corollary}

\begin{remark}
    Observe that the relative interior of a convex cone are always convex set too. This is immediate from Corollary \ref{cor:ri_scalar_multiple}, because a convex set $C$ is a convex cone iff $\lambda C = C$ for every $\lambda >0$.
\end{remark}

% \begin{theorem}
%     Let $C_1,\ldots, C_m$ be non-empty convex sets in $\R^n$, and let $C_0 = \mathrm{conv}\ (C_1\cup \ldots \cup C_m)$. Then 
%     \begin{equation*}
%         \mathrm{ri}\ C_0 = \bigcup \{\lambda_1\mathrm{ri}\ C_1+\ldots+\lambda_m\mathrm{ri}\ C_m\;:\; \lambda_i>0,\lambda_1+\ldots+\lambda_m = 1\}.
%     \end{equation*}
% \end{theorem}

\subsection{Closures of Convex Functions}

\begin{definition}
    An extended-real-valued function $f$ given on a set $S\subseteq \R^n$ is said to be \highlight{lower semi-continuous} at a point $x$ of $S$ if 
    \begin{equation*}
        f(x)\le \lim_{i\rightarrow \infty} f(x_i)
    \end{equation*}
    for every sequence $x_1,x_2,\ldots,$ in $S$ such that $x_i$ converges to $x$ and the limit of $f(x_1),f(x_2),\ldots,$ exists in $[-\infty,+\infty]$. This condition may be expressed as:
    \begin{equation*}
        f(x) = \liminf_{y\rightarrow x} f(y) = \lim_{\varepsilon\downarrow 0} (\inf \{f(y)\;:\; \|y-x\|\le \varepsilon\}).
    \end{equation*}
\end{definition}

\newpage
\section*{Convex Cones - Fenchel Notes}

\subsection*{Cones}

% Let $\R^n$ be an $n$-dimensional Euclidean real vector space with the inner product defined for any $x,y\in \R^n$ as $\innerproduct{x}{y} = x^Ty = \sum_{i=1}^n x_i y_i$, where $x = [x_1,\ldots, x_n]^T$ and $y = [y_1,\ldots, y_n]^T$ are the coordinates of $x$ and $y$ respectively. Let the norm and the metric on $\R^n$ be defined as $\|x\| = \sqrt{\innerproduct{x}{x}}$ and $d(x,y) = \|x-y\|$, respectively.

\begin{definition}
    A subset $M$ of $\R^n$ is called a \highlight{cone} if $0$ is in $M$ and $x\in M$ implies $\lambda x\in M$ for every non-negative real scalar $\lambda$. The particular cones consisting of a non-zero vector $x$ and all its multiples $\lambda x$ ($\lambda \ge 0$) is a \highlight{ray}, denoted by $(x)$. 
\end{definition}

\begin{remark}
    A cone which contains atleast one non-zero vector is therefore just the union of the rays it contains.
\end{remark}

Since all non-trivial cones may be thought of as set of rays, it is desirable to introduce a metric-topology on the set of all rays from the metric-topology of $\R^n$ induced from the canonical metric $d(\cdot,\cdot)$.

\begin{remark}
    Let $S^n = \{x\in \R^n\ : \|x\| = 1\}$ be the unit sphere in $\R^n$. Then for any $x\in \R^n$, we have
    \begin{equation*}
        (x) \text{ is a ray in } \R^n \Leftrightarrow \frac{x}{\|x\|} \text{ is in } S^n.
    \end{equation*}
    Thus, from here on we will represent $S^n$ to be the set of all rays in $\R^n$.
\end{remark}


\begin{proposition}
    Suppose for any $x,y\in S^n$, we define the map $\mapping{\metric{\cdot}{\cdot}}{S^n\times S^n}{\R}$ as
    \begin{equation*}
        \metric{x}{y} = \sqrt{2(1 - x^Ty)},
    \end{equation*}
    which is the chord distance between $x$ and $y$. Then, $\metric{\cdot}{\cdot}$ is a metric on $S^n$ and consequently, $(S^n, \metric{\cdot}{\cdot})$ is a metric space.
\end{proposition}

\begin{proof}
    Note that 
    \begin{align*}
        \metric{x}{y} &= \sqrt{2(1 - x^Ty)} \\
        &= \sqrt{1 + 1 - 2x^Ty} \\
        &= \sqrt{\|x\|^2 + \|y\|^2 - 2x^Ty} \\
        &= \|x-y\| = d(x,y).
    \end{align*}
    Then, $\metric{\cdot}{\cdot}$ is a restriction of $d(\cdot,\cdot)$ on $S^n$, in other words, $\metric{\cdot}{\cdot} = \bigvert{d(\cdot,\cdot)}{S^n}$. 
\end{proof}


\begin{definition}\label{def:cone_topology}
    We review definitions regarding the metric topology induced by $\metric{\cdot}{\cdot}$ on $S^n$.
    \begin{enumerate}[(a)]
        \item A \highlight{sequence of rays} $(x^\nu)$ is said to \highlight{converge} to a ray $(x)$ if $\metric{x^\nu}{x}\rightarrow 0$. 
        \item A ray $(x)$ is called a \highlight{limit ray} of a cone $M$ if there is a sequence of rays of the cone which are different from $(x)$ and which converges to $(x)$. 
        \item A \highlight{closed cone} or a closed set of rays is a cone which contains all its limit points. 
        \item For any $\varepsilon>0$, the \highlight{$\varepsilon$-neighborhood} of $(x)$ $\B_\varepsilon(x)$ is the set of all rays $(y)$ with $\metric{x}{y} < \varepsilon$.
        \item $M$ is \highlight{open} if for every $(x)\in M$, there is an $\varepsilon>0$, such that $\B_\varepsilon(x)\subseteq M$.
        \item A ray $(x)$ is called an \highlight{interior ray} of a cone $M$ if $M$ contains an $\varepsilon$-neighborhood of $(x)$ for some $\varepsilon>0$.
        \item A ray $(x)$ such that the complementary cone to the cone $M$ contains a neighborhood of $(x)$ is called an \highlight{exterior ray} of $M$.
        \item A \highlight{boundary ray} of a cone $M$ is a limit ray of $M$ which is not an interior ray of $M$.
    \end{enumerate}
\end{definition}

\begin{lemma}\label{lem:closed_sets_sphere}
    A set $M\subseteq S^n$ is closed in $S^n\subseteq \R^n$ iff $M$ is closed in $\R^n$.
\end{lemma}

\begin{proof}
    \converse If $M$ is closed in $\R^n$, then it is closed in $S^n$ by subspace topology. 
    
    \forward First we show that $S^n$ itself is closed in $\R^n$. Consider the norm $\mapping{F}{\R^n}{\R}$ defined as $F(x) = \|x\|$. Since $F$ is continuous and $S^n = F^{-1}(\{1\})$, i.e., $S^n$ is the preimage of the closed set $\{1\}$ under $F$, $S^n$ is closed in $\R^n$. Suppose if $M$ is closed in $S^n$, then there exists a closed set $M^\prime$ in $\R^n$ such that $M = S^n \cap M^\prime$, which is intersection of two closed sets in $\R^n$. Hence $M$ is closed in $\R^n$ as well.
\end{proof}

\begin{proposition}
    An alternate definition of open and closed cones are given below.
    \begin{enumerate}[(a)]
        \item A cone is {closed} in the sense of Def.~\ref{def:cone_topology}(c) iff it is closed in $\R^n$.
        \item A cone is {open} in the sense of Def.~\ref{def:cone_topology}(e) iff it is open in $\R^n$.
    \end{enumerate}
\end{proposition}

\begin{proof}
    (a) It follows from Lemma \ref{lem:closed_sets_sphere} and the fact that we identified the set of all rays in $\R^n$ with $S^n$. 

    (b)
\end{proof}

With any cone $M$ there is associated a smallest linear subspace of $\R^n$ which contains $M$.

\begin{definition}
    The intersection of all subspaces containing the cone $M$ is denoted by $S(M)$. The dimension $d(M)$ of the space $S(M)$ is called \highlight{linear dimension} of the cone $M$.
\end{definition}

For the results on cones in later sections, open or closed relative to $S(M)$, and interior, exterior, and boundary rays relative to $S(M)$ will be considered rather than their counterparts in the topology of the full space $\R^n$. They will be called for simplicity relative interior, relative exterior, relative boundary rays.

\subsection*{Convex Cones}

\begin{definition}
    A cone $C$ is \highlight{convex} if the ray $(x+y)$ is in $C$ whenever $(x)$ and $(y)$ are rays in $C$. The largest subspace $s(C)$ contained in a convex cone $C$ is called the \highlight{lineality space} of $C$ and the dimension $l(C)$ of $s(C)$ is called the lineality of $C$.
\end{definition}

\begin{remark}
    A set $C$ of vectors is a convex cone iff it contains all vectors
    \begin{equation*}
    \lambda x + \mu y\ (\lambda, \mu \ge 0; x,y\in C).
    \end{equation*}
\end{remark}

\begin{lemma}
    If $(x)$ is an interior ray of a convex cone $C$ relative to $S(C)$ and $(y)$ is a boundary or interior ray of $C$ relative to $S(C)$, then every ray $(\lambda x+\mu y)$, where $\lambda$ and $\mu$ are positive real numbers, is an interior ray of $C$ relative to $S(C)$. 
\end{lemma}


% \newpage

% \begin{thebibliography}{9}
% \bibitem{ShaiBenDavid}
% Shalev-Shwartz S., Ben-David S. (2014) Understanding Machine Learning: From Theory to Algorithms, Cambridge University Press

% \bibitem{Seq Lemma (b)}
% [Sequence Lemma (b)] \href{https://math.stackexchange.com/questions/1876224/}{math.SE}

% \end{thebibliography}

\begin{thebibliography}{9}
\bibitem{Rockafellar} 
    Rockafellar, R.T. Convex Analysis. Princeton University Press, 1972.
    
\bibitem{Fenchel}
    Fenchel, W. Convex Cones, Sets, and Functions. Princeton University, 1953.
    
    
\end{thebibliography}
    

\end{document}